{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Kaggle_final_version.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"lsUOFyQ8HZc3","colab_type":"text"},"source":["## **Kaggle Project**\n","\n","---\n","\n","\n","#### Applications of Deep Learning(WUSTL, Fall 2019): Natural Language Understanding: Are Two Sentences of the same Topic\n","#### **Team**: Kattle\n","#### **Team member**: Gao Yaming, Lan Gangqi, Li Hao, Lu Wei\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"CeIh11SY6EpZ","colab_type":"text"},"source":["We devide this project into 4 parts: pre-processing, feature engineering, feature selection and modeling. Following description shows how we work on this project.\n","\n","\n","\n","### 1.   Pre-processing <br>\n","&nbsp;&nbsp;&nbsp;&nbsp;We use three kinds of pre-processing methods for the following work:\n","<br>&nbsp;&nbsp;&nbsp;&nbsp;**The first method** is to turn all the words into lowercase and remove the \n","stop words and lemma the words(for counting the same words between sent1 and sent2). \n","<br>&nbsp;&nbsp;&nbsp;&nbsp;**The second method** is only to turn all the words into lowercase (for word2vec and GloVe). \n","<br>&nbsp;&nbsp;&nbsp;&nbsp;**The third method** is to remove the stop words, and then use PoS to remove some ADP, DET, CCONJ, etc, and then do the lemmatization (for BoW).\n","\n","\n","### 2.   Feature Engineering <br>\n","&nbsp;&nbsp;&nbsp;&nbsp;Our features are derived from two parts: some basic statistical counting based on sent1 and sent2, and socre the similarity between two sentences by using embedding skills and algorithms.<br>\n","**Statistical counting:**\n","*   tf-idf/ same words (lemma version)\n","*   dummy: We select 100 words occurred with the highest frequency in the training set and turn them into 100 dummies. Besides, we also manually choose 18 categories and choose some common words in these categories. If sent1 and sent2 both contains the words from one category, then they can be put into the same category. Last, we also create dummies of if two sentences both contains some special characters ($, %, 0-9, etc).\n","*   kaggle_help_2: We adopt all the varibles you provided in help_2 <br>\n","\n","**Similarity:** <br>\n","Similarity can be counted by firstly embedding the words into vectors by using techniques like word2vec and GloVe, and then using algorithms such as cosine similarity, word mover distance to count the similarity between two sentences. We use permutations in embedding methods and algorithms to generate about 400 features in this step.\n","*   embedding: word2vec models: different parameters(min_count, size, window); GloVe models:(different vector length)\n","*   algorithms: bow_cosine, bow_cityblock, bow_jaccard, bow_canberra, bow_euclidean, bow_minkowski, bow_braycurtis, word mover distance.\n","\n","### 3. Feature Selection <br>\n","&nbsp;&nbsp;&nbsp;&nbsp;We firstly drop the features with high Spearman correlation, then we use Xgboost to select features with the highest importance score and DNN input perturbation ranking to select the most predictive features. \n","\n","### 4. Modelling <br>\n","&nbsp;&nbsp;&nbsp;&nbsp;We tried three models: Xgboost, LightGbm, DNN. When using Xgboost and LightGbm, we use grid search to find the best parameters. When use DNN, we use bayesian optimization to find the best sturcture. In addition, we try model ensembling. We finally find that Xgboost has the lowest log loss.\n","\n"]},{"cell_type":"code","metadata":{"id":"MM7Ffqc7jNfr","colab_type":"code","outputId":"e37a7f92-2c9a-4d45-800a-726068684dd5","executionInfo":{"status":"ok","timestamp":1573421136468,"user_tz":-480,"elapsed":8181,"user":{"displayName":"陆韡","photoUrl":"","userId":"00756817164624838101"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["import warnings \n","warnings.filterwarnings(action='ignore') \n","from itertools import combinations\n","import numpy as np \n","import pandas as pd \n","import csv\n","\n","from nltk.tokenize import RegexpTokenizer\n","tokenizer = RegexpTokenizer(r'[A-Za-z]+')\n","import nltk\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","from spacy.lang.en.stop_words import STOP_WORDS\n","import spacy\n","nlp = spacy.load(\"en\")\n","from collections import Counter\n","from tqdm import tqdm_notebook as tqdm\n","import pickle\n","import gensim\n","from gensim.models import Word2Vec \n","\n","from scipy.spatial.distance import cosine, cityblock, jaccard, canberra, euclidean, minkowski, braycurtis\n","from scipy.stats import zscore\n","\n","import xgboost as xgb\n","import lightgbm as lgb\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, log_loss\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.model_selection import GridSearchCV\n","\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Activation, Dropout, BatchNormalization\n","from tensorflow.keras.callbacks import EarlyStopping\n","from keras import optimizers"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"CSKkz_TwjSBu","colab_type":"code","outputId":"344e22f1-e80f-43e1-a461-416dbf3e128a","executionInfo":{"status":"ok","timestamp":1573421159955,"user_tz":-480,"elapsed":19686,"user":{"displayName":"陆韡","photoUrl":"","userId":"00756817164624838101"}},"colab":{"base_uri":"https://localhost:8080/","height":127}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"x4HSVGl6jUra","colab_type":"code","colab":{}},"source":["df_train = pd.read_csv('/content/drive//My Drive/data/train.csv')\n","df_test = pd.read_csv('/content/drive//My Drive/data/test.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7x1G8wQUjfki","colab_type":"text"},"source":["## **Pre-Processing**\n"]},{"cell_type":"code","metadata":{"id":"gyhvgxQnjd7z","colab_type":"code","colab":{}},"source":["TRAIN_PKL = '/content/drive//My Drive/data/train.pkl'\n","TRAIN_CSV = '/content/drive//My Drive/data/train.csv'\n","TEST_PKL = '/content/drive//My Drive/data/test.pkl'\n","TEST_CSV = '/content/drive//My Drive/data/test.csv'\n","\n","def token_pickle(file_name, pic_file_name):\n","    result = []\n","    with open(file_name, encoding=\"utf8\") as csvfile:\n","        readCSV = csv.reader(csvfile)\n","        i = 0\n","        next(readCSV) # Skip header\n","\n","        for row in tqdm(readCSV):\n","            \n","            # row: ['1', 'June – Moctezuma II, Aztec ruler of Tenochtitlan,...', 'The Swedish regent Sten Sture ...', '1']\n","            id = row[0]\n","            s1 = list(nlp(row[1]))\n","            s2 = list(nlp(row[2]))\n","            \n","            i += 1\n","            #if i>50: break\n","            \n","            s1 = [word.lemma_ for word in s1 if word.text.lower() not in STOP_WORDS and (len(word.text)>1 or word.text=='$') and word.pos_ not in ('ADP','DET','CCONJ','PRON','PART','INTJ','AUX')]\n","            s2 = [word.lemma_ for word in s2 if word.text.lower() not in STOP_WORDS and (len(word.text)>1 or word.text=='$') and word.pos_ not in ('ADP','DET','CCONJ','PRON','PART','INTJ','AUX')]\n","            \n","            \n","            if len(row)==4:\n","                result.append([id, s1, s2, int(row[3])])\n","            else:\n","                result.append([id, s1, s2])\n","\n","    with open(pic_file_name, 'wb') as handle:\n","        pickle.dump(result, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","#token_pickle(TRAIN_CSV, TRAIN_PKL)\n","#token_pickle(TEST_CSV, TEST_PKL)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vtLq76G_lLl1","colab_type":"code","colab":{}},"source":["with (open(TRAIN_PKL, \"rb\")) as openfile:\n","  train_data = pickle.load(openfile)\n","\n","td = pd.DataFrame(train_data)\n","td.columns = ['id', 'sent1_tradition', 'sent2_tradition', 'same_source']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CTXzntXDpzZe","colab_type":"code","colab":{}},"source":["td = pd.concat([df_train[['sent1','sent2']], td], axis=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"emUqGx3DlfzU","colab_type":"code","colab":{}},"source":["#  turn all the words into lowercase \n","tokenizer = RegexpTokenizer(r'[A-Za-z]+')\n","def pre_trained_embedding_preprocess(s):\n","    word = tokenizer.tokenize(s.lower())\n","    return word\n","\n","sent1_pre_trained = []\n","sent2_pre_trained = []\n","for i in range(td.shape[0]):\n","    sent1_pre_trained.append(pre_trained_embedding_preprocess(td.iat[i,0]))\n","    sent2_pre_trained.append(pre_trained_embedding_preprocess(td.iat[i,1]))\n","\n","td.insert(6,'sent1_pre_trained',sent1_pre_trained)\n","td.insert(7,'sent2_pre_trained',sent2_pre_trained)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EDaoFobnfJia","colab_type":"text"},"source":["## **Feature Engineering**"]},{"cell_type":"markdown","metadata":{"id":"_-Ku25CE-vJK","colab_type":"text"},"source":["#### **kaggle_help_2**\n","These features are from Kaggle_help_2\n","\n"]},{"cell_type":"code","metadata":{"id":"uRKDIogJ8ll0","colab_type":"code","colab":{}},"source":["def get_weight(count, eps=10000, min_count=2):\n","    return 0 if count < min_count else 1 / (count + eps)\n","\n","train_qs = pd.Series(df_train['sent1'].tolist() + df_train['sent2'].tolist()).astype(str)\n","words = (\" \".join(train_qs)).lower().split()\n","counts = Counter(words)\n","weights = {word: get_weight(count) for word, count in counts.items()}\n","\n","stops = set(stopwords.words(\"english\"))\n","def word_shares(row):\n","    q1 = set(str(row['sent1']).lower().split())\n","    q1words = q1.difference(stops)\n","    if len(q1words) == 0:\n","        return '0:0:0:0:0'\n","\n","    q2 = set(str(row['sent2']).lower().split())\n","    q2words = q2.difference(stops)\n","    if len(q2words) == 0:\n","        return '0:0:0:0:0'\n","\n","    q1stops = q1.intersection(stops)\n","    q2stops = q2.intersection(stops)\n","\n","    shared_words = q1words.intersection(q2words)\n","    shared_weights = [weights.get(w, 0) for w in shared_words] ## if no w, returns 0\n","    total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\n","    \n","    R1 = np.sum(shared_weights) / np.sum(total_weights) #tfidf share\n","    R2 = len(shared_words) / (len(q1words) + len(q2words)) #count share\n","    R31 = len(q1stops) / len(q1words) #stops in q1\n","    R32 = len(q2stops) / len(q2words) #stops in q2\n","    return '{}:{}:{}:{}:{}'.format(R1, R2, len(shared_words), R31, R32)\n","\n","df = pd.concat([df_train, df_test])\n","df['word_shares'] = df.apply(word_shares, axis=1, raw=True)\n","\n","x = pd.DataFrame()\n","\n","x['word_match']       = df['word_shares'].apply(lambda x: float(x.split(':')[0]))\n","x['tfidf_word_match'] = df['word_shares'].apply(lambda x: float(x.split(':')[1]))\n","x['shared_count']     = df['word_shares'].apply(lambda x: float(x.split(':')[2]))\n","\n","x['stops1_ratio']     = df['word_shares'].apply(lambda x: float(x.split(':')[3]))\n","x['stops2_ratio']     = df['word_shares'].apply(lambda x: float(x.split(':')[4]))\n","x['diff_stops_r']     = x['stops1_ratio'] - x['stops2_ratio']\n","\n","x['len_q1'] = df['sent1'].apply(lambda x: len(str(x)))\n","x['len_q2'] = df['sent2'].apply(lambda x: len(str(x)))\n","x['diff_len'] = x['len_q1'] - x['len_q2']\n","\n","x['len_char_q1'] = df['sent1'].apply(lambda x: len(str(x).replace(' ', '')))\n","x['len_char_q2'] = df['sent2'].apply(lambda x: len(str(x).replace(' ', '')))\n","x['diff_len_char'] = x['len_char_q1'] - x['len_char_q2']\n","\n","x['len_word_q1'] = df['sent1'].apply(lambda x: len(str(x).split()))\n","x['len_word_q2'] = df['sent2'].apply(lambda x: len(str(x).split()))\n","x['diff_len_word'] = x['len_word_q1'] - x['len_word_q2']\n","\n","x['avg_world_len1'] = x['len_char_q1'] / x['len_word_q1']\n","x['avg_world_len2'] = x['len_char_q2'] / x['len_word_q2']\n","x['diff_avg_word'] = x['avg_world_len1'] - x['avg_world_len2']\n","\n","x['exactly_same'] = (df['sent1'] == df['sent2']).astype(int)\n","x['duplicated'] = df.duplicated(['sent1','sent2']).astype(int)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QnNmxnNSMkBJ","colab_type":"text"},"source":["#### same words (lemma version)\n","Use lemma to preprocess all the words and then count the number of same words from two sentences."]},{"cell_type":"code","metadata":{"id":"5vazdHE2Mf4t","colab_type":"code","colab":{}},"source":["from nltk.stem.porter import PorterStemmer\n","tokenizer = RegexpTokenizer(r'[A-Za-z]+')\n","\n","def stem(row):\n","    ps = PorterStemmer()\n","    s1 = tokenizer.tokenize(td['sent1'][row].lower())\n","    s2 = tokenizer.tokenize(td['sent2'][row].lower())\n","\n","    s1 = [ps.stem(w) for w in s1 if (w not in STOP_WORDS) and len(w)>1]\n","    s2 = [ps.stem(w) for w in s2 if (w not in STOP_WORDS) and len(w)>1]\n","    return len(set(s1).intersection(set(s2)))\n","\n","stemming = pd.DataFrame()\n","same_word_stem = []\n","for i in range(td.shape[0]):\n","    same_word_stem.append(stem(i))\n","stemming['same_words_stem'] = pd.Series(same_word_stem)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yTcpUG3xbtJg","colab_type":"code","outputId":"a7f5d43a-1614-49ad-b144-4f09c10fc758","executionInfo":{"status":"ok","timestamp":1573421328122,"user_tz":-480,"elapsed":632,"user":{"displayName":"陆韡","photoUrl":"","userId":"00756817164624838101"}},"colab":{"base_uri":"https://localhost:8080/","height":141}},"source":["pd.concat([stemming['same_words_stem'], td['same_source']], axis=1).groupby('same_source').mean()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>same_words_stem</th>\n","    </tr>\n","    <tr>\n","      <th>same_source</th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.093188</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.574450</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["             same_words_stem\n","same_source                 \n","0                   0.093188\n","1                   0.574450"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"uLhl9twOxtft","colab_type":"text"},"source":["#### dummy \n"]},{"cell_type":"markdown","metadata":{"id":"OkU1ezdhfsI_","colab_type":"text"},"source":["##### (1) 100 common words\n","We select the most common 100 words and then turn them into dummy.\n","Eg: \"age\" is the most common words, if sent1 contains \"age\", then 1, else 0."]},{"cell_type":"code","metadata":{"id":"IKco0gSasIpG","colab_type":"code","colab":{}},"source":["train_qs = pd.Series(df_train['sent1'].tolist() + df_train['sent2'].tolist()).astype(str)\n","words = (\" \".join(train_qs)).lower().split()\n","counts = Counter(words)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"osqdUSVSsKRS","colab_type":"code","colab":{}},"source":["for stop_words in STOP_WORDS:\n","    del counts[stop_words]\n","most_common = counts.most_common(100)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RrYBg6MO3ykx","colab_type":"code","colab":{}},"source":["dummy = pd.DataFrame(data=td.index).drop(columns=0)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"I5U2PPHB7any","colab_type":"code","colab":{}},"source":["for i in range(len(most_common)):\n","    dummy[most_common[i][0]] = 0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vr7cog5XwJe1","colab_type":"code","colab":{}},"source":["for row in range(td.shape[0]):\n","    s1 = tokenizer.tokenize(td['sent1'][row])\n","    s2 = tokenizer.tokenize(td['sent2'][row])\n","    flag1, flag2 = 0, 0\n","    for i in range(len(most_common)):\n","        for word in s1:\n","            if word.lower() == most_common[i][0]:\n","                flag1 = 1\n","        for word in s2:\n","            if word.lower() == most_common[i][0]:\n","                flag2 = 1\n","        if (flag1 == 1) and (flag2 == 1):\n","            dummy.iat[row,i] = 1\n","        else:\n","            dummy.iat[row,i] = 0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DIypL7hi-XMF","colab_type":"code","outputId":"243ffbbf-c4a2-4f52-ba8d-3674d8fcc13a","executionInfo":{"status":"ok","timestamp":1573157113974,"user_tz":-480,"elapsed":1489,"user":{"displayName":"陆韡","photoUrl":"","userId":"00756817164624838101"}},"colab":{"base_uri":"https://localhost:8080/","height":190}},"source":["grp = pd.concat([td['same_source'], dummy], axis=1).groupby('same_source').mean()\n","grp"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>age</th>\n","      <th>population</th>\n","      <th>income</th>\n","      <th>18</th>\n","      <th>average</th>\n","      <th>median</th>\n","      <th>living</th>\n","      <th>city</th>\n","      <th>united</th>\n","      <th>size</th>\n","      <th>family</th>\n","      <th>census</th>\n","      <th>density</th>\n","      <th>household</th>\n","      <th>county</th>\n","      <th>families</th>\n","      <th>100</th>\n","      <th>township</th>\n","      <th>new</th>\n","      <th>males.</th>\n","      <th>town</th>\n","      <th>area</th>\n","      <th>square</th>\n","      <th>65</th>\n","      <th>mile</th>\n","      <th>american,</th>\n","      <th>states</th>\n","      <th>years</th>\n","      <th>including</th>\n","      <th>people</th>\n","      <th>according</th>\n","      <th>children</th>\n","      <th>present,</th>\n","      <th>married</th>\n","      <th>makeup</th>\n","      <th>householder</th>\n","      <th>,</th>\n","      <th>located</th>\n","      <th>units</th>\n","      <th>them,</th>\n","      <th>...</th>\n","      <th>known</th>\n","      <th>2010</th>\n","      <th>capita</th>\n","      <th>males</th>\n","      <th>18,</th>\n","      <th>older.</th>\n","      <th>45</th>\n","      <th>24,</th>\n","      <th>females.</th>\n","      <th>over,</th>\n","      <th>village</th>\n","      <th>versus</th>\n","      <th>spread</th>\n","      <th>64,</th>\n","      <th>poverty</th>\n","      <th>44,</th>\n","      <th>line,</th>\n","      <th>african</th>\n","      <th>population.</th>\n","      <th>race</th>\n","      <th>north</th>\n","      <th>land</th>\n","      <th>2000,</th>\n","      <th>bureau,</th>\n","      <th>hispanic</th>\n","      <th>latino</th>\n","      <th>females,</th>\n","      <th>states.</th>\n","      <th>south</th>\n","      <th>national</th>\n","      <th>called</th>\n","      <th>early</th>\n","      <th>named</th>\n","      <th>over.</th>\n","      <th>time</th>\n","      <th>races,</th>\n","      <th>census.</th>\n","      <th>high</th>\n","      <th>asian,</th>\n","      <th>year</th>\n","    </tr>\n","    <tr>\n","      <th>same_source</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.003627</td>\n","      <td>0.013113</td>\n","      <td>0.025467</td>\n","      <td>0.025467</td>\n","      <td>0.037573</td>\n","      <td>0.037588</td>\n","      <td>0.038456</td>\n","      <td>0.055088</td>\n","      <td>0.069100</td>\n","      <td>0.069999</td>\n","      <td>0.073084</td>\n","      <td>0.081810</td>\n","      <td>0.082074</td>\n","      <td>0.082182</td>\n","      <td>0.098814</td>\n","      <td>0.099682</td>\n","      <td>0.099682</td>\n","      <td>0.105929</td>\n","      <td>0.120375</td>\n","      <td>0.120375</td>\n","      <td>0.131659</td>\n","      <td>0.141502</td>\n","      <td>0.142354</td>\n","      <td>0.142354</td>\n","      <td>0.142959</td>\n","      <td>0.142959</td>\n","      <td>0.145129</td>\n","      <td>0.154119</td>\n","      <td>0.160645</td>\n","      <td>0.166550</td>\n","      <td>0.171014</td>\n","      <td>0.173014</td>\n","      <td>0.173014</td>\n","      <td>0.174626</td>\n","      <td>0.179385</td>\n","      <td>0.179400</td>\n","      <td>0.179400</td>\n","      <td>0.189770</td>\n","      <td>0.191025</td>\n","      <td>0.191025</td>\n","      <td>...</td>\n","      <td>0.235573</td>\n","      <td>0.235573</td>\n","      <td>0.235666</td>\n","      <td>0.235883</td>\n","      <td>0.235883</td>\n","      <td>0.235883</td>\n","      <td>0.235883</td>\n","      <td>0.235883</td>\n","      <td>0.235883</td>\n","      <td>0.235883</td>\n","      <td>0.238813</td>\n","      <td>0.238906</td>\n","      <td>0.239603</td>\n","      <td>0.239603</td>\n","      <td>0.239743</td>\n","      <td>0.239743</td>\n","      <td>0.239743</td>\n","      <td>0.24089</td>\n","      <td>0.24089</td>\n","      <td>0.242192</td>\n","      <td>0.248640</td>\n","      <td>0.251740</td>\n","      <td>0.251740</td>\n","      <td>0.251740</td>\n","      <td>0.251802</td>\n","      <td>0.251802</td>\n","      <td>0.251802</td>\n","      <td>0.251802</td>\n","      <td>0.256219</td>\n","      <td>0.263055</td>\n","      <td>0.272526</td>\n","      <td>0.279439</td>\n","      <td>0.284957</td>\n","      <td>0.284957</td>\n","      <td>0.297915</td>\n","      <td>0.297915</td>\n","      <td>0.297915</td>\n","      <td>0.302798</td>\n","      <td>0.302798</td>\n","      <td>0.311943</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.008478</td>\n","      <td>0.029177</td>\n","      <td>0.058647</td>\n","      <td>0.058647</td>\n","      <td>0.091954</td>\n","      <td>0.091985</td>\n","      <td>0.092279</td>\n","      <td>0.122941</td>\n","      <td>0.147368</td>\n","      <td>0.147832</td>\n","      <td>0.149116</td>\n","      <td>0.167572</td>\n","      <td>0.167664</td>\n","      <td>0.167757</td>\n","      <td>0.196872</td>\n","      <td>0.197584</td>\n","      <td>0.197584</td>\n","      <td>0.212388</td>\n","      <td>0.221330</td>\n","      <td>0.221330</td>\n","      <td>0.239801</td>\n","      <td>0.251156</td>\n","      <td>0.251775</td>\n","      <td>0.251775</td>\n","      <td>0.252425</td>\n","      <td>0.252425</td>\n","      <td>0.253616</td>\n","      <td>0.257948</td>\n","      <td>0.261135</td>\n","      <td>0.264151</td>\n","      <td>0.266503</td>\n","      <td>0.267400</td>\n","      <td>0.267400</td>\n","      <td>0.268050</td>\n","      <td>0.277146</td>\n","      <td>0.277146</td>\n","      <td>0.277146</td>\n","      <td>0.291363</td>\n","      <td>0.291951</td>\n","      <td>0.291951</td>\n","      <td>...</td>\n","      <td>0.345400</td>\n","      <td>0.345400</td>\n","      <td>0.345431</td>\n","      <td>0.345508</td>\n","      <td>0.345508</td>\n","      <td>0.345508</td>\n","      <td>0.345508</td>\n","      <td>0.345508</td>\n","      <td>0.345508</td>\n","      <td>0.345508</td>\n","      <td>0.349453</td>\n","      <td>0.349484</td>\n","      <td>0.349855</td>\n","      <td>0.349855</td>\n","      <td>0.350010</td>\n","      <td>0.350010</td>\n","      <td>0.350010</td>\n","      <td>0.35100</td>\n","      <td>0.35100</td>\n","      <td>0.351882</td>\n","      <td>0.357884</td>\n","      <td>0.361195</td>\n","      <td>0.361195</td>\n","      <td>0.361195</td>\n","      <td>0.361226</td>\n","      <td>0.361257</td>\n","      <td>0.361257</td>\n","      <td>0.361257</td>\n","      <td>0.365527</td>\n","      <td>0.371730</td>\n","      <td>0.377052</td>\n","      <td>0.381801</td>\n","      <td>0.387169</td>\n","      <td>0.387169</td>\n","      <td>0.394749</td>\n","      <td>0.394749</td>\n","      <td>0.394749</td>\n","      <td>0.397317</td>\n","      <td>0.397317</td>\n","      <td>0.403738</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2 rows × 100 columns</p>\n","</div>"],"text/plain":["                  age  population    income  ...      high    asian,      year\n","same_source                                  ...                              \n","0            0.003627    0.013113  0.025467  ...  0.302798  0.302798  0.311943\n","1            0.008478    0.029177  0.058647  ...  0.397317  0.397317  0.403738\n","\n","[2 rows x 100 columns]"]},"metadata":{"tags":[]},"execution_count":54}]},{"cell_type":"code","metadata":{"id":"RUJeVqHqc8re","colab_type":"code","outputId":"15275eb2-3a09-483b-9568-13731436bb6d","executionInfo":{"status":"ok","timestamp":1573157708858,"user_tz":-480,"elapsed":1157,"user":{"displayName":"陆韡","photoUrl":"","userId":"00756817164624838101"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["for i in range(grp.shape[1]):\n","    print( abs(grp.iat[0,i]-grp.iat[1,i])/max(grp.iat[0,i],grp.iat[0,i]), grp.columns[i], grp.iat[1,i]/grp.iat[0,i] )"],"execution_count":0,"outputs":[{"output_type":"stream","text":["1.3373154848534252 age 2.337315484853425\n","1.2249689803399701 population 2.22496898033997\n","1.3028669990365722 income 2.3028669990365724\n","1.3028669990365722 18 2.3028669990365724\n","1.4473654261977922 average 2.447365426197792\n","1.4471793379995492 median 2.4471793379995495\n","1.3995859973118763 living 2.3995859973118763\n","1.231713418930895 city 2.231713418930895\n","1.1326674878914529 united 2.1326674878914527\n","1.1119072595664254 size 2.111907259566425\n","1.0403417599869595 family 2.0403417599869593\n","1.0482921504745488 census 2.048292150474549\n","1.0428468885441067 density 2.0428468885441067\n","1.0412792492413092 household 2.041279249241309\n","0.9923441778437834 county 1.9923441778437834\n","0.9821341112268187 families 1.9821341112268187\n","0.9821341112268187 100 1.9821341112268187\n","1.0050101051802298 township 2.00501010518023\n","0.8386701526447685 new 1.8386701526447684\n","0.8386701526447685 males. 1.8386701526447684\n","0.8213780348573291 town 1.8213780348573292\n","0.7749320073678959 area 1.774932007367896\n","0.7686494257590044 square 1.7686494257590044\n","0.7686494257590044 65 1.7686494257590044\n","0.7657155436002747 mile 1.7657155436002747\n","0.7657155436002747 american, 1.7657155436002747\n","0.7475215601971048 states 1.7475215601971048\n","0.6736898908266841 years 1.6736898908266842\n","0.6255400466684653 including 1.6255400466684653\n","0.586013675602997 people 1.586013675602997\n","0.5583631525250675 according 1.5583631525250676\n","0.5455391154477128 children 1.545539115447713\n","0.5455391154477128 present, 1.545539115447713\n","0.5349925207777388 married 1.5349925207777388\n","0.5449822512267245 makeup 1.5449822512267246\n","0.5448487639058998 householder 1.5448487639058996\n","0.5448487639058998 , 1.5448487639058996\n","0.5353498532700471 located 1.5353498532700471\n","0.528336107022793 units 1.528336107022793\n","0.528336107022793 them, 1.528336107022793\n","0.5289426100223861 housing 1.5289426100223862\n","0.5244128462535432 female 1.5244128462535431\n","0.5244128462535432 households, 1.5244128462535431\n","0.5244128462535432 together, 1.5244128462535431\n","0.5241625438850595 couples 1.5241625438850595\n","0.5229473834394976 husband 1.5229473834394975\n","0.5229473834394976 people, 1.5229473834394975\n","0.5229473834394976 non-families. 1.5229473834394975\n","0.5213855951179485 racial 1.5213855951179485\n","0.5221076575741032 households 1.5221076575741033\n","0.51736563282677 total 1.51736563282677\n","0.5176845243589145 residing 1.5176845243589145\n","0.5176845243589145 county, 1.5176845243589145\n","0.494778620482818 state 1.494778620482818\n","0.5176833944734498 females 1.5176833944734498\n","0.5176833944734498 white, 1.5176833944734498\n","0.5176833944734498 races. 1.5176833944734498\n","0.5176833944734498 25 1.5176833944734498\n","0.5164946004518984 native 1.5164946004518984\n","0.5097603241847078 school 1.509760324184708\n","0.466211317458147 known 1.466211317458147\n","0.466211317458147 2010 1.466211317458147\n","0.46576399002067714 capita 1.4657639900206771\n","0.4647434589392839 males 1.4647434589392838\n","0.4647434589392839 18, 1.4647434589392838\n","0.4647434589392839 older. 1.4647434589392838\n","0.4647434589392839 45 1.4647434589392838\n","0.4647434589392839 24, 1.4647434589392838\n","0.4647434589392839 females. 1.4647434589392838\n","0.4647434589392839 over, 1.4647434589392838\n","0.46329388623249285 village 1.463293886232493\n","0.46285376024975505 versus 1.462853760249755\n","0.4601447939756099 spread 1.4601447939756098\n","0.4601447939756099 64, 1.4601447939756098\n","0.45994043660711664 poverty 1.4599404366071167\n","0.45994043660711664 44, 1.4599404366071167\n","0.45994043660711664 line, 1.4599404366071167\n","0.4570988985558814 african 1.4570988985558815\n","0.4570988985558814 population. 1.4570988985558815\n","0.452906424096162 race 1.452906424096162\n","0.4393682798031001 north 1.4393682798031\n","0.43479400539902835 land 1.4347940053990282\n","0.43479400539902835 2000, 1.4347940053990282\n","0.43479400539902835 bureau, 1.4347940053990282\n","0.43456359145774565 hispanic 1.4345635914577457\n","0.43468646628356633 latino 1.4346864662835663\n","0.43468646628356633 females, 1.4346864662835663\n","0.43468646628356633 states. 1.4346864662835663\n","0.4266148613460281 south 1.426614861346028\n","0.41312603959191135 national 1.4131260395919114\n","0.38354518733552706 called 1.3835451873355271\n","0.36631312796545124 early 1.3663131279654512\n","0.3586932489718368 named 1.3586932489718369\n","0.3586932489718368 over. 1.3586932489718369\n","0.3250396237115322 time 1.3250396237115323\n","0.3250396237115322 races, 1.3250396237115323\n","0.3250396237115322 census. 1.3250396237115323\n","0.3121544917332339 high 1.312154491733234\n","0.3121544917332339 asian, 1.312154491733234\n","0.29426728281696496 year 1.294267282816965\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AdR6__HytboE","colab_type":"code","outputId":"a3e28b14-5e34-4d7f-ef82-bd9402708545","executionInfo":{"status":"ok","timestamp":1573173342377,"user_tz":-480,"elapsed":1608,"user":{"displayName":"陆韡","photoUrl":"","userId":"00756817164624838101"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["counts.most_common(100)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('age', 22533),\n"," ('population', 16108),\n"," ('income', 14905),\n"," ('18', 14700),\n"," ('average', 14041),\n"," ('median', 11793),\n"," ('living', 10268),\n"," ('city', 10229),\n"," ('united', 9453),\n"," ('size', 9427),\n"," ('family', 9337),\n"," ('census', 8420),\n"," ('density', 8355),\n"," ('household', 8312),\n"," ('county', 8261),\n"," ('families', 8016),\n"," ('100', 7767),\n"," ('township', 7698),\n"," ('new', 7538),\n"," ('males.', 7296),\n"," ('town', 7194),\n"," ('area', 6648),\n"," ('square', 6400),\n"," ('65', 6386),\n"," ('mile', 6048),\n"," ('american,', 5994),\n"," ('states', 5924),\n"," ('years', 5867),\n"," ('including', 5848),\n"," ('people', 5581),\n"," ('according', 5265),\n"," ('children', 5179),\n"," ('present,', 5179),\n"," ('married', 5143),\n"," ('makeup', 5126),\n"," ('householder', 5110),\n"," (',', 5072),\n"," ('located', 5027),\n"," ('units', 4878),\n"," ('them,', 4813),\n"," ('housing', 4794),\n"," ('female', 4737),\n"," ('households,', 4685),\n"," ('together,', 4666),\n"," ('couples', 4622),\n"," ('husband', 4600),\n"," ('people,', 4450),\n"," ('non-families.', 4424),\n"," ('racial', 4388),\n"," ('households', 4384),\n"," ('total', 4355),\n"," ('residing', 4265),\n"," ('county,', 4251),\n"," ('state', 4238),\n"," ('females', 4125),\n"," ('white,', 4032),\n"," ('races.', 4020),\n"," ('25', 3928),\n"," ('native', 3861),\n"," ('school', 3833),\n"," ('known', 3797),\n"," ('2010', 3795),\n"," ('capita', 3785),\n"," ('males', 3708),\n"," ('18,', 3695),\n"," ('older.', 3677),\n"," ('45', 3664),\n"," ('24,', 3650),\n"," ('females.', 3619),\n"," ('over,', 3607),\n"," ('village', 3599),\n"," ('versus', 3596),\n"," ('spread', 3595),\n"," ('64,', 3572),\n"," ('poverty', 3550),\n"," ('44,', 3533),\n"," ('line,', 3508),\n"," ('african', 3477),\n"," ('population.', 3439),\n"," ('race', 3411),\n"," ('north', 3405),\n"," ('land', 3389),\n"," ('2000,', 3361),\n"," ('bureau,', 3345),\n"," ('hispanic', 3343),\n"," ('latino', 3301),\n"," ('females,', 3271),\n"," ('states.', 3215),\n"," ('south', 3150),\n"," ('national', 3126),\n"," ('called', 3105),\n"," ('early', 3090),\n"," ('named', 3071),\n"," ('over.', 2965),\n"," ('time', 2881),\n"," ('races,', 2876),\n"," ('census.', 2872),\n"," ('high', 2844),\n"," ('asian,', 2835),\n"," ('year', 2805)]"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"S8lfb1J0habS","colab_type":"code","colab":{}},"source":["#td[td['sent1'].str.contains('population')][['sent1','sent2','same_source']].to_excel('/content/drive//My Drive/data/median_income.xlsx', index=False)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LB5NSk39FhaH","colab_type":"text"},"source":["##### (2) % $ 0-9\n","if contains \"%\" or \"$\" or 0-9, then turn it to dummy."]},{"cell_type":"code","metadata":{"id":"buqjsEkHFlz6","colab_type":"code","colab":{}},"source":["def token_pickle(file_name, pic_file_name):\n","    result = []\n","    with open(file_name, encoding=\"utf8\") as csvfile:\n","        readCSV = csv.reader(csvfile)\n","        i = 0\n","        next(readCSV) # Skip header\n","\n","        for row in tqdm(readCSV):\n","            \n","            id = row[0]\n","            s1 = list(nlp(row[1]))\n","            s2 = list(nlp(row[2]))\n","            \n","            i += 1\n","            \n","            #if word.text=='$':s1='$'\n","            #if word.text=='$':s2='$'\n","            #s1 = ['$' if word.text=='$']\n","            #s2 = ['$' if word.text=='$']\n","            #s1 = len(['%' for word in s1 if word.text=='%' ])\n","            #s2 = len(['%' for word in s2 if word.text=='%' ])\n","            s1 = len(['%' for word in s1 if is_number(replace_all_blank(word.text))==1 ])\n","            s2 = len(['%' for word in s2 if is_number(replace_all_blank(word.text))==1 ])\n","\n","            \n","            if len(row)==4:\n","                result.append([id, s1, s2, int(row[3])])\n","            else:\n","                result.append([id, s1, s2])\n","\n","    with open(pic_file_name, 'wb') as handle:\n","        pickle.dump(result, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TOLiXQII3wmv","colab_type":"text"},"source":["##### categorical words\n","We define 20 categories, and add words to it. If sentences contains catorical words, then 1, else 0. (a manual process)"]},{"cell_type":"code","metadata":{"id":"VPdVUAxqRo80","colab_type":"code","colab":{}},"source":["discipline = ['census','median','income','household','population','per','capita','racial','males','females','housing','community']\n","people = ['racial','makeup','gender','male','female','every','age','children','marry','family','couple','house','writer']\n","industry = ['energy','manufacture','market','power','electric','weapon','infrastructure','sustainable','transport','vehicle','product','production','factory']\n","business = ['finance','commercial','income','stock','CDP']\n","culture = ['philosophy','religion','nations','language','humanities','literature','ritual','linguistic','rhetoric']\n","economy = ['economies','economics','consumer','labor','money','trade','wealth','capital','GDP']\n","education = ['academic','curricula','literacy','literature','research','student','school','university','tutor','professor','teach','train','knowledge','learn','information','wisdom','research','scholar','understand','intellectual','logic','cognitive']\n","entertainment = ['recreation','amuse','movie','comedy','theater','media','film','game','humor','magic','art','television','toy','food','drink','cuisine','dairy','theme','visual','club','bar','sport','player','gamble','card']\n","art = ['opera','genre','paint','library','comedy','tragedy','drama','dance','artistic','artist','sculptures']\n","society = ['location','accident','conflict','event','emergency','news','individual','behavior','human','communication','personal','affair','violence','gun','crime','kill','shoot','injure','information','sociology','people','community','facility','rail','station','housing','violent','place']\n","geography = ['environmental','ecosystem','land','region','zone','decay','plate','cluster','curve','rock','lithic','map','mountain','soil','landscape','island','equator','earth','contour','local','phenomena','geological','border','ocean','bay','area','rocky','peninsula','hemisphere','locate','island','regional','cloud','rain','rainfall','lake','park','Asia','Africa','English','resort']\n","health = ['patient','treat','medical','transcript','healthcare','alcohol','diet','disability','disease','nutrition','fitness','heal','injury','death','life']\n","history = ['period','era','epoch','historian','chronology']\n","law = ['legislation','judicial','just','bill','act','constitution','legal','right','state','abrogation','legislative','judge']\n","mathematics = ['formula','equation','theorem','mathematical','statistics','distribution','mean','medium','median','max','min','probability','possibility','sample','data','analysis','rate','more','average']\n","statistics = ['distribution','mean','medium','median','max','min','probability','possibility','sample','data','analysis','rate','per capita','more','average']\n","military = ['weapon','missile','force','army','corp','kill','armed','assault','attack','force','war','ink']\n","nature = ['animal','ecosystem','earth','natural','life','phenomena','river','forest','village','mountain','hiking','cableway','hamlet','goose','wild']\n","government  = ['citizen','politic','citizenship','agency','civil','service','election','vote','debt','tax','public','policy','presidency','administration','office','state','enforce','compulsory','constitution','hierarchical','parliament','congress','council','policy','agenda','regime','senator','democracy','field','committee','scandal','reform','political','society','republic','people','leader','population','county','portion','commonwealth','census','international','racial','race','electoral','congress','communist','capital','sign','bureau','legislative','authority','consolidate','execution','federal','enforce','diplomatic','judicial','senate','treaty','reform','county','president','holy','revolutionaries','metropolitan','hotel','cabins','Norway','religion','religious','baptism','monasticism','monk','church']\n","technology = ['science','develop','technique','information','fiction','computer','scientific','satellite','site','technologies','ray','radiate','radiation','unit','refract','frequency','charge','ion','atom','thermal','voltage','particle','chip','optical','electronic','properties','analysis','amplifier']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2GH4pcUfxxd1","colab_type":"code","colab":{}},"source":["# numbers?\n","dummy = pd.DataFrame()\n","\n","def is_number(s):\n","    try:\n","        float(s)\n","        return True\n","    except ValueError:\n","        pass\n","    try:\n","        import unicodedata\n","        unicodedata.numeric(s)\n","        return True\n","    except (TypeError, ValueError):\n","        pass\n","    return False\n","\n","def dummys(row):\n","    sent1, sent2, a, b = 0, 0, 0, 0\n","    sent11, sent22, c, d = 0, 0, 0, 0\n","\n","    dis, ind, peo, bus, cul, eco, edu, ent, artt, soc, geo = 0,0,0,0,0,0,0,0,0,0,0\n","    gov, hea, his, laww, mat, sta, mil, nat, tec = 0,0,0,0,0,0,0,0,0\n","    dis1, ind1, peo1, bus1, cul1, eco1, edu1, ent1, art1, soc1, geo1 = 0,0,0,0,0,0,0,0,0,0,0\n","    gov1, hea1, his1, law1, mat1, sta1, mil1, nat1, tec1 = 0,0,0,0,0,0,0,0,0\n","    dis2, ind2, peo2, bus2, cul2, eco2, edu2, ent2, art2, soc2, geo2 = 0,0,0,0,0,0,0,0,0,0,0\n","    gov2, hea2, his2, law2, mat2, sta2, mil2, nat2, tec2 = 0,0,0,0,0,0,0,0,0\n","\n","    for word in row['sent1_tradition']:\n","        if is_number(word): sent1=1\n","        if word=='$': sent11=1 \n","\n","        if word in (discipline): dis1=1\n","        if word in (industry): ind1=1\n","        if word in (people): peo1=1\n","        if word in (business): bus1=1\n","        if word in (culture): cul1=1\n","        if word in (economy): eco1=1\n","        if word in (education): edu1=1\n","        if word in (entertainment): ent1=1\n","        if word in (art): art1=1\n","        if word in (society): soc1=1\n","        if word in (geography): geo1=1\n","        if word in (government): gov1=1\n","        if word in (history): his1=1\n","        if word in (health): hea1=1\n","        if word in (law): law1=1\n","        if word in (mathematics): mat1=1\n","        if word in (statistics): sta1=1\n","        if word in (military): mil1=1\n","        if word in (nature): nat1=1\n","        if word in (technology): tec1=1\n","\n","\n","    for word in row['sent2_tradition']:\n","        if is_number(word): sent2=1 \n","        if word=='$': sent22=1 \n","\n","        if word in (discipline): dis2=1\n","        if word in (industry): ind2=1\n","        if word in (people): peo2=1\n","        if word in (business): bus2=1\n","        if word in (culture): cul2=1\n","        if word in (economy): eco2=1\n","        if word in (education): edu2=1\n","        if word in (entertainment): ent2=1\n","        if word in (art): art2=1\n","        if word in (society): soc2=1\n","        if word in (geography): geo2=1\n","        if word in (government): gov2=1\n","        if word in (health): hea2=1\n","        if word in (history): his2=1\n","        if word in (law): law2=1\n","        if word in (mathematics): mat2=1\n","        if word in (statistics): sta2=1\n","        if word in (military): mil2=1\n","        if word in (nature): nat2=1\n","        if word in (technology): tec2=1\n","\n","    # both contain number?\n","    a=0 if (sent1 == 1 and sent2 == 1) or (sent1 == 0 and sent2 == 0) else 1\n","    b=0 if (sent1 == 1 and sent2 == 1) else 1\n","\n","    # contain $?\n","    c=0 if (sent11 == 1 and sent22 == 1) or (sent11 == 0 and sent22 == 0) else 1\n","\n","    # category words?\n","    dis=1 if (dis1 == 1 and dis2 == 1) else 0\n","    ind=1 if (ind1 == 1 and ind2 == 1) else 0\n","    peo=1 if (peo1 == 1 and peo2 == 1) else 0\n","    bus=1 if (bus1 == 1 and bus2 == 1) else 0\n","    cul=1 if (cul1 == 1 and cul2 == 1) else 0\n","    eco=1 if (eco1 == 1 and eco2 == 1) else 0\n","    edu=1 if (edu1 == 1 and edu2 == 1) else 0\n","    ent=1 if (ent1 == 1 and ent2 == 1) else 0\n","    artt=1 if (art1 == 1 and art2 == 1) else 0\n","    soc=1 if (soc1 == 1 and soc2 == 1) else 0\n","    geo=1 if (geo1 == 1 and geo2 == 1) else 0\n","    gov=1 if (gov1 == 1 and gov2 == 1) else 0\n","    hea=1 if (hea1 == 1 and hea2 == 1) else 0\n","    his=1 if (his1 == 1 and his2 == 1) else 0\n","    laww=1 if (law1 == 1 and law2 == 1) else 0\n","    mat=1 if (mat1 == 1 and mat2 == 1) else 0\n","    sta=1 if (sta1 == 1 and sta2 == 1) else 0\n","    mil=1 if (mil1 == 1 and mil2 == 1) else 0\n","    nat=1 if (nat1 == 1 and nat2 == 1) else 0\n","    tec=1 if (tec1 == 1 and tec2 == 1) else 0\n","\n","    return '{}:{}:{}:{}:{}:{}:{}:{}:{}:{}:{}:{}:{}:{}:{}:{}:{}:{}:{}:{}:{}:{}:{}'.format(a, b, c, dis, ind, peo, bus, cul, eco, edu, ent, artt, soc, geo, gov, hea, his, laww, mat, sta, mil, nat, tec)\n","\n","dummy['dummys'] = td.apply(dummys, axis=1)\n","dummy['both_contain_number'] = dummy['dummys'].apply(lambda x: float(x.split(':')[0]))\n","dummy['contain_number'] = dummy['dummys'].apply(lambda x: float(x.split(':')[1]))\n","dummy['contain_$'] = dummy['dummys'].apply(lambda x: float(x.split(':')[2]))\n","\n","dummy['discipline'] = dummy['dummys'].apply(lambda x: float(x.split(':')[3]))\n","dummy['industry'] = dummy['dummys'].apply(lambda x: float(x.split(':')[4]))\n","dummy['people'] = dummy['dummys'].apply(lambda x: float(x.split(':')[5]))\n","dummy['business'] = dummy['dummys'].apply(lambda x: float(x.split(':')[6]))\n","dummy['culture'] = dummy['dummys'].apply(lambda x: float(x.split(':')[7]))\n","dummy['economy'] = dummy['dummys'].apply(lambda x: float(x.split(':')[8]))\n","dummy['education'] = dummy['dummys'].apply(lambda x: float(x.split(':')[9]))\n","dummy['entertainment'] = dummy['dummys'].apply(lambda x: float(x.split(':')[10]))\n","dummy['art'] = dummy['dummys'].apply(lambda x: float(x.split(':')[11]))\n","dummy['society'] = dummy['dummys'].apply(lambda x: float(x.split(':')[12]))\n","dummy['geography'] = dummy['dummys'].apply(lambda x: float(x.split(':')[13]))\n","dummy['government'] = dummy['dummys'].apply(lambda x: float(x.split(':')[14]))\n","dummy['health'] = dummy['dummys'].apply(lambda x: float(x.split(':')[15]))\n","dummy['law'] = dummy['dummys'].apply(lambda x: float(x.split(':')[16]))\n","dummy['history'] = dummy['dummys'].apply(lambda x: float(x.split(':')[17]))\n","dummy['mathematics'] = dummy['dummys'].apply(lambda x: float(x.split(':')[18]))\n","dummy['statistics'] = dummy['dummys'].apply(lambda x: float(x.split(':')[19]))\n","dummy['military'] = dummy['dummys'].apply(lambda x: float(x.split(':')[20]))\n","dummy['nature'] = dummy['dummys'].apply(lambda x: float(x.split(':')[21]))\n","dummy['technology'] = dummy['dummys'].apply(lambda x: float(x.split(':')[22]))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"enDljd3fEl4x","colab_type":"code","colab":{}},"source":["dummy.drop(columns='dummys', inplace=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FBDIYereI7r0","colab_type":"code","outputId":"9d8fc6ca-03e2-4665-b966-5f8ede683caa","executionInfo":{"status":"ok","timestamp":1573176860416,"user_tz":-480,"elapsed":1174,"user":{"displayName":"陆韡","photoUrl":"","userId":"00756817164624838101"}},"colab":{"base_uri":"https://localhost:8080/","height":161}},"source":["pd.concat([dummy, td['same_source']], axis=1).groupby('same_source').mean()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>both_contain_number</th>\n","      <th>contain_number</th>\n","      <th>contain_$</th>\n","      <th>discipline</th>\n","      <th>industry</th>\n","      <th>people</th>\n","      <th>business</th>\n","      <th>culture</th>\n","      <th>economy</th>\n","      <th>education</th>\n","      <th>entertainment</th>\n","      <th>art</th>\n","      <th>society</th>\n","      <th>geography</th>\n","      <th>government</th>\n","      <th>health</th>\n","      <th>law</th>\n","      <th>history</th>\n","      <th>mathematics</th>\n","      <th>statistics</th>\n","      <th>military</th>\n","      <th>nature</th>\n","      <th>technology</th>\n","    </tr>\n","    <tr>\n","      <th>same_source</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.484647</td>\n","      <td>0.831760</td>\n","      <td>0.087887</td>\n","      <td>0.0</td>\n","      <td>0.000589</td>\n","      <td>0.012292</td>\n","      <td>0.011796</td>\n","      <td>0.000093</td>\n","      <td>0.000016</td>\n","      <td>0.000775</td>\n","      <td>0.001395</td>\n","      <td>0.000047</td>\n","      <td>0.005379</td>\n","      <td>0.002139</td>\n","      <td>0.034519</td>\n","      <td>0.000233</td>\n","      <td>0.000202</td>\n","      <td>0.001132</td>\n","      <td>0.000016</td>\n","      <td>0.006929</td>\n","      <td>0.000806</td>\n","      <td>0.000124</td>\n","      <td>0.000295</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.392909</td>\n","      <td>0.781609</td>\n","      <td>0.084018</td>\n","      <td>0.0</td>\n","      <td>0.002305</td>\n","      <td>0.030290</td>\n","      <td>0.031219</td>\n","      <td>0.001361</td>\n","      <td>0.000356</td>\n","      <td>0.003264</td>\n","      <td>0.006064</td>\n","      <td>0.000650</td>\n","      <td>0.005971</td>\n","      <td>0.004038</td>\n","      <td>0.050773</td>\n","      <td>0.001129</td>\n","      <td>0.000650</td>\n","      <td>0.002816</td>\n","      <td>0.000263</td>\n","      <td>0.012546</td>\n","      <td>0.003991</td>\n","      <td>0.000774</td>\n","      <td>0.001841</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["             both_contain_number  contain_number  ...    nature  technology\n","same_source                                       ...                      \n","0                       0.484647        0.831760  ...  0.000124    0.000295\n","1                       0.392909        0.781609  ...  0.000774    0.001841\n","\n","[2 rows x 23 columns]"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"id":"bqySW92Apmvw","colab_type":"code","outputId":"29977ec0-bf1e-482b-c6ad-b987ee2cd1cd","executionInfo":{"status":"ok","timestamp":1573176961343,"user_tz":-480,"elapsed":1219,"user":{"displayName":"陆韡","photoUrl":"","userId":"00756817164624838101"}},"colab":{"base_uri":"https://localhost:8080/","height":431}},"source":["grp = pd.concat([td['same_source'], dummy], axis=1).groupby('same_source').mean()\n","for i in range(grp.shape[1]):\n","    print( abs(grp.iat[0,i]-grp.iat[1,i])/max(grp.iat[0,i],grp.iat[0,i]), grp.columns[i], grp.iat[1,i]/grp.iat[0,i] )"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0.1892892338318083 both_contain_number 0.8107107661681917\n","0.06029486514639477 contain_number 0.9397051348536052\n","0.044018739560268895 contain_$ 0.9559812604397311\n","nan discipline nan\n","2.913409608859947 industry 3.9134096088599466\n","1.46429181967734 people 2.46429181967734\n","1.646605071442698 business 2.6466050714426985\n","13.63807800003094 culture 14.63807800003094\n","21.955167772775795 economy 22.955167772775795\n","3.2117742609179936 education 4.211774260917993\n","3.3470655878879763 entertainment 4.347065587887976\n","12.972710818211352 art 13.972710818211352\n","0.11022362614853472 society 1.1102236261485348\n","0.8876177658142664 geography 1.8876177658142665\n","0.47085883973858683 government 1.4708588397385869\n","3.8571804272829935 health 4.8571804272829935\n","2.2244717272795427 law 3.224471727279543\n","1.4882909676266791 history 2.488290967626679\n","15.9668631363995 mathematics 16.9668631363995\n","0.8107811558915637 statistics 1.8107811558915639\n","3.951867295465012 military 4.951867295465012\n","5.23781732955864 nature 6.23781732955864\n","5.250949576568237 technology 6.250949576568237\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yRNuxKWr--ac","colab_type":"text"},"source":["#### word_match_share"]},{"cell_type":"code","metadata":{"id":"cYznkV_-_Pah","colab_type":"code","colab":{}},"source":["stops = STOP_WORDS\n","def word_match_share(row):\n","    q1words = {}\n","    q2words = {}\n","    for word in str(row['sent1']).lower().split():\n","        if word not in stops:\n","            q1words[word] = 1\n","    for word in str(row['sent2']).lower().split():\n","        if word not in stops:\n","            q2words[word] = 1\n","    if len(q1words) == 0 or len(q2words) == 0:\n","        # The computer-generated chaff includes a few questions that are nothing but stopwords\n","        return 0\n","    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n","    # print(shared_words_in_q1)\n","    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n","    R = (len(shared_words_in_q1) + len(shared_words_in_q2))/(len(q1words) + len(q2words))\n","    return R\n","    \n","td['train_word_match'] = td.apply(word_match_share, axis=1, raw=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VOENtOx1AmxJ","colab_type":"text"},"source":["####  TF-IDF"]},{"cell_type":"code","metadata":{"id":"pVoHZS0F_30C","colab_type":"code","colab":{}},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from scipy.linalg import norm\n","def tfidf_similarity(s1, s2):\n","    def add_space(s):\n","        return ' '.join(list(s))\n","\n","    # 将字中间加入空格\n","    s1, s2 = add_space(s1), add_space(s2)\n","    # 转化为TF矩阵\n","    cv = TfidfVectorizer(tokenizer=lambda s: s.split())\n","    corpus = [s1, s2]\n","    vectors = cv.fit_transform(corpus).toarray()\n","    # 计算TF系数\n","    return np.dot(vectors[0], vectors[1]) / (norm(vectors[0]) * norm(vectors[1]))\n","\n","tfidf_sim = []\n","for i in range(df_train.shape[0]):\n","    tfidf_sim.append(tfidf_similarity(df_train.iat[i,1], df_train.iat[i,2]))\n","    \n","td['tfidf_sim'] = tfidf_sim"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UyTgFiuCBZHi","colab_type":"text"},"source":["#### same words"]},{"cell_type":"code","metadata":{"id":"4SDujjIm_5mu","colab_type":"code","colab":{}},"source":["td['same_words'] = 0\n","\n","for i in range(td.shape[0]):\n","    count = 0\n","    for word1 in td.loc[i,'sent1_tradition']:\n","        for word2 in td.loc[i,'sent2_tradition']:\n","            if word1==word2:\n","                count += 1\n","    \n","    td.loc[i,'same_words'] = count"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q3DmPupRPku7","colab_type":"text"},"source":["#### BoW + basic distances\n","Count bag of words of two sentences in the same row, then use basic distances functions (cosine, cityblock, jaccard, canberra, euclidean, minkowski, braycurtis distances) to count the distance between sent1 and sent2."]},{"cell_type":"code","metadata":{"id":"pPo_eRwU2g3F","colab_type":"code","colab":{}},"source":["bow_basic_distance = pd.DataFrame()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WMevvx_5zji1","colab_type":"code","colab":{}},"source":["# cosine, cityblock, jaccard, canberra, euclidean, minkowski, braycurtis\n","def bow_sent(sent1, sent2):\n","    # turn list to string by using \" \".join()\n","    vect = CountVectorizer(stop_words=\"english\").fit([\" \".join(sent1+sent2)]) #Convert a collection of text documents to a matrix of token counts\n","    v_1, v_2 = vect.transform([\" \".join(sent1), \" \".join(sent2)]) #Learn the vocabulary dictionary and return term-document matrixv\n","    v_1 = v_1.toarray().ravel()\n","    v_2 = v_2.toarray().ravel()\n","    bow_cosine = cosine(v_1, v_2)\n","    bow_cityblock = cityblock(v_1, v_2)\n","    bow_jaccard = jaccard(v_1, v_2)\n","    bow_canberra = canberra(v_1, v_2)\n","    bow_euclidean = euclidean(v_1, v_2)\n","    bow_minkowski = minkowski(v_1, v_2, 3)\n","    bow_braycurtis = braycurtis(v_1, v_2)\n","    #return bow_cosine, bow_cityblock, bow_jaccard, bow_canberra, bow_euclidean, bow_minkowski, bow_braycurtis\n","    return '{}:{}:{}:{}:{}:{}:{}'.format(bow_cosine, bow_cityblock, bow_jaccard, bow_canberra, bow_euclidean, bow_minkowski, bow_braycurtis)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"u5mvEUrCSrM8","colab_type":"code","colab":{}},"source":["bow_basic_distance['temp'] = td[['sent1_tradition', 'sent2_tradition']].apply(lambda x: bow_sent(x[0], x[1]), axis=1)\n","bow_basic_distance['bow_cosine'] = bow_basic_distance['temp'].apply(lambda x: float(x.split(':')[0]))\n","bow_basic_distance['bow_cityblock'] = bow_basic_distance['temp'].apply(lambda x: float(x.split(':')[1]))\n","bow_basic_distance['bow_jaccard'] = bow_basic_distance['temp'].apply(lambda x: float(x.split(':')[2]))\n","bow_basic_distance['bow_canberra'] = bow_basic_distance['temp'].apply(lambda x: float(x.split(':')[3]))\n","bow_basic_distance['bow_euclidean'] = bow_basic_distance['temp'].apply(lambda x: float(x.split(':')[4]))\n","bow_basic_distance['bow_minkowski'] = bow_basic_distance['temp'].apply(lambda x: float(x.split(':')[5]))\n","bow_basic_distance['bow_braycurtis'] = bow_basic_distance['temp'].apply(lambda x: float(x.split(':')[6]))\n","\n","bow_basic_distance.drop(columns='temp', inplace=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SPejnmNevUd0","colab_type":"text"},"source":["#### Word2Vec + basic distances\n","We use word2vec model to count every words' vector. Then we count each sentence's vector by using sent2vec function. We can get every sentence's vector in this step. Then we use basic distances to count the distance between sent1 and sent2.\n","\n","\n","*word2vec function has several hyperparameters, so we adjust min_count, size, window parameters to create several word2vec models in this step.</br>\n","*We hope to create as much features as possible and use xgboost or DNN models to select the most important features for us."]},{"cell_type":"code","metadata":{"id":"3XMYrR9ljqs5","colab_type":"code","colab":{}},"source":["from nltk.tokenize import RegexpTokenizer\n","\n","## creating corpus by using training set and testing set\n","train_qs = pd.Series(df_train['sent1'].tolist() + df_train['sent2'].tolist()).astype(str)\n","test_qs = pd.Series(df_test['sent1'].tolist() + df_test['sent2'].tolist()).astype(str)\n","\n","train_string = ''\n","for i in range(len(train_qs)):\n","    train_string += train_qs[i]\n","    i = i+1\n","\n","test_string = ''\n","for i in range(len(test_qs)):\n","    train_string += train_qs[i]\n","    i = i+1\n","\n","data = train_string + test_string\n","tokenizer = RegexpTokenizer(r'\\w+')\n","words = tokenizer.tokenize(data)\n","\n","## creating word2vec model by using different paramerters (min_count, size, window)\n","for min_count in range(1,3):\n","    for size in range(40,210,30):\n","        for window in range(3,8,2):\n","            print('model_w2v_' + str(min_count) + '_' + str(size) + '_' + str(window))\n","            locals()['model_w2v_' + str(min_count) + '_' + str(size) + '_' + str(window)] = gensim.models.Word2Vec([words], min_count=min_count, size=size, window=window)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HgH3NydLvqu9","colab_type":"code","colab":{}},"source":["model_w2v_2_250_5 = gensim.models.Word2Vec([words], min_count=2, size=250, window=5)\n","model_w2v_2_300_5 = gensim.models.Word2Vec([words], min_count=2, size=300, window=5)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fmqRGeW2_GxL","colab_type":"code","colab":{}},"source":["## counting every sent's score\n","def sent2vec(s, model):\n","    words = tokenizer.tokenize(s.lower())\n","    words = [w for w in words if not w in STOP_WORDS]\n","    words = [w for w in words if w.isalpha()]\n","    M = []\n","    for w in words:\n","        try:\n","            M.append(model[w])\n","        except:\n","            continue\n","    M = np.array(M)\n","    v = M.sum(axis=0)\n","    return v / np.sqrt((v ** 2).sum())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"T8PGWVwPMEFC","colab_type":"code","colab":{}},"source":["from scipy.spatial.distance import cosine, cityblock, jaccard, canberra, euclidean, minkowski, braycurtis\n","\n","data = pd.DataFrame()\n","def sent1_score(sent1, sent2, model, model_name):\n","    \n","    sent1_vectors = np.zeros((td.shape[0], model.vector_size))\n","    count = 0\n","    for i in td[sent1].values:\n","        sent1_vectors[count,:] = sent2vec(i, model)\n","        count += 1\n","\n","    sent2_vectors = np.zeros((td.shape[0], model.vector_size))\n","    count = 0\n","    for i in td[sent2].values:\n","        sent2_vectors[count,:] = sent2vec(i, model)\n","        count += 1\n","    \n","    data['cosine_distance_' + model_name] = [cosine(x, y) for (x, y) in zip(np.nan_to_num(sent1_vectors),np.nan_to_num(sent2_vectors))]\n","    data['cityblock_distance_' + model_name] = [cityblock(x, y) for (x, y) in zip(np.nan_to_num(sent1_vectors),np.nan_to_num(sent2_vectors))]\n","    data['jaccard_distance_' + model_name] = [jaccard(x, y) for (x, y) in zip(np.nan_to_num(sent1_vectors),np.nan_to_num(sent2_vectors))]\n","    data['canberra_distance_' + model_name] = [canberra(x, y) for (x, y) in zip(np.nan_to_num(sent1_vectors),np.nan_to_num(sent2_vectors))]\n","    data['euclidean_distance_' + model_name] = [euclidean(x, y) for (x, y) in zip(np.nan_to_num(sent1_vectors),np.nan_to_num(sent2_vectors))]\n","    data['minkowski_distance_' + model_name] = [minkowski(x, y, 3) for (x, y) in zip(np.nan_to_num(sent1_vectors),np.nan_to_num(sent2_vectors))]\n","    data['braycurtis_distance_' + model_name] = [braycurtis(x, y) for (x, y) in zip(np.nan_to_num(sent1_vectors),np.nan_to_num(sent2_vectors))]\n","    print('-')\n","    return data"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cg7rBHWrowsA","colab_type":"code","colab":{}},"source":["sent1_score('sent1', 'sent2', model_w2v_1_40_3, 'model_w2v_1_40_3') \n","sent1_score('sent1', 'sent2', model_w2v_1_40_5, 'model_w2v_1_40_5')\n","sent1_score('sent1', 'sent2', model_w2v_1_40_7, 'model_w2v_1_40_7') \n","sent1_score('sent1', 'sent2', model_w2v_1_70_3, 'model_w2v_1_70_3') \n","sent1_score('sent1', 'sent2', model_w2v_1_70_5, 'model_w2v_1_70_5') \n","sent1_score('sent1', 'sent2', model_w2v_1_70_7, 'model_w2v_1_70_7')\n","sent1_score('sent1', 'sent2', model_w2v_1_100_3, 'model_w2v_1_100_3') \n","sent1_score('sent1', 'sent2', model_w2v_1_100_5, 'model_w2v_1_100_5')\n","sent1_score('sent1', 'sent2', model_w2v_1_100_7, 'model_w2v_1_100_7')\n","sent1_score('sent1', 'sent2', model_w2v_1_130_3, 'model_w2v_1_130_3')\n","sent1_score('sent1', 'sent2', model_w2v_1_130_5, 'model_w2v_1_130_5')\n","sent1_score('sent1', 'sent2', model_w2v_1_130_7, 'model_w2v_1_130_7')\n","sent1_score('sent1', 'sent2', model_w2v_1_160_3, 'model_w2v_1_160_3')\n","sent1_score('sent1', 'sent2', model_w2v_1_160_5, 'model_w2v_1_160_5')\n","sent1_score('sent1', 'sent2', model_w2v_1_160_7, 'model_w2v_1_160_7')\n","sent1_score('sent1', 'sent2', model_w2v_1_190_3, 'model_w2v_1_190_3')\n","sent1_score('sent1', 'sent2', model_w2v_1_190_5, 'model_w2v_1_190_5')\n","sent1_score('sent1', 'sent2', model_w2v_1_190_7, 'model_w2v_1_190_7')\n","sent1_score('sent1', 'sent2', model_w2v_2_40_3, 'model_w2v_2_40_3') \n","sent1_score('sent1', 'sent2', model_w2v_2_40_5, 'model_w2v_2_40_5')\n","sent1_score('sent1', 'sent2', model_w2v_2_40_7, 'model_w2v_2_40_7') \n","sent1_score('sent1', 'sent2', model_w2v_2_70_3, 'model_w2v_2_70_3') \n","sent1_score('sent1', 'sent2', model_w2v_2_70_5, 'model_w2v_2_70_5') \n","sent1_score('sent1', 'sent2', model_w2v_2_70_7, 'model_w2v_2_70_7')\n","sent1_score('sent1', 'sent2', model_w2v_2_100_3, 'model_w2v_2_100_3') \n","sent1_score('sent1', 'sent2', model_w2v_2_100_5, 'model_w2v_2_100_5')\n","sent1_score('sent1', 'sent2', model_w2v_2_100_7, 'model_w2v_2_100_7')\n","sent1_score('sent1', 'sent2', model_w2v_2_130_3, 'model_w2v_2_130_3')\n","sent1_score('sent1', 'sent2', model_w2v_2_130_5, 'model_w2v_2_130_5')\n","sent1_score('sent1', 'sent2', model_w2v_2_130_7, 'model_w2v_2_130_7')\n","sent1_score('sent1', 'sent2', model_w2v_2_160_3, 'model_w2v_2_160_3')\n","sent1_score('sent1', 'sent2', model_w2v_2_160_5, 'model_w2v_2_160_5')\n","sent1_score('sent1', 'sent2', model_w2v_2_160_7, 'model_w2v_2_160_7')\n","sent1_score('sent1', 'sent2', model_w2v_2_190_3, 'model_w2v_2_190_3')\n","sent1_score('sent1', 'sent2', model_w2v_2_190_5, 'model_w2v_2_190_5')\n","sent1_score('sent1', 'sent2', model_w2v_2_190_7, 'model_w2v_2_190_7')\n","sent1_score('sent1', 'sent2', model_w2v_2_250_5, 'model_w2v_2_250_5')\n","sent1_score('sent1', 'sent2', model_w2v_2_300_5, 'model_w2v_2_300_5')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j3agJWswN8Ek","colab_type":"text"},"source":["#### Word2Vec + WMD\n","Use word move distance to count distance between two sentences."]},{"cell_type":"code","metadata":{"id":"8C3T1SQSOB6x","colab_type":"code","colab":{}},"source":["data_w2v_wmd = pd.DataFrame()\n","\n","def w2v_wmd_score(model, model_name):\n","    data_w2v_wmd['w2v_wmd' + model_name] = td[['sent1_tradition', 'sent2_tradition']].apply(lambda x: model.wmdistance(x[0], x[1]), axis=1)\n","    print('-')\n","    return data_w2v_wmd"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"It52SEEcQYj-","colab_type":"code","colab":{}},"source":["w2v_wmd_score(model_w2v_1_40_3, 'model_w2v_1_40_3') \n","w2v_wmd_score(model_w2v_1_40_5, 'model_w2v_1_40_5')\n","w2v_wmd_score(model_w2v_1_40_7, 'model_w2v_1_40_7') \n","w2v_wmd_score(model_w2v_1_70_3, 'model_w2v_1_70_3') \n","w2v_wmd_score(model_w2v_1_70_5, 'model_w2v_1_70_5') \n","w2v_wmd_score(model_w2v_1_70_7, 'model_w2v_1_70_7')\n","w2v_wmd_score(model_w2v_1_100_3, 'model_w2v_1_100_3') \n","w2v_wmd_score(model_w2v_1_100_5, 'model_w2v_1_100_5')\n","w2v_wmd_score(model_w2v_1_100_7, 'model_w2v_1_100_7')\n","w2v_wmd_score(model_w2v_1_130_3, 'model_w2v_1_130_3')\n","w2v_wmd_score(model_w2v_1_130_5, 'model_w2v_1_130_5')\n","w2v_wmd_score(model_w2v_1_130_7, 'model_w2v_1_130_7')\n","w2v_wmd_score(model_w2v_1_160_3, 'model_w2v_1_160_3')\n","w2v_wmd_score(model_w2v_1_160_5, 'model_w2v_1_160_5')\n","w2v_wmd_score(model_w2v_1_160_7, 'model_w2v_1_160_7')\n","w2v_wmd_score(model_w2v_1_190_3, 'model_w2v_1_190_3')\n","w2v_wmd_score(model_w2v_1_190_5, 'model_w2v_1_190_5')\n","w2v_wmd_score(model_w2v_1_190_7, 'model_w2v_1_190_7')\n","w2v_wmd_score(model_w2v_2_40_3, 'model_w2v_2_40_3') \n","w2v_wmd_score(model_w2v_2_40_5, 'model_w2v_2_40_5')\n","w2v_wmd_score(model_w2v_2_40_7, 'model_w2v_2_40_7') \n","w2v_wmd_score(model_w2v_2_70_3, 'model_w2v_2_70_3') \n","w2v_wmd_score(model_w2v_2_70_5, 'model_w2v_2_70_5') \n","w2v_wmd_score(model_w2v_2_70_7, 'model_w2v_2_70_7')\n","w2v_wmd_score(model_w2v_2_100_3, 'model_w2v_2_100_3') \n","w2v_wmd_score(model_w2v_2_100_5, 'model_w2v_2_100_5')\n","w2v_wmd_score(model_w2v_2_100_7, 'model_w2v_2_100_7')\n","w2v_wmd_score(model_w2v_2_130_3, 'model_w2v_2_130_3')\n","w2v_wmd_score(model_w2v_2_130_5, 'model_w2v_2_130_5')\n","w2v_wmd_score(model_w2v_2_130_7, 'model_w2v_2_130_7')\n","w2v_wmd_score(model_w2v_2_160_3, 'model_w2v_2_160_3')\n","w2v_wmd_score(model_w2v_2_160_5, 'model_w2v_2_160_5')\n","w2v_wmd_score(model_w2v_2_160_7, 'model_w2v_2_160_7')\n","w2v_wmd_score(model_w2v_2_190_3, 'model_w2v_2_190_3')\n","w2v_wmd_score(model_w2v_2_190_5, 'model_w2v_2_190_5')\n","w2v_wmd_score(model_w2v_2_190_7, 'model_w2v_2_190_7')\n","w2v_wmd_score(model_w2v_2_250_5, 'model_w2v_2_250_5')\n","w2v_wmd_score(model_w2v_2_300_5, 'model_w2v_2_300_5')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q8yUa-jEGiM-","colab_type":"text"},"source":["#### GloVe\n","GloVe is similar to word2vec, it can also turn words to vectors. We use GloVe to count word vectors in this step."]},{"cell_type":"code","metadata":{"id":"WpLaV8vLGk4U","colab_type":"code","colab":{}},"source":["#200d\n","from gensim.scripts.glove2word2vec import glove2word2vec\n","glove_input_file='/content/drive//My Drive/data/glove.6B.200d.txt'\n","word2vec_output_file='/content/drive//My Drive/data/glove.6B.200d.txt.word2vec'\n","glove2word2vec(glove_input_file,word2vec_output_file)\n","from gensim.models import KeyedVectors\n","filename=word2vec_output_file\n","model2=KeyedVectors.load_word2vec_format(filename,binary=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5QQo3kpiGygv","colab_type":"code","colab":{}},"source":["def sent2vec(s, model):\n","    words = tokenizer.tokenize(s.lower())\n","    words = [w for w in words if not w in STOP_WORDS]\n","    words = [w for w in words if w.isalpha()]\n","    M = []\n","    for w in words:\n","        try:\n","            M.append(model[w])\n","        except:\n","            continue\n","    M = np.array(M)\n","    v = M.sum(axis=0)\n","    return v / np.sqrt((v ** 2).sum())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0H7QYQtXGy63","colab_type":"code","colab":{}},"source":["sent1_vectors = np.zeros((td.shape[0], model2.vector_size))\n","count = 0\n","for i in td.sent1.values:\n","    sent1_vectors[count,:] = sent2vec(i, model2)\n","    count += 1\n","\n","sent2_vectors = np.zeros((td.shape[0], model2.vector_size))\n","count = 0\n","for i in td.sent2.values:\n","    sent2_vectors[count,:] = sent2vec(i, model2)\n","    count += 1"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AGVV6SYgG52M","colab_type":"text"},"source":["#### Glove + Cosine Distance"]},{"cell_type":"code","metadata":{"id":"sSR0T_liG75R","colab_type":"code","colab":{}},"source":["data = pd.DataFrame()\n","data['cosine_distance'] = [cosine(x, y) for (x, y) in zip(sent1_vectors, sent2_vectors)]\n","data['cityblock_distance'] = [cityblock(x, y) for (x, y) in zip(sent1_vectors, sent2_vectors)]\n","data['jaccard_distance'] = [jaccard(x, y) for (x, y) in zip(sent1_vectors, sent2_vectors)]\n","data['canberra_distance'] = [canberra(x, y) for (x, y) in zip(sent1_vectors, sent2_vectors)]\n","data['euclidean_distance'] = [euclidean(x, y) for (x, y) in zip(sent1_vectors, sent2_vectors)]\n","data['minkowski_distance'] = [minkowski(x, y, 3) for (x, y) in zip(sent1_vectors, sent2_vectors)]\n","data['braycurtis_distance'] = [braycurtis(x, y) for (x, y) in zip(sent1_vectors, sent2_vectors)]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tyFFz6OsG94b","colab_type":"code","colab":{}},"source":["td.insert(8,'glove_cosine',data['cosine_distance'])\n","td.insert(9,'glove_cityblock',data['cityblock_distance'])\n","td.insert(10,'glove_jaccard',data['jaccard_distance'])\n","td.insert(11,'glove_canberra',data['canberra_distance'])\n","td.insert(12,'glove_braycurtis',data['braycurtis_distance'])\n","td.insert(13,'glove_euclidean',data['euclidean_distance'])\n","td.insert(14,'glove_minkowski',data['minkowski_distance'])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gP_FsFrgHBJ_","colab_type":"text"},"source":["#### GloVe + wmdistance"]},{"cell_type":"code","metadata":{"id":"VTi8qGjTHC3_","colab_type":"code","colab":{}},"source":["td['glove_wmd_1'] = td[['sent1_tradition', 'sent2_tradition']].apply(lambda x: model2.wmdistance(x[0], x[1]), axis=1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A56JBZ8lgQNy","colab_type":"text"},"source":["## Modelling"]},{"cell_type":"markdown","metadata":{"id":"NiGfEgB30-x3","colab_type":"text"},"source":["#### Xgboost"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ZZpaNe5GKqcj","colab":{}},"source":["all_data_train = pd.read_csv('/content/drive//My Drive/data/train_features.csv')\n","all_data_clean = all_data_train.replace([np.inf, -np.inf], np.nan).dropna(how=\"any\")\n","\n","x_train = all_data_clean.drop(columns=['same_source','id'])\n","y_train = all_data_clean['same_source']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WZOT5GrXAaNp","colab_type":"code","colab":{}},"source":["all_data_test = pd.read_csv('/content/drive//My Drive/data/test_features.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hy_e9x2a4rKi","colab_type":"code","colab":{}},"source":["def train_xgb(X, y, params):\n","\tx, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=RS)\n","\n","\txg_train = xgb.DMatrix(x, label=y_train)\n","\txg_val = xgb.DMatrix(X_val, label=y_val)\n","\n","\twatchlist  = [(xg_train,'train'), (xg_val,'eval')]\n","\treturn xgb.train(params, xg_train, ROUNDS, watchlist)\n","\n","def train_xgb_silent(X, y, params):\n","\tx, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=RS)\n","\n","\txg_train = xgb.DMatrix(x, label=y_train)\n","\txg_val = xgb.DMatrix(X_val, label=y_val)\n","\n","\t#watchlist  = [(xg_train,'train'), (xg_val,'eval')]\n","\treturn xgb.train(params, xg_train, ROUNDS)\n"," \n","def predict_xgb(clr, X_test):\n","\treturn clr.predict(xgb.DMatrix(X_test))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"W2VrsyB55Fsj","colab_type":"code","colab":{}},"source":["RS = 47\n","ROUNDS = 500\n","params = {}\n","params['objective'] = 'binary:logistic'\n","params['eval_metric'] = 'logloss'\n","params['eta'] = 0.1\n","params['max_depth'] = 5\n","params['silent'] = 1\n","params['seed'] = RS"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YcAhb0Cw9E5n","colab_type":"code","outputId":"4d0bf7a9-6519-4da0-f796-d8e81b388287","executionInfo":{"status":"ok","timestamp":1572907461146,"user_tz":-480,"elapsed":29263,"user":{"displayName":"陆韡","photoUrl":"","userId":"00756817164624838101"}},"colab":{"base_uri":"https://localhost:8080/","height":269}},"source":["# feature selection - spearman\n","## drop the high correlated features by using spearman. If two features' correlation score is above threshold(0.98), then drop one feature.\n","X_train_corr = all_data[['w2v_wmdmodel_w2v_1_40_3', 'w2v_wmdmodel_w2v_1_40_5',\n","       'w2v_wmdmodel_w2v_1_40_7', 'w2v_wmdmodel_w2v_1_70_3',\n","       'w2v_wmdmodel_w2v_1_70_5', 'w2v_wmdmodel_w2v_1_70_7',\n","       'w2v_wmdmodel_w2v_1_100_3', 'w2v_wmdmodel_w2v_1_100_5',\n","       'w2v_wmdmodel_w2v_1_100_7', 'w2v_wmdmodel_w2v_1_130_3',\n","       'w2v_wmdmodel_w2v_1_130_5', 'w2v_wmdmodel_w2v_1_130_7',\n","       'w2v_wmdmodel_w2v_1_160_3', 'w2v_wmdmodel_w2v_1_160_5',\n","       'w2v_wmdmodel_w2v_1_160_7', 'w2v_wmdmodel_w2v_1_190_3',\n","       'w2v_wmdmodel_w2v_1_190_5', 'w2v_wmdmodel_w2v_1_190_7',\n","       'w2v_wmdmodel_w2v_2_40_3', 'w2v_wmdmodel_w2v_2_40_5',\n","       'w2v_wmdmodel_w2v_2_40_7', 'w2v_wmdmodel_w2v_2_70_3',\n","       'w2v_wmdmodel_w2v_2_70_5', 'w2v_wmdmodel_w2v_2_70_7',\n","       'w2v_wmdmodel_w2v_2_100_3', 'w2v_wmdmodel_w2v_2_100_5',\n","       'w2v_wmdmodel_w2v_2_100_7', 'w2v_wmdmodel_w2v_2_130_3',\n","       'w2v_wmdmodel_w2v_2_130_5', 'w2v_wmdmodel_w2v_2_130_7',\n","       'w2v_wmdmodel_w2v_2_160_3', 'w2v_wmdmodel_w2v_2_160_5',\n","       'w2v_wmdmodel_w2v_2_160_7', 'w2v_wmdmodel_w2v_2_190_3',\n","       'w2v_wmdmodel_w2v_2_190_5', 'w2v_wmdmodel_w2v_2_190_7',\n","       'w2v_wmdmodel_w2v_2_250_5', 'w2v_wmdmodel_w2v_2_300_5']].corr(method='spearman')\n","\n","# 将对角线变为0\n","mask = np.ones(X_train_corr.columns.size) - np.eye(X_train_corr.columns.size)\n","X_train_corr = mask * X_train_corr\n","\n","drops = []\n","for col in X_train_corr.columns.values:\n","    # if we've already determined to drop the current variable, continue\n","    if np.in1d([col],drops):\n","        continue\n","\n","    # 找出高相关的变量\n","    corr = X_train_corr[abs(X_train_corr[col]) > 0.98].index\n","    drops = np.union1d(drops, corr)\n","\n","print(\"nDropping\", drops.shape[0], \"highly correlated features...n\", drops)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["nDropping 27 highly correlated features...n ['w2v_wmdmodel_w2v_1_100_5' 'w2v_wmdmodel_w2v_1_100_7'\n"," 'w2v_wmdmodel_w2v_1_130_3' 'w2v_wmdmodel_w2v_1_130_7'\n"," 'w2v_wmdmodel_w2v_1_160_5' 'w2v_wmdmodel_w2v_1_190_3'\n"," 'w2v_wmdmodel_w2v_1_190_5' 'w2v_wmdmodel_w2v_1_190_7'\n"," 'w2v_wmdmodel_w2v_2_100_3' 'w2v_wmdmodel_w2v_2_100_5'\n"," 'w2v_wmdmodel_w2v_2_100_7' 'w2v_wmdmodel_w2v_2_130_3'\n"," 'w2v_wmdmodel_w2v_2_130_5' 'w2v_wmdmodel_w2v_2_130_7'\n"," 'w2v_wmdmodel_w2v_2_160_3' 'w2v_wmdmodel_w2v_2_160_5'\n"," 'w2v_wmdmodel_w2v_2_160_7' 'w2v_wmdmodel_w2v_2_190_3'\n"," 'w2v_wmdmodel_w2v_2_190_7' 'w2v_wmdmodel_w2v_2_250_5'\n"," 'w2v_wmdmodel_w2v_2_300_5' 'w2v_wmdmodel_w2v_2_40_3'\n"," 'w2v_wmdmodel_w2v_2_40_5' 'w2v_wmdmodel_w2v_2_40_7'\n"," 'w2v_wmdmodel_w2v_2_70_3' 'w2v_wmdmodel_w2v_2_70_5'\n"," 'w2v_wmdmodel_w2v_2_70_7']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7mrswdydlxfR","colab_type":"code","colab":{}},"source":["drops=['same_source','braycurtis_distance_model_w2v_1_100_7','braycurtis_distance_model_w2v_1_130_7','braycurtis_distance_model_w2v_1_160_7','braycurtis_distance_model_w2v_1_190_5','braycurtis_distance_model_w2v_1_40_7','braycurtis_distance_model_w2v_1_70_7','braycurtis_distance_model_w2v_2_100_7','braycurtis_distance_model_w2v_2_130_5','braycurtis_distance_model_w2v_2_130_7','braycurtis_distance_model_w2v_2_160_5','braycurtis_distance_model_w2v_2_190_5','braycurtis_distance_model_w2v_2_190_7','braycurtis_distance_model_w2v_2_250_5','braycurtis_distance_model_w2v_2_40_7','braycurtis_distance_model_w2v_2_70_7','cityblock_distance_model_w2v_1_190_7','cityblock_distance_model_w2v_2_160_7','cityblock_distance_model_w2v_2_190_7','cityblock_distance_model_w2v_2_300_5','cosine_distance_model_w2v_1_190_7','cosine_distance_model_w2v_2_100_3','cosine_distance_model_w2v_2_100_7','cosine_distance_model_w2v_2_130_7','cosine_distance_model_w2v_2_160_7','cosine_distance_model_w2v_2_300_5','cosine_distance_model_w2v_2_40_7','cosine_distance_model_w2v_2_70_7','diff_len_char','euclidean_distance_model_w2v_1_100_3','euclidean_distance_model_w2v_1_100_5','euclidean_distance_model_w2v_1_100_7','euclidean_distance_model_w2v_1_130_3','euclidean_distance_model_w2v_1_130_5','euclidean_distance_model_w2v_1_130_7','euclidean_distance_model_w2v_1_160_3','euclidean_distance_model_w2v_1_160_5','euclidean_distance_model_w2v_1_160_7','euclidean_distance_model_w2v_1_190_3','euclidean_distance_model_w2v_1_190_5','euclidean_distance_model_w2v_1_190_7','euclidean_distance_model_w2v_1_40_3','euclidean_distance_model_w2v_1_40_5','euclidean_distance_model_w2v_1_40_7','euclidean_distance_model_w2v_1_70_3','euclidean_distance_model_w2v_1_70_5','euclidean_distance_model_w2v_1_70_7','euclidean_distance_model_w2v_2_100_5','euclidean_distance_model_w2v_2_100_7','euclidean_distance_model_w2v_2_130_3','euclidean_distance_model_w2v_2_130_5','euclidean_distance_model_w2v_2_130_7','euclidean_distance_model_w2v_2_160_3','euclidean_distance_model_w2v_2_160_5','euclidean_distance_model_w2v_2_160_7','euclidean_distance_model_w2v_2_190_3','euclidean_distance_model_w2v_2_190_5','euclidean_distance_model_w2v_2_190_7','euclidean_distance_model_w2v_2_250_5','euclidean_distance_model_w2v_2_300_5','euclidean_distance_model_w2v_2_40_3','euclidean_distance_model_w2v_2_40_5','euclidean_distance_model_w2v_2_40_7','euclidean_distance_model_w2v_2_70_3','euclidean_distance_model_w2v_2_70_5','jaccard_distance_model_w2v_1_100_3','jaccard_distance_model_w2v_1_100_5','jaccard_distance_model_w2v_1_100_7','jaccard_distance_model_w2v_1_130_3','jaccard_distance_model_w2v_1_130_5','jaccard_distance_model_w2v_1_130_7','jaccard_distance_model_w2v_1_160_3','jaccard_distance_model_w2v_1_160_5','jaccard_distance_model_w2v_1_160_7','jaccard_distance_model_w2v_1_190_3','jaccard_distance_model_w2v_1_190_5','jaccard_distance_model_w2v_1_190_7','jaccard_distance_model_w2v_1_40_5','jaccard_distance_model_w2v_1_40_7','jaccard_distance_model_w2v_1_70_3','jaccard_distance_model_w2v_1_70_5','jaccard_distance_model_w2v_1_70_7','jaccard_distance_model_w2v_2_100_3','jaccard_distance_model_w2v_2_100_5','jaccard_distance_model_w2v_2_100_7','jaccard_distance_model_w2v_2_130_3','jaccard_distance_model_w2v_2_130_5','jaccard_distance_model_w2v_2_130_7','jaccard_distance_model_w2v_2_160_3','jaccard_distance_model_w2v_2_160_5','jaccard_distance_model_w2v_2_160_7','jaccard_distance_model_w2v_2_190_3','jaccard_distance_model_w2v_2_190_5','jaccard_distance_model_w2v_2_190_7','jaccard_distance_model_w2v_2_250_5','jaccard_distance_model_w2v_2_300_5','jaccard_distance_model_w2v_2_40_3','jaccard_distance_model_w2v_2_40_5','jaccard_distance_model_w2v_2_40_7','jaccard_distance_model_w2v_2_70_3','jaccard_distance_model_w2v_2_70_5','jaccard_distance_model_w2v_2_70_7','len_char_q1','len_char_q2','minkowski_distance_model_w2v_1_100_7','minkowski_distance_model_w2v_1_130_7','minkowski_distance_model_w2v_1_160_7','minkowski_distance_model_w2v_1_40_7','minkowski_distance_model_w2v_1_70_7','minkowski_distance_model_w2v_2_130_5','minkowski_distance_model_w2v_2_160_5','minkowski_distance_model_w2v_2_160_7','minkowski_distance_model_w2v_2_190_5','minkowski_distance_model_w2v_2_190_7','minkowski_distance_model_w2v_2_250_5','minkowski_distance_model_w2v_2_70_7','w2v_wmdmodel_w2v_1_100_5','w2v_wmdmodel_w2v_1_100_7','w2v_wmdmodel_w2v_1_130_3','w2v_wmdmodel_w2v_1_130_7','w2v_wmdmodel_w2v_1_160_5','w2v_wmdmodel_w2v_1_190_3','w2v_wmdmodel_w2v_1_190_5','w2v_wmdmodel_w2v_1_190_7','w2v_wmdmodel_w2v_2_100_3','w2v_wmdmodel_w2v_2_100_5','w2v_wmdmodel_w2v_2_100_7','w2v_wmdmodel_w2v_2_130_3','w2v_wmdmodel_w2v_2_130_5','w2v_wmdmodel_w2v_2_130_7','w2v_wmdmodel_w2v_2_160_3','w2v_wmdmodel_w2v_2_160_5','w2v_wmdmodel_w2v_2_160_7','w2v_wmdmodel_w2v_2_190_3','w2v_wmdmodel_w2v_2_190_7','w2v_wmdmodel_w2v_2_250_5','w2v_wmdmodel_w2v_2_300_5','w2v_wmdmodel_w2v_2_40_3','w2v_wmdmodel_w2v_2_40_5','w2v_wmdmodel_w2v_2_40_7','w2v_wmdmodel_w2v_2_70_3','w2v_wmdmodel_w2v_2_70_5','w2v_wmdmodel_w2v_2_70_7']\n","x_train = all_data_clean.drop(columns = drops)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FgG95n6VJbp7","colab_type":"code","outputId":"76af801e-efeb-4c79-81ba-cb8dfbdb9c7a","executionInfo":{"status":"ok","timestamp":1573087455218,"user_tz":-480,"elapsed":1550201,"user":{"displayName":"陆韡","photoUrl":"","userId":"00756817164624838101"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["xgbt = train_xgb(x_train, y_train, params)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[0]\ttrain-logloss:0.663879\teval-logloss:0.666232\n","[1]\ttrain-logloss:0.639898\teval-logloss:0.644155\n","[2]\ttrain-logloss:0.619991\teval-logloss:0.626366\n","[3]\ttrain-logloss:0.60318\teval-logloss:0.611524\n","[4]\ttrain-logloss:0.588639\teval-logloss:0.598561\n","[5]\ttrain-logloss:0.576322\teval-logloss:0.587783\n","[6]\ttrain-logloss:0.565615\teval-logloss:0.578249\n","[7]\ttrain-logloss:0.556325\teval-logloss:0.57021\n","[8]\ttrain-logloss:0.548316\teval-logloss:0.563134\n","[9]\ttrain-logloss:0.541181\teval-logloss:0.557022\n","[10]\ttrain-logloss:0.535049\teval-logloss:0.55165\n","[11]\ttrain-logloss:0.529769\teval-logloss:0.547277\n","[12]\ttrain-logloss:0.525046\teval-logloss:0.543281\n","[13]\ttrain-logloss:0.520818\teval-logloss:0.53974\n","[14]\ttrain-logloss:0.517312\teval-logloss:0.536973\n","[15]\ttrain-logloss:0.51382\teval-logloss:0.534279\n","[16]\ttrain-logloss:0.510715\teval-logloss:0.531857\n","[17]\ttrain-logloss:0.507802\teval-logloss:0.52962\n","[18]\ttrain-logloss:0.505537\teval-logloss:0.528189\n","[19]\ttrain-logloss:0.503328\teval-logloss:0.526824\n","[20]\ttrain-logloss:0.501217\teval-logloss:0.5252\n","[21]\ttrain-logloss:0.499403\teval-logloss:0.523742\n","[22]\ttrain-logloss:0.497648\teval-logloss:0.522498\n","[23]\ttrain-logloss:0.496148\teval-logloss:0.521254\n","[24]\ttrain-logloss:0.494385\teval-logloss:0.520186\n","[25]\ttrain-logloss:0.493136\teval-logloss:0.519655\n","[26]\ttrain-logloss:0.491975\teval-logloss:0.519211\n","[27]\ttrain-logloss:0.490785\teval-logloss:0.518728\n","[28]\ttrain-logloss:0.489543\teval-logloss:0.51788\n","[29]\ttrain-logloss:0.488554\teval-logloss:0.517501\n","[30]\ttrain-logloss:0.487622\teval-logloss:0.517023\n","[31]\ttrain-logloss:0.486566\teval-logloss:0.516315\n","[32]\ttrain-logloss:0.485741\teval-logloss:0.515857\n","[33]\ttrain-logloss:0.484612\teval-logloss:0.514898\n","[34]\ttrain-logloss:0.483874\teval-logloss:0.514234\n","[35]\ttrain-logloss:0.482923\teval-logloss:0.513544\n","[36]\ttrain-logloss:0.482221\teval-logloss:0.513\n","[37]\ttrain-logloss:0.481538\teval-logloss:0.512693\n","[38]\ttrain-logloss:0.480702\teval-logloss:0.512079\n","[39]\ttrain-logloss:0.479906\teval-logloss:0.511721\n","[40]\ttrain-logloss:0.479377\teval-logloss:0.511348\n","[41]\ttrain-logloss:0.478776\teval-logloss:0.510886\n","[42]\ttrain-logloss:0.478289\teval-logloss:0.510907\n","[43]\ttrain-logloss:0.477334\teval-logloss:0.510449\n","[44]\ttrain-logloss:0.47675\teval-logloss:0.510146\n","[45]\ttrain-logloss:0.475745\teval-logloss:0.50914\n","[46]\ttrain-logloss:0.475345\teval-logloss:0.509062\n","[47]\ttrain-logloss:0.474775\teval-logloss:0.508868\n","[48]\ttrain-logloss:0.474396\teval-logloss:0.508672\n","[49]\ttrain-logloss:0.473882\teval-logloss:0.508306\n","[50]\ttrain-logloss:0.47329\teval-logloss:0.508159\n","[51]\ttrain-logloss:0.472837\teval-logloss:0.507951\n","[52]\ttrain-logloss:0.472481\teval-logloss:0.507929\n","[53]\ttrain-logloss:0.472049\teval-logloss:0.507473\n","[54]\ttrain-logloss:0.471698\teval-logloss:0.507351\n","[55]\ttrain-logloss:0.471189\teval-logloss:0.507092\n","[56]\ttrain-logloss:0.470716\teval-logloss:0.506939\n","[57]\ttrain-logloss:0.470302\teval-logloss:0.506757\n","[58]\ttrain-logloss:0.469862\teval-logloss:0.506595\n","[59]\ttrain-logloss:0.469453\teval-logloss:0.506106\n","[60]\ttrain-logloss:0.46924\teval-logloss:0.506131\n","[61]\ttrain-logloss:0.468929\teval-logloss:0.506014\n","[62]\ttrain-logloss:0.468472\teval-logloss:0.505858\n","[63]\ttrain-logloss:0.468162\teval-logloss:0.5056\n","[64]\ttrain-logloss:0.467745\teval-logloss:0.505577\n","[65]\ttrain-logloss:0.467202\teval-logloss:0.505255\n","[66]\ttrain-logloss:0.466685\teval-logloss:0.505047\n","[67]\ttrain-logloss:0.466487\teval-logloss:0.505011\n","[68]\ttrain-logloss:0.466113\teval-logloss:0.505032\n","[69]\ttrain-logloss:0.465582\teval-logloss:0.504552\n","[70]\ttrain-logloss:0.465204\teval-logloss:0.504309\n","[71]\ttrain-logloss:0.464762\teval-logloss:0.504048\n","[72]\ttrain-logloss:0.464576\teval-logloss:0.503867\n","[73]\ttrain-logloss:0.464114\teval-logloss:0.503471\n","[74]\ttrain-logloss:0.463942\teval-logloss:0.503258\n","[75]\ttrain-logloss:0.463761\teval-logloss:0.503208\n","[76]\ttrain-logloss:0.463482\teval-logloss:0.503326\n","[77]\ttrain-logloss:0.462974\teval-logloss:0.503026\n","[78]\ttrain-logloss:0.462767\teval-logloss:0.502956\n","[79]\ttrain-logloss:0.46242\teval-logloss:0.502721\n","[80]\ttrain-logloss:0.462068\teval-logloss:0.502508\n","[81]\ttrain-logloss:0.461815\teval-logloss:0.502485\n","[82]\ttrain-logloss:0.461335\teval-logloss:0.502123\n","[83]\ttrain-logloss:0.46095\teval-logloss:0.501899\n","[84]\ttrain-logloss:0.460774\teval-logloss:0.501913\n","[85]\ttrain-logloss:0.460416\teval-logloss:0.50192\n","[86]\ttrain-logloss:0.460066\teval-logloss:0.50163\n","[87]\ttrain-logloss:0.459814\teval-logloss:0.50155\n","[88]\ttrain-logloss:0.459466\teval-logloss:0.501423\n","[89]\ttrain-logloss:0.459257\teval-logloss:0.50145\n","[90]\ttrain-logloss:0.459089\teval-logloss:0.501479\n","[91]\ttrain-logloss:0.458903\teval-logloss:0.501391\n","[92]\ttrain-logloss:0.458603\teval-logloss:0.501214\n","[93]\ttrain-logloss:0.45836\teval-logloss:0.501117\n","[94]\ttrain-logloss:0.457453\teval-logloss:0.500408\n","[95]\ttrain-logloss:0.456933\teval-logloss:0.499933\n","[96]\ttrain-logloss:0.456649\teval-logloss:0.500088\n","[97]\ttrain-logloss:0.456449\teval-logloss:0.500156\n","[98]\ttrain-logloss:0.456316\teval-logloss:0.500139\n","[99]\ttrain-logloss:0.455979\teval-logloss:0.499928\n","[100]\ttrain-logloss:0.455474\teval-logloss:0.49967\n","[101]\ttrain-logloss:0.455174\teval-logloss:0.499252\n","[102]\ttrain-logloss:0.454956\teval-logloss:0.499366\n","[103]\ttrain-logloss:0.454834\teval-logloss:0.499539\n","[104]\ttrain-logloss:0.4545\teval-logloss:0.499385\n","[105]\ttrain-logloss:0.454234\teval-logloss:0.499524\n","[106]\ttrain-logloss:0.454051\teval-logloss:0.499721\n","[107]\ttrain-logloss:0.45383\teval-logloss:0.499782\n","[108]\ttrain-logloss:0.453544\teval-logloss:0.499709\n","[109]\ttrain-logloss:0.453258\teval-logloss:0.499655\n","[110]\ttrain-logloss:0.453066\teval-logloss:0.499477\n","[111]\ttrain-logloss:0.45234\teval-logloss:0.498979\n","[112]\ttrain-logloss:0.452107\teval-logloss:0.49882\n","[113]\ttrain-logloss:0.451775\teval-logloss:0.498584\n","[114]\ttrain-logloss:0.451252\teval-logloss:0.49844\n","[115]\ttrain-logloss:0.451096\teval-logloss:0.498428\n","[116]\ttrain-logloss:0.450918\teval-logloss:0.498305\n","[117]\ttrain-logloss:0.450746\teval-logloss:0.498194\n","[118]\ttrain-logloss:0.450517\teval-logloss:0.498162\n","[119]\ttrain-logloss:0.450378\teval-logloss:0.498131\n","[120]\ttrain-logloss:0.449729\teval-logloss:0.497856\n","[121]\ttrain-logloss:0.449554\teval-logloss:0.49788\n","[122]\ttrain-logloss:0.449414\teval-logloss:0.497689\n","[123]\ttrain-logloss:0.449159\teval-logloss:0.497708\n","[124]\ttrain-logloss:0.448933\teval-logloss:0.497632\n","[125]\ttrain-logloss:0.448657\teval-logloss:0.497416\n","[126]\ttrain-logloss:0.448337\teval-logloss:0.497051\n","[127]\ttrain-logloss:0.447911\teval-logloss:0.496796\n","[128]\ttrain-logloss:0.447772\teval-logloss:0.496621\n","[129]\ttrain-logloss:0.447544\teval-logloss:0.496511\n","[130]\ttrain-logloss:0.447095\teval-logloss:0.496283\n","[131]\ttrain-logloss:0.446764\teval-logloss:0.496169\n","[132]\ttrain-logloss:0.446487\teval-logloss:0.49608\n","[133]\ttrain-logloss:0.446103\teval-logloss:0.495957\n","[134]\ttrain-logloss:0.446006\teval-logloss:0.49593\n","[135]\ttrain-logloss:0.445828\teval-logloss:0.495997\n","[136]\ttrain-logloss:0.445403\teval-logloss:0.495981\n","[137]\ttrain-logloss:0.445123\teval-logloss:0.495773\n","[138]\ttrain-logloss:0.444886\teval-logloss:0.495527\n","[139]\ttrain-logloss:0.444578\teval-logloss:0.495389\n","[140]\ttrain-logloss:0.444302\teval-logloss:0.49537\n","[141]\ttrain-logloss:0.444104\teval-logloss:0.49535\n","[142]\ttrain-logloss:0.443918\teval-logloss:0.495285\n","[143]\ttrain-logloss:0.443786\teval-logloss:0.495225\n","[144]\ttrain-logloss:0.443557\teval-logloss:0.494971\n","[145]\ttrain-logloss:0.443311\teval-logloss:0.494883\n","[146]\ttrain-logloss:0.443016\teval-logloss:0.494547\n","[147]\ttrain-logloss:0.442788\teval-logloss:0.494456\n","[148]\ttrain-logloss:0.442568\teval-logloss:0.49451\n","[149]\ttrain-logloss:0.442501\teval-logloss:0.494489\n","[150]\ttrain-logloss:0.4423\teval-logloss:0.4944\n","[151]\ttrain-logloss:0.442049\teval-logloss:0.494432\n","[152]\ttrain-logloss:0.44183\teval-logloss:0.494328\n","[153]\ttrain-logloss:0.441598\teval-logloss:0.494222\n","[154]\ttrain-logloss:0.441376\teval-logloss:0.494292\n","[155]\ttrain-logloss:0.441134\teval-logloss:0.494168\n","[156]\ttrain-logloss:0.440963\teval-logloss:0.494057\n","[157]\ttrain-logloss:0.440686\teval-logloss:0.494143\n","[158]\ttrain-logloss:0.440563\teval-logloss:0.494244\n","[159]\ttrain-logloss:0.440475\teval-logloss:0.49429\n","[160]\ttrain-logloss:0.440378\teval-logloss:0.49436\n","[161]\ttrain-logloss:0.440007\teval-logloss:0.494297\n","[162]\ttrain-logloss:0.439766\teval-logloss:0.494304\n","[163]\ttrain-logloss:0.439584\teval-logloss:0.49436\n","[164]\ttrain-logloss:0.439392\teval-logloss:0.494232\n","[165]\ttrain-logloss:0.439067\teval-logloss:0.49415\n","[166]\ttrain-logloss:0.438885\teval-logloss:0.494066\n","[167]\ttrain-logloss:0.438829\teval-logloss:0.494055\n","[168]\ttrain-logloss:0.438761\teval-logloss:0.494051\n","[169]\ttrain-logloss:0.438676\teval-logloss:0.49401\n","[170]\ttrain-logloss:0.438492\teval-logloss:0.494061\n","[171]\ttrain-logloss:0.438375\teval-logloss:0.49411\n","[172]\ttrain-logloss:0.43816\teval-logloss:0.49408\n","[173]\ttrain-logloss:0.437905\teval-logloss:0.494094\n","[174]\ttrain-logloss:0.437525\teval-logloss:0.493911\n","[175]\ttrain-logloss:0.437231\teval-logloss:0.493884\n","[176]\ttrain-logloss:0.437017\teval-logloss:0.493914\n","[177]\ttrain-logloss:0.436757\teval-logloss:0.493812\n","[178]\ttrain-logloss:0.436481\teval-logloss:0.493859\n","[179]\ttrain-logloss:0.436122\teval-logloss:0.493771\n","[180]\ttrain-logloss:0.43576\teval-logloss:0.493614\n","[181]\ttrain-logloss:0.43543\teval-logloss:0.49353\n","[182]\ttrain-logloss:0.435196\teval-logloss:0.493635\n","[183]\ttrain-logloss:0.435127\teval-logloss:0.493655\n","[184]\ttrain-logloss:0.435011\teval-logloss:0.493571\n","[185]\ttrain-logloss:0.43473\teval-logloss:0.493598\n","[186]\ttrain-logloss:0.434521\teval-logloss:0.493589\n","[187]\ttrain-logloss:0.434326\teval-logloss:0.493515\n","[188]\ttrain-logloss:0.434275\teval-logloss:0.493501\n","[189]\ttrain-logloss:0.433986\teval-logloss:0.493389\n","[190]\ttrain-logloss:0.433937\teval-logloss:0.493398\n","[191]\ttrain-logloss:0.433869\teval-logloss:0.493343\n","[192]\ttrain-logloss:0.433597\teval-logloss:0.493405\n","[193]\ttrain-logloss:0.433445\teval-logloss:0.493461\n","[194]\ttrain-logloss:0.43333\teval-logloss:0.493438\n","[195]\ttrain-logloss:0.433257\teval-logloss:0.493438\n","[196]\ttrain-logloss:0.433165\teval-logloss:0.49344\n","[197]\ttrain-logloss:0.432979\teval-logloss:0.493489\n","[198]\ttrain-logloss:0.432741\teval-logloss:0.493391\n","[199]\ttrain-logloss:0.432541\teval-logloss:0.493284\n","[200]\ttrain-logloss:0.432292\teval-logloss:0.493158\n","[201]\ttrain-logloss:0.432055\teval-logloss:0.493146\n","[202]\ttrain-logloss:0.431824\teval-logloss:0.493141\n","[203]\ttrain-logloss:0.431592\teval-logloss:0.493155\n","[204]\ttrain-logloss:0.431249\teval-logloss:0.493071\n","[205]\ttrain-logloss:0.431153\teval-logloss:0.49304\n","[206]\ttrain-logloss:0.430847\teval-logloss:0.492985\n","[207]\ttrain-logloss:0.430782\teval-logloss:0.492883\n","[208]\ttrain-logloss:0.430669\teval-logloss:0.492925\n","[209]\ttrain-logloss:0.430617\teval-logloss:0.492961\n","[210]\ttrain-logloss:0.430531\teval-logloss:0.492924\n","[211]\ttrain-logloss:0.43032\teval-logloss:0.493104\n","[212]\ttrain-logloss:0.43021\teval-logloss:0.49309\n","[213]\ttrain-logloss:0.429975\teval-logloss:0.493199\n","[214]\ttrain-logloss:0.429697\teval-logloss:0.493156\n","[215]\ttrain-logloss:0.429582\teval-logloss:0.493152\n","[216]\ttrain-logloss:0.429492\teval-logloss:0.493189\n","[217]\ttrain-logloss:0.429318\teval-logloss:0.493091\n","[218]\ttrain-logloss:0.429127\teval-logloss:0.49298\n","[219]\ttrain-logloss:0.42883\teval-logloss:0.492786\n","[220]\ttrain-logloss:0.428713\teval-logloss:0.492794\n","[221]\ttrain-logloss:0.42847\teval-logloss:0.492675\n","[222]\ttrain-logloss:0.428056\teval-logloss:0.492507\n","[223]\ttrain-logloss:0.42781\teval-logloss:0.492419\n","[224]\ttrain-logloss:0.427567\teval-logloss:0.492389\n","[225]\ttrain-logloss:0.427312\teval-logloss:0.492315\n","[226]\ttrain-logloss:0.426945\teval-logloss:0.492294\n","[227]\ttrain-logloss:0.42669\teval-logloss:0.49226\n","[228]\ttrain-logloss:0.426615\teval-logloss:0.492243\n","[229]\ttrain-logloss:0.426381\teval-logloss:0.49218\n","[230]\ttrain-logloss:0.426197\teval-logloss:0.492287\n","[231]\ttrain-logloss:0.426081\teval-logloss:0.492345\n","[232]\ttrain-logloss:0.425925\teval-logloss:0.492251\n","[233]\ttrain-logloss:0.425827\teval-logloss:0.492346\n","[234]\ttrain-logloss:0.425618\teval-logloss:0.492452\n","[235]\ttrain-logloss:0.425394\teval-logloss:0.492237\n","[236]\ttrain-logloss:0.425363\teval-logloss:0.492192\n","[237]\ttrain-logloss:0.425221\teval-logloss:0.49217\n","[238]\ttrain-logloss:0.425124\teval-logloss:0.492212\n","[239]\ttrain-logloss:0.425003\teval-logloss:0.492173\n","[240]\ttrain-logloss:0.424789\teval-logloss:0.492021\n","[241]\ttrain-logloss:0.424758\teval-logloss:0.491969\n","[242]\ttrain-logloss:0.424661\teval-logloss:0.491883\n","[243]\ttrain-logloss:0.424579\teval-logloss:0.492056\n","[244]\ttrain-logloss:0.42436\teval-logloss:0.492118\n","[245]\ttrain-logloss:0.424035\teval-logloss:0.49211\n","[246]\ttrain-logloss:0.423979\teval-logloss:0.492042\n","[247]\ttrain-logloss:0.423777\teval-logloss:0.492146\n","[248]\ttrain-logloss:0.4234\teval-logloss:0.491925\n","[249]\ttrain-logloss:0.423064\teval-logloss:0.491672\n","[250]\ttrain-logloss:0.422846\teval-logloss:0.491533\n","[251]\ttrain-logloss:0.422618\teval-logloss:0.491644\n","[252]\ttrain-logloss:0.422435\teval-logloss:0.491588\n","[253]\ttrain-logloss:0.422248\teval-logloss:0.491571\n","[254]\ttrain-logloss:0.42197\teval-logloss:0.491556\n","[255]\ttrain-logloss:0.421839\teval-logloss:0.491596\n","[256]\ttrain-logloss:0.421814\teval-logloss:0.491598\n","[257]\ttrain-logloss:0.421655\teval-logloss:0.491612\n","[258]\ttrain-logloss:0.421453\teval-logloss:0.491556\n","[259]\ttrain-logloss:0.421262\teval-logloss:0.491407\n","[260]\ttrain-logloss:0.421081\teval-logloss:0.491261\n","[261]\ttrain-logloss:0.421036\teval-logloss:0.491318\n","[262]\ttrain-logloss:0.420714\teval-logloss:0.491314\n","[263]\ttrain-logloss:0.420524\teval-logloss:0.491573\n","[264]\ttrain-logloss:0.420334\teval-logloss:0.491527\n","[265]\ttrain-logloss:0.420244\teval-logloss:0.491481\n","[266]\ttrain-logloss:0.420038\teval-logloss:0.491547\n","[267]\ttrain-logloss:0.41997\teval-logloss:0.491514\n","[268]\ttrain-logloss:0.419825\teval-logloss:0.491552\n","[269]\ttrain-logloss:0.419682\teval-logloss:0.491504\n","[270]\ttrain-logloss:0.419396\teval-logloss:0.491299\n","[271]\ttrain-logloss:0.419229\teval-logloss:0.491365\n","[272]\ttrain-logloss:0.41918\teval-logloss:0.491347\n","[273]\ttrain-logloss:0.418916\teval-logloss:0.491335\n","[274]\ttrain-logloss:0.418703\teval-logloss:0.491188\n","[275]\ttrain-logloss:0.418535\teval-logloss:0.491015\n","[276]\ttrain-logloss:0.418328\teval-logloss:0.491058\n","[277]\ttrain-logloss:0.418089\teval-logloss:0.491134\n","[278]\ttrain-logloss:0.417871\teval-logloss:0.491107\n","[279]\ttrain-logloss:0.417767\teval-logloss:0.490985\n","[280]\ttrain-logloss:0.417733\teval-logloss:0.490905\n","[281]\ttrain-logloss:0.417555\teval-logloss:0.490909\n","[282]\ttrain-logloss:0.417342\teval-logloss:0.490953\n","[283]\ttrain-logloss:0.417096\teval-logloss:0.490983\n","[284]\ttrain-logloss:0.416895\teval-logloss:0.490967\n","[285]\ttrain-logloss:0.416855\teval-logloss:0.490926\n","[286]\ttrain-logloss:0.41669\teval-logloss:0.490794\n","[287]\ttrain-logloss:0.416493\teval-logloss:0.490797\n","[288]\ttrain-logloss:0.41639\teval-logloss:0.490791\n","[289]\ttrain-logloss:0.41635\teval-logloss:0.490785\n","[290]\ttrain-logloss:0.416277\teval-logloss:0.490688\n","[291]\ttrain-logloss:0.416146\teval-logloss:0.490782\n","[292]\ttrain-logloss:0.416013\teval-logloss:0.490942\n","[293]\ttrain-logloss:0.415797\teval-logloss:0.490882\n","[294]\ttrain-logloss:0.415502\teval-logloss:0.49084\n","[295]\ttrain-logloss:0.415337\teval-logloss:0.490792\n","[296]\ttrain-logloss:0.415104\teval-logloss:0.490718\n","[297]\ttrain-logloss:0.414888\teval-logloss:0.490593\n","[298]\ttrain-logloss:0.414704\teval-logloss:0.490462\n","[299]\ttrain-logloss:0.414471\teval-logloss:0.490396\n","[300]\ttrain-logloss:0.414205\teval-logloss:0.49025\n","[301]\ttrain-logloss:0.413958\teval-logloss:0.490365\n","[302]\ttrain-logloss:0.413765\teval-logloss:0.490328\n","[303]\ttrain-logloss:0.413574\teval-logloss:0.490293\n","[304]\ttrain-logloss:0.413427\teval-logloss:0.490386\n","[305]\ttrain-logloss:0.413363\teval-logloss:0.49039\n","[306]\ttrain-logloss:0.413334\teval-logloss:0.490307\n","[307]\ttrain-logloss:0.41313\teval-logloss:0.490381\n","[308]\ttrain-logloss:0.412962\teval-logloss:0.490322\n","[309]\ttrain-logloss:0.412729\teval-logloss:0.490233\n","[310]\ttrain-logloss:0.412397\teval-logloss:0.490179\n","[311]\ttrain-logloss:0.412158\teval-logloss:0.4902\n","[312]\ttrain-logloss:0.411934\teval-logloss:0.490207\n","[313]\ttrain-logloss:0.411764\teval-logloss:0.490201\n","[314]\ttrain-logloss:0.411553\teval-logloss:0.490204\n","[315]\ttrain-logloss:0.411321\teval-logloss:0.49009\n","[316]\ttrain-logloss:0.411105\teval-logloss:0.490027\n","[317]\ttrain-logloss:0.410882\teval-logloss:0.490074\n","[318]\ttrain-logloss:0.410746\teval-logloss:0.490079\n","[319]\ttrain-logloss:0.410606\teval-logloss:0.490185\n","[320]\ttrain-logloss:0.41048\teval-logloss:0.490251\n","[321]\ttrain-logloss:0.410412\teval-logloss:0.490259\n","[322]\ttrain-logloss:0.410362\teval-logloss:0.490353\n","[323]\ttrain-logloss:0.410175\teval-logloss:0.490277\n","[324]\ttrain-logloss:0.410094\teval-logloss:0.490322\n","[325]\ttrain-logloss:0.409876\teval-logloss:0.490236\n","[326]\ttrain-logloss:0.409686\teval-logloss:0.490169\n","[327]\ttrain-logloss:0.409473\teval-logloss:0.490192\n","[328]\ttrain-logloss:0.409224\teval-logloss:0.490013\n","[329]\ttrain-logloss:0.409184\teval-logloss:0.490111\n","[330]\ttrain-logloss:0.409066\teval-logloss:0.490054\n","[331]\ttrain-logloss:0.408952\teval-logloss:0.490107\n","[332]\ttrain-logloss:0.408709\teval-logloss:0.490001\n","[333]\ttrain-logloss:0.408645\teval-logloss:0.489972\n","[334]\ttrain-logloss:0.408377\teval-logloss:0.489942\n","[335]\ttrain-logloss:0.408166\teval-logloss:0.489828\n","[336]\ttrain-logloss:0.407921\teval-logloss:0.48972\n","[337]\ttrain-logloss:0.407739\teval-logloss:0.489747\n","[338]\ttrain-logloss:0.407676\teval-logloss:0.489733\n","[339]\ttrain-logloss:0.407432\teval-logloss:0.489773\n","[340]\ttrain-logloss:0.407065\teval-logloss:0.489817\n","[341]\ttrain-logloss:0.40689\teval-logloss:0.489784\n","[342]\ttrain-logloss:0.406662\teval-logloss:0.489973\n","[343]\ttrain-logloss:0.406386\teval-logloss:0.489942\n","[344]\ttrain-logloss:0.406173\teval-logloss:0.489753\n","[345]\ttrain-logloss:0.405947\teval-logloss:0.489869\n","[346]\ttrain-logloss:0.4058\teval-logloss:0.489821\n","[347]\ttrain-logloss:0.405614\teval-logloss:0.489803\n","[348]\ttrain-logloss:0.405422\teval-logloss:0.489719\n","[349]\ttrain-logloss:0.405205\teval-logloss:0.489807\n","[350]\ttrain-logloss:0.405122\teval-logloss:0.489728\n","[351]\ttrain-logloss:0.404869\teval-logloss:0.489682\n","[352]\ttrain-logloss:0.404625\teval-logloss:0.489531\n","[353]\ttrain-logloss:0.404428\teval-logloss:0.489621\n","[354]\ttrain-logloss:0.404249\teval-logloss:0.489584\n","[355]\ttrain-logloss:0.404084\teval-logloss:0.48948\n","[356]\ttrain-logloss:0.403984\teval-logloss:0.48942\n","[357]\ttrain-logloss:0.403857\teval-logloss:0.489485\n","[358]\ttrain-logloss:0.403821\teval-logloss:0.489465\n","[359]\ttrain-logloss:0.403709\teval-logloss:0.489505\n","[360]\ttrain-logloss:0.403463\teval-logloss:0.489518\n","[361]\ttrain-logloss:0.403397\teval-logloss:0.489459\n","[362]\ttrain-logloss:0.403115\teval-logloss:0.48923\n","[363]\ttrain-logloss:0.40288\teval-logloss:0.48914\n","[364]\ttrain-logloss:0.402661\teval-logloss:0.48919\n","[365]\ttrain-logloss:0.402516\teval-logloss:0.48909\n","[366]\ttrain-logloss:0.402427\teval-logloss:0.489078\n","[367]\ttrain-logloss:0.402387\teval-logloss:0.489163\n","[368]\ttrain-logloss:0.402234\teval-logloss:0.489057\n","[369]\ttrain-logloss:0.402029\teval-logloss:0.489077\n","[370]\ttrain-logloss:0.401896\teval-logloss:0.488982\n","[371]\ttrain-logloss:0.401798\teval-logloss:0.489011\n","[372]\ttrain-logloss:0.401666\teval-logloss:0.488997\n","[373]\ttrain-logloss:0.401515\teval-logloss:0.488962\n","[374]\ttrain-logloss:0.40146\teval-logloss:0.488997\n","[375]\ttrain-logloss:0.401412\teval-logloss:0.488992\n","[376]\ttrain-logloss:0.401361\teval-logloss:0.488959\n","[377]\ttrain-logloss:0.401226\teval-logloss:0.488906\n","[378]\ttrain-logloss:0.401027\teval-logloss:0.488827\n","[379]\ttrain-logloss:0.40098\teval-logloss:0.488767\n","[380]\ttrain-logloss:0.400765\teval-logloss:0.488667\n","[381]\ttrain-logloss:0.400687\teval-logloss:0.488638\n","[382]\ttrain-logloss:0.40051\teval-logloss:0.488577\n","[383]\ttrain-logloss:0.400359\teval-logloss:0.488505\n","[384]\ttrain-logloss:0.400155\teval-logloss:0.488415\n","[385]\ttrain-logloss:0.400003\teval-logloss:0.488402\n","[386]\ttrain-logloss:0.399875\teval-logloss:0.488502\n","[387]\ttrain-logloss:0.399648\teval-logloss:0.488468\n","[388]\ttrain-logloss:0.399482\teval-logloss:0.488518\n","[389]\ttrain-logloss:0.399286\teval-logloss:0.48846\n","[390]\ttrain-logloss:0.399104\teval-logloss:0.488264\n","[391]\ttrain-logloss:0.39898\teval-logloss:0.48821\n","[392]\ttrain-logloss:0.398767\teval-logloss:0.488188\n","[393]\ttrain-logloss:0.398609\teval-logloss:0.488226\n","[394]\ttrain-logloss:0.398463\teval-logloss:0.488263\n","[395]\ttrain-logloss:0.398402\teval-logloss:0.488184\n","[396]\ttrain-logloss:0.398361\teval-logloss:0.488255\n","[397]\ttrain-logloss:0.398281\teval-logloss:0.488335\n","[398]\ttrain-logloss:0.398244\teval-logloss:0.488346\n","[399]\ttrain-logloss:0.397992\teval-logloss:0.488306\n","[400]\ttrain-logloss:0.3978\teval-logloss:0.488304\n","[401]\ttrain-logloss:0.397636\teval-logloss:0.488403\n","[402]\ttrain-logloss:0.397472\teval-logloss:0.488192\n","[403]\ttrain-logloss:0.397282\teval-logloss:0.488263\n","[404]\ttrain-logloss:0.397141\teval-logloss:0.488243\n","[405]\ttrain-logloss:0.39705\teval-logloss:0.488231\n","[406]\ttrain-logloss:0.396936\teval-logloss:0.488289\n","[407]\ttrain-logloss:0.396649\teval-logloss:0.488123\n","[408]\ttrain-logloss:0.39649\teval-logloss:0.488033\n","[409]\ttrain-logloss:0.396241\teval-logloss:0.487974\n","[410]\ttrain-logloss:0.396002\teval-logloss:0.487896\n","[411]\ttrain-logloss:0.395832\teval-logloss:0.488015\n","[412]\ttrain-logloss:0.395704\teval-logloss:0.487909\n","[413]\ttrain-logloss:0.395495\teval-logloss:0.487731\n","[414]\ttrain-logloss:0.395263\teval-logloss:0.487633\n","[415]\ttrain-logloss:0.395003\teval-logloss:0.487597\n","[416]\ttrain-logloss:0.39481\teval-logloss:0.487566\n","[417]\ttrain-logloss:0.394663\teval-logloss:0.487536\n","[418]\ttrain-logloss:0.394465\teval-logloss:0.487442\n","[419]\ttrain-logloss:0.394411\teval-logloss:0.48733\n","[420]\ttrain-logloss:0.394187\teval-logloss:0.487044\n","[421]\ttrain-logloss:0.394141\teval-logloss:0.487053\n","[422]\ttrain-logloss:0.393938\teval-logloss:0.48703\n","[423]\ttrain-logloss:0.393794\teval-logloss:0.486956\n","[424]\ttrain-logloss:0.393688\teval-logloss:0.486999\n","[425]\ttrain-logloss:0.393553\teval-logloss:0.486913\n","[426]\ttrain-logloss:0.39352\teval-logloss:0.486998\n","[427]\ttrain-logloss:0.393414\teval-logloss:0.487101\n","[428]\ttrain-logloss:0.393282\teval-logloss:0.487111\n","[429]\ttrain-logloss:0.39305\teval-logloss:0.487062\n","[430]\ttrain-logloss:0.392855\teval-logloss:0.487151\n","[431]\ttrain-logloss:0.392756\teval-logloss:0.487235\n","[432]\ttrain-logloss:0.392651\teval-logloss:0.487202\n","[433]\ttrain-logloss:0.392469\teval-logloss:0.487201\n","[434]\ttrain-logloss:0.392257\teval-logloss:0.487324\n","[435]\ttrain-logloss:0.392108\teval-logloss:0.487322\n","[436]\ttrain-logloss:0.392082\teval-logloss:0.487301\n","[437]\ttrain-logloss:0.391822\teval-logloss:0.487178\n","[438]\ttrain-logloss:0.391656\teval-logloss:0.487099\n","[439]\ttrain-logloss:0.391609\teval-logloss:0.487157\n","[440]\ttrain-logloss:0.391515\teval-logloss:0.487175\n","[441]\ttrain-logloss:0.391414\teval-logloss:0.487141\n","[442]\ttrain-logloss:0.391267\teval-logloss:0.487079\n","[443]\ttrain-logloss:0.391036\teval-logloss:0.486939\n","[444]\ttrain-logloss:0.390847\teval-logloss:0.486817\n","[445]\ttrain-logloss:0.39068\teval-logloss:0.486853\n","[446]\ttrain-logloss:0.390485\teval-logloss:0.486967\n","[447]\ttrain-logloss:0.390452\teval-logloss:0.48696\n","[448]\ttrain-logloss:0.390304\teval-logloss:0.487071\n","[449]\ttrain-logloss:0.390232\teval-logloss:0.487162\n","[450]\ttrain-logloss:0.390034\teval-logloss:0.487158\n","[451]\ttrain-logloss:0.389969\teval-logloss:0.487254\n","[452]\ttrain-logloss:0.389919\teval-logloss:0.487245\n","[453]\ttrain-logloss:0.389709\teval-logloss:0.487364\n","[454]\ttrain-logloss:0.389556\teval-logloss:0.487356\n","[455]\ttrain-logloss:0.389419\teval-logloss:0.48729\n","[456]\ttrain-logloss:0.389333\teval-logloss:0.487267\n","[457]\ttrain-logloss:0.389232\teval-logloss:0.487345\n","[458]\ttrain-logloss:0.389057\teval-logloss:0.487254\n","[459]\ttrain-logloss:0.38894\teval-logloss:0.487275\n","[460]\ttrain-logloss:0.388628\teval-logloss:0.487078\n","[461]\ttrain-logloss:0.388396\teval-logloss:0.487009\n","[462]\ttrain-logloss:0.388265\teval-logloss:0.487053\n","[463]\ttrain-logloss:0.388169\teval-logloss:0.486946\n","[464]\ttrain-logloss:0.388032\teval-logloss:0.48688\n","[465]\ttrain-logloss:0.38786\teval-logloss:0.486878\n","[466]\ttrain-logloss:0.387823\teval-logloss:0.486858\n","[467]\ttrain-logloss:0.387656\teval-logloss:0.486908\n","[468]\ttrain-logloss:0.38761\teval-logloss:0.486904\n","[469]\ttrain-logloss:0.387393\teval-logloss:0.486769\n","[470]\ttrain-logloss:0.387224\teval-logloss:0.486849\n","[471]\ttrain-logloss:0.387111\teval-logloss:0.486656\n","[472]\ttrain-logloss:0.38689\teval-logloss:0.486714\n","[473]\ttrain-logloss:0.386765\teval-logloss:0.486746\n","[474]\ttrain-logloss:0.386558\teval-logloss:0.4867\n","[475]\ttrain-logloss:0.386332\teval-logloss:0.486579\n","[476]\ttrain-logloss:0.386196\teval-logloss:0.486635\n","[477]\ttrain-logloss:0.38605\teval-logloss:0.486546\n","[478]\ttrain-logloss:0.385858\teval-logloss:0.486432\n","[479]\ttrain-logloss:0.385751\teval-logloss:0.486333\n","[480]\ttrain-logloss:0.385581\teval-logloss:0.486272\n","[481]\ttrain-logloss:0.385434\teval-logloss:0.48631\n","[482]\ttrain-logloss:0.385291\teval-logloss:0.486391\n","[483]\ttrain-logloss:0.385093\teval-logloss:0.486513\n","[484]\ttrain-logloss:0.384876\teval-logloss:0.486331\n","[485]\ttrain-logloss:0.384689\teval-logloss:0.486346\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RZNVLmq-X1Yj","colab_type":"code","outputId":"a39e522c-6b68-4153-e2a0-ff69644b8864","executionInfo":{"status":"ok","timestamp":1573356968251,"user_tz":-480,"elapsed":811,"user":{"displayName":"陆韡","photoUrl":"","userId":"00756817164624838101"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# import collections\n","# print([item for item, count in collections.Counter(all_data.columns).items() if count > 1])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["['both_contain_number', 'contain_number', 'contain_$', 'people']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dz2Te7BguSa8","colab_type":"code","colab":{}},"source":["preds = predict_xgb(xgbt, all_data_test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zJzV4CJlOnkI","colab_type":"code","colab":{}},"source":["# We try to use the most important 40 features in the model and get a better performance in Xgboost. \n","# But we find that the performance of using 40 features is worse than using all the five hundred features.\n","# We feel confused here because we suppose with less while strong features, the model should perform better.\n","import operator\n","importance = clr.get_fscore()\n","importance = sorted(importance.items(), key=operator.itemgetter(1))\n","\n","list = []\n","for i in range(40):\n","    list.append(importance[-i][0])\n","\n","clr = train_xgb(all_data_clean[list], y_train, params)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SWTOf2xZPPrt","colab_type":"code","colab":{}},"source":["cols = ['state',\n"," 'word_match',\n"," '300d_glove_braycurtis',\n"," 'w2v_wmdmodel_w2v_1_160_7',\n"," '50d_glove_braycurtis',\n"," 'tfidf_sim',\n"," '200d_glove_canberra',\n"," 'w2v_wmdmodel_w2v_2_190_5',\n"," '200d_glove_wmd_1',\n"," 'stops2_ratio',\n"," '50d_glove_canberra',\n"," '100d_glove_minkowski',\n"," '300d_glove_canberra',\n"," 'stops1_ratio',\n"," '100d_glove_wmd_1',\n"," 'w2v_wmdmodel_w2v_1_40_7',\n"," 'tfidf_word_match',\n"," 'diff_avg_word',\n"," 'avg_world_len1',\n"," '50d_glove_cosine',\n"," 'same_words.1',\n"," 'len_q2',\n"," 'len_q1',\n"," '50d_glove_wmd_1',\n"," 'diff_len',\n"," 'avg_world_len2',\n"," 'w2v_wmdmodel_w2v_1_70_7',\n"," '300d_glove_cityblock',\n"," 'diff_stops_r',\n"," 'w2v_wmdmodel_w2v_1_40_3']\n","\n","for var in combinations(cols, 25):\n","    li = list(var)\n","\n","    model = train_xgb_silent(x[li], y_train, params)\n","    pred = predict_xgb(model, X_val[li])\n","    logloss = log_loss(y_val, pred)\n","    print(logloss)\n","    if logloss<0.48:\n","        print(logloss, li)\n","\n","    del li"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yMJ5Hzczgx0f","colab_type":"text"},"source":["#### LightGBM"]},{"cell_type":"code","metadata":{"id":"knObzzAGjkuv","colab_type":"code","colab":{}},"source":["x, X_val, y, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=47)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bovGGGTOGaPN","colab_type":"code","outputId":"7d7283ed-ac98-4c0d-e775-e16da79f9421","executionInfo":{"status":"ok","timestamp":1573021144253,"user_tz":-480,"elapsed":1646987,"user":{"displayName":"陆韡","photoUrl":"","userId":"00756817164624838101"}},"colab":{"base_uri":"https://localhost:8080/","height":107}},"source":["parameters = {\n","              'max_depth': [6, 7, 8],\n","              'learning_rate': [0.09, 0.11, 0.12],\n","              #'feature_fraction': [0.6, 0.7, 0.8, 0.9, 0.95],\n","              #'bagging_fraction': [0.6, 0.7, 0.8, 0.9, 0.95],\n","              #'bagging_freq': [2, 4, 5, 6, 8],\n","              'lambda_l1': [0.1, 0.4, 0.6],\n","              #'lambda_l2': [0, 10, 15, 35, 40],\n","              #'cat_smooth': [1, 10, 15, 20, 35]\n","}\n","\n","gbm = lgb.LGBMClassifier(boosting_type='gbdt',\n","                         objective = 'binary',\n","                         metric = 'logloss',\n","                         verbose = 0,\n","                         max_depth = 5,\n","                         learning_rate = 0.11,\n","                         feature_fraction = 0.9)\n","                         #num_leaves = 35,\n","                         #lambda_l1= 0.6,\n","                         #lambda_l2= 0)\n","\n","\n","gsearch = GridSearchCV(gbm, param_grid=parameters, scoring='accuracy', cv=3) #neg_log_loss\n","gsearch.fit(x, y)\n","\n","print(\"Best score: %0.3f\" % gsearch.best_score_)\n","print(\"Best parameters set:\")\n","best_parameters = gsearch.best_estimator_.get_params()\n","for param_name in sorted(parameters.keys()):\n","    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Best score: 0.720\n","Best parameters set:\n","\tlambda_l1: 0.1\n","\tlearning_rate: 0.12\n","\tmax_depth: 7\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KCCI1oS-g1GF","colab_type":"code","outputId":"e455b315-083e-4e71-a8ac-ec7172d5541e","executionInfo":{"status":"ok","timestamp":1573060135451,"user_tz":-480,"elapsed":18111,"user":{"displayName":"陆韡","photoUrl":"","userId":"00756817164624838101"}},"colab":{"base_uri":"https://localhost:8080/","height":143}},"source":["lgb_model = lgb.LGBMClassifier(max_depth=7, learning_rate=0.12, objective='binary', metric='binary_logloss', lambda_l1=0.1)\n","x, X_val, y, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=47)\n","lgb_model.fit(x, y)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n","               importance_type='split', lambda_l1=0.1, learning_rate=0.12,\n","               max_depth=7, metric='binary_logloss', min_child_samples=20,\n","               min_child_weight=0.001, min_split_gain=0.0, n_estimators=100,\n","               n_jobs=-1, num_leaves=31, objective='binary', random_state=None,\n","               reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,\n","               subsample_for_bin=200000, subsample_freq=0)"]},"metadata":{"tags":[]},"execution_count":63}]},{"cell_type":"code","metadata":{"id":"xQlUWCTAFIBl","colab_type":"code","outputId":"ad71db66-32ab-4173-8a31-e5dba256d57c","executionInfo":{"status":"ok","timestamp":1573060135779,"user_tz":-480,"elapsed":14347,"user":{"displayName":"陆韡","photoUrl":"","userId":"00756817164624838101"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["pred = lgb_model.predict_proba(X_val)\n","log_loss(y_val, pred[:,1])"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.5446318431163707"]},"metadata":{"tags":[]},"execution_count":64}]},{"cell_type":"code","metadata":{"id":"10cL-qAtX3DE","colab_type":"code","colab":{}},"source":["importances = model.feature_importances_\n","indices = np.argsort(importances)[::-1]\n","\n","feat_labels = x_train.columns\n","for f in range(x_train.shape[1]):\n","    print(\"%2d) %-*s %f\" % (f + 1, 30, feat_labels[indices[f]], importances[indices[f]]))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bDonq6AEkP5O","colab_type":"code","outputId":"3163ede8-378f-4d10-8f72-bed967e313e0","executionInfo":{"status":"ok","timestamp":1573058808508,"user_tz":-480,"elapsed":365,"user":{"displayName":"陆韡","photoUrl":"","userId":"00756817164624838101"}},"colab":{"base_uri":"https://localhost:8080/","height":467}},"source":["indices_ = indices[0:50]\n","X_train_model = x.iloc[:,indices_]\n","X_test_model = X_val.iloc[:,indices_]\n","X_train_model.columns"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['word_match', 'w2v_wmdmodel_w2v_1_160_7', 'tfidf_sim',\n","       'tfidf_word_match', 'w2v_wmdmodel_w2v_2_190_5', 'stops2_ratio',\n","       'w2v_wmdmodel_w2v_1_40_7', 'len_q1', 'len_q2',\n","       'cosine_distance_model_w2v_2_190_7', 'avg_world_len2', 'stops1_ratio',\n","       'same_words.1', 'bow_minkowski', 'avg_world_len1', 'bow_cosine',\n","       'w2v_wmdmodel_w2v_1_70_7', 'diff_avg_word', 'bow_canberra',\n","       'len_word_q1', 'w2v_wmdmodel_w2v_1_100_3', 'w2v_wmdmodel_w2v_1_70_3',\n","       'minkowski_distance_model_w2v_2_40_7', 'contain_number',\n","       'bow_cityblock', 'minkowski_distance_model_w2v_2_130_7',\n","       'minkowski_distance_model_w2v_2_100_7',\n","       'minkowski_distance_model_w2v_2_300_5', 'len_word_q2',\n","       'w2v_wmdmodel_w2v_1_160_3', 'diff_len', 'bow_euclidean', 'diff_stops_r',\n","       'w2v_wmdmodel_w2v_1_40_3', 'canberra_distance_model_w2v_1_40_7',\n","       'w2v_wmdmodel_w2v_1_70_5', 'w2v_wmdmodel_w2v_1_130_5', 'bow_jaccard',\n","       'canberra_distance_model_w2v_1_100_7',\n","       'minkowski_distance_model_w2v_2_40_3',\n","       'braycurtis_distance_model_w2v_2_300_5',\n","       'canberra_distance_model_w2v_1_100_3',\n","       'canberra_distance_model_w2v_2_40_3', 'w2v_wmdmodel_w2v_1_40_5',\n","       'canberra_distance_model_w2v_2_70_3',\n","       'braycurtis_distance_model_w2v_2_130_3',\n","       'canberra_distance_model_w2v_1_160_3',\n","       'canberra_distance_model_w2v_2_70_7', 'diff_len_word',\n","       'canberra_distance_model_w2v_1_130_3'],\n","      dtype='object')"]},"metadata":{"tags":[]},"execution_count":46}]},{"cell_type":"code","metadata":{"id":"CL14UOKhkeBb","colab_type":"code","colab":{}},"source":["cols = ['word_match', 'w2v_wmdmodel_w2v_1_160_7', 'tfidf_sim',\n","       'tfidf_word_match', 'w2v_wmdmodel_w2v_2_190_5', 'stops2_ratio',\n","       'w2v_wmdmodel_w2v_1_40_7', 'len_q1', 'len_q2',\n","       'cosine_distance_model_w2v_2_190_7', 'avg_world_len2', 'stops1_ratio',\n","       'same_words.1', 'bow_minkowski', 'avg_world_len1', 'bow_cosine',\n","       'w2v_wmdmodel_w2v_1_70_7', 'diff_avg_word', 'bow_canberra',\n","       'len_word_q1', 'w2v_wmdmodel_w2v_1_100_3', 'w2v_wmdmodel_w2v_1_70_3',\n","       'minkowski_distance_model_w2v_2_40_7', 'contain_number',\n","       'bow_cityblock', 'minkowski_distance_model_w2v_2_130_7',\n","       'minkowski_distance_model_w2v_2_100_7',\n","       'minkowski_distance_model_w2v_2_300_5', 'len_word_q2',\n","       'w2v_wmdmodel_w2v_1_160_3', 'diff_len', 'bow_euclidean', 'diff_stops_r',\n","       'w2v_wmdmodel_w2v_1_40_3', 'canberra_distance_model_w2v_1_40_7',\n","       'w2v_wmdmodel_w2v_1_70_5', 'w2v_wmdmodel_w2v_1_130_5', 'bow_jaccard',\n","       'canberra_distance_model_w2v_1_100_7',\n","       'minkowski_distance_model_w2v_2_40_3',\n","       'braycurtis_distance_model_w2v_2_300_5',\n","       'canberra_distance_model_w2v_1_100_3',\n","       'canberra_distance_model_w2v_2_40_3', 'w2v_wmdmodel_w2v_1_40_5',\n","       'canberra_distance_model_w2v_2_70_3',\n","       'braycurtis_distance_model_w2v_2_130_3',\n","       'canberra_distance_model_w2v_1_160_3',\n","       'canberra_distance_model_w2v_2_70_7', 'diff_len_word',\n","       'canberra_distance_model_w2v_1_130_3']\n","\n","for var in combinations(cols, 45):\n","    li = list(var)\n","    X_train_model_3 = X_train_model[li]\n","    X_test_model_3 = X_test_model[li]\n","\n","    model = lgb.LGBMClassifier(max_depth=7, learning_rate=0.12, objective='binary', metric='binary_logloss', lambda_l1=0.1)\n","    model.fit(X_train_model_3, y)\n","    pred = model.predict_proba(X_val[li])\n","    logloss = log_loss(y_val, pred[:,1])\n","    print(logloss)\n","    if logloss<0.544:\n","        print(logloss, li)\n","\n","    del li"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6StkpHkjJWoH","colab_type":"text"},"source":["#### DNN"]},{"cell_type":"code","metadata":{"id":"DmwxOyY7gxVF","colab_type":"code","colab":{}},"source":["x = all_data_clean.iloc[:,1:].values#[impt_features]\n","y = all_data_clean.iloc[:,0].values\n","x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EvETn8g2Jp1m","colab_type":"code","outputId":"da57bd87-e25e-4584-b625-19571266dbd1","executionInfo":{"status":"ok","timestamp":1573147658284,"user_tz":-480,"elapsed":2189545,"user":{"displayName":"陆韡","photoUrl":"","userId":"00756817164624838101"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# Build the neural network\n","model = Sequential()\n","\n","model.add(Dense(150, input_dim=x.shape[1], activation='relu'))\n","model.add(BatchNormalization())\n","#model.add(activation='relu')\n","model.add(Dense(50, activation='relu'))\n","model.add(Dropout(0.2))\n","\n","model.add(BatchNormalization())\n","model.add(Dense(20, activation='relu'))\n","model.add(Dropout(0.2))\n","\n","model.add(Dense(1))\n","Adam = optimizers.Adam(lr=0.00003, beta_1=0.9, beta_2=0.999, amsgrad=False)\n","#SGD = optimizers.SGD(learning_rate=0.01, momentum=0.5, nesterov=True)\n","model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['acc'])\n","# optimizer: Nadam/ Adam\n","\n","monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=100, verbose=1, mode='auto', restore_best_weights=True)\n","history = model.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor],verbose=2,epochs=200)\n","\n","pred = model.predict(x_test)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train on 103324 samples, validate on 25832 samples\n","Epoch 1/200\n","103324/103324 - 22s - loss: 0.7392 - acc: 0.5016 - val_loss: 0.6947 - val_acc: 0.4999\n","Epoch 2/200\n","103324/103324 - 22s - loss: 0.6976 - acc: 0.4987 - val_loss: 0.6932 - val_acc: 0.4999\n","Epoch 3/200\n","103324/103324 - 22s - loss: 0.6945 - acc: 0.4998 - val_loss: 0.6932 - val_acc: 0.4999\n","Epoch 4/200\n","103324/103324 - 21s - loss: 0.6935 - acc: 0.5005 - val_loss: 0.6932 - val_acc: 0.5001\n","Epoch 5/200\n","103324/103324 - 21s - loss: 0.6933 - acc: 0.5022 - val_loss: 0.6933 - val_acc: 0.5001\n","Epoch 6/200\n","103324/103324 - 21s - loss: 0.6933 - acc: 0.4986 - val_loss: 0.6933 - val_acc: 0.4999\n","Epoch 7/200\n","103324/103324 - 21s - loss: 0.6932 - acc: 0.4999 - val_loss: 0.6932 - val_acc: 0.4999\n","Epoch 8/200\n","103324/103324 - 21s - loss: 0.6932 - acc: 0.5002 - val_loss: 0.6932 - val_acc: 0.5001\n","Epoch 9/200\n","103324/103324 - 21s - loss: 0.6933 - acc: 0.4983 - val_loss: 0.6932 - val_acc: 0.4999\n","Epoch 10/200\n","103324/103324 - 21s - loss: 0.6932 - acc: 0.5003 - val_loss: 0.6934 - val_acc: 0.4999\n","Epoch 11/200\n","103324/103324 - 22s - loss: 0.6933 - acc: 0.4973 - val_loss: 0.6932 - val_acc: 0.4999\n","Epoch 12/200\n","103324/103324 - 21s - loss: 0.6932 - acc: 0.5009 - val_loss: 0.6933 - val_acc: 0.5001\n","Epoch 13/200\n","103324/103324 - 21s - loss: 0.6932 - acc: 0.5025 - val_loss: 0.6932 - val_acc: 0.5001\n","Epoch 14/200\n","103324/103324 - 21s - loss: 0.6932 - acc: 0.4994 - val_loss: 0.6932 - val_acc: 0.4999\n","Epoch 15/200\n","103324/103324 - 21s - loss: 0.6933 - acc: 0.4981 - val_loss: 0.6932 - val_acc: 0.4999\n","Epoch 16/200\n","103324/103324 - 21s - loss: 0.6933 - acc: 0.4985 - val_loss: 0.6932 - val_acc: 0.4999\n","Epoch 17/200\n","103324/103324 - 22s - loss: 0.6933 - acc: 0.4989 - val_loss: 0.6934 - val_acc: 0.4999\n","Epoch 18/200\n","103324/103324 - 22s - loss: 0.6932 - acc: 0.5009 - val_loss: 0.6943 - val_acc: 0.4999\n","Epoch 19/200\n","103324/103324 - 22s - loss: 0.6932 - acc: 0.5000 - val_loss: 0.6932 - val_acc: 0.4999\n","Epoch 20/200\n","103324/103324 - 21s - loss: 0.6932 - acc: 0.4995 - val_loss: 0.6931 - val_acc: 0.4999\n","Epoch 21/200\n","103324/103324 - 22s - loss: 0.6932 - acc: 0.4980 - val_loss: 0.6934 - val_acc: 0.5001\n","Epoch 22/200\n","103324/103324 - 22s - loss: 0.6932 - acc: 0.4999 - val_loss: 0.6932 - val_acc: 0.4999\n","Epoch 23/200\n","103324/103324 - 22s - loss: 0.6933 - acc: 0.4970 - val_loss: 0.6932 - val_acc: 0.4999\n","Epoch 24/200\n","103324/103324 - 22s - loss: 0.6932 - acc: 0.5023 - val_loss: 0.6933 - val_acc: 0.4999\n","Epoch 25/200\n","103324/103324 - 22s - loss: 0.6932 - acc: 0.5006 - val_loss: 0.6934 - val_acc: 0.4999\n","Epoch 26/200\n","103324/103324 - 22s - loss: 0.6932 - acc: 0.5000 - val_loss: 0.6931 - val_acc: 0.5001\n","Epoch 27/200\n","103324/103324 - 22s - loss: 0.6932 - acc: 0.4994 - val_loss: 0.6932 - val_acc: 0.5001\n","Epoch 28/200\n","103324/103324 - 21s - loss: 0.6932 - acc: 0.5007 - val_loss: 0.6932 - val_acc: 0.4999\n","Epoch 29/200\n","103324/103324 - 22s - loss: 0.6932 - acc: 0.4997 - val_loss: 0.6932 - val_acc: 0.5001\n","Epoch 30/200\n","103324/103324 - 21s - loss: 0.6932 - acc: 0.4979 - val_loss: 0.6932 - val_acc: 0.5001\n","Epoch 31/200\n","103324/103324 - 21s - loss: 0.6932 - acc: 0.4999 - val_loss: 0.6933 - val_acc: 0.4999\n","Epoch 32/200\n","103324/103324 - 22s - loss: 0.6932 - acc: 0.4979 - val_loss: 0.6931 - val_acc: 0.4999\n","Epoch 33/200\n","103324/103324 - 21s - loss: 0.6932 - acc: 0.5001 - val_loss: 0.6932 - val_acc: 0.5001\n","Epoch 34/200\n","103324/103324 - 22s - loss: 0.6932 - acc: 0.5004 - val_loss: 0.6932 - val_acc: 0.5001\n","Epoch 35/200\n","103324/103324 - 21s - loss: 0.6932 - acc: 0.4992 - val_loss: 0.6931 - val_acc: 0.5001\n","Epoch 36/200\n","103324/103324 - 22s - loss: 0.6932 - acc: 0.5007 - val_loss: 0.6932 - val_acc: 0.4999\n","Epoch 37/200\n","103324/103324 - 22s - loss: 0.6933 - acc: 0.4982 - val_loss: 0.6932 - val_acc: 0.5001\n","Epoch 38/200\n","103324/103324 - 22s - loss: 0.6933 - acc: 0.4994 - val_loss: 0.6932 - val_acc: 0.5001\n","Epoch 39/200\n","103324/103324 - 22s - loss: 0.6932 - acc: 0.5016 - val_loss: 0.6932 - val_acc: 0.4999\n","Epoch 40/200\n","103324/103324 - 22s - loss: 0.6933 - acc: 0.4975 - val_loss: 0.6931 - val_acc: 0.4999\n","Epoch 41/200\n","103324/103324 - 21s - loss: 0.6932 - acc: 0.5002 - val_loss: 0.6932 - val_acc: 0.4999\n","Epoch 42/200\n","103324/103324 - 21s - loss: 0.6932 - acc: 0.5010 - val_loss: 0.6934 - val_acc: 0.4999\n","Epoch 43/200\n","103324/103324 - 22s - loss: 0.6932 - acc: 0.5008 - val_loss: 0.6931 - val_acc: 0.5001\n","Epoch 44/200\n","103324/103324 - 21s - loss: 0.6932 - acc: 0.5010 - val_loss: 0.6933 - val_acc: 0.4999\n","Epoch 45/200\n","103324/103324 - 22s - loss: 0.6932 - acc: 0.5012 - val_loss: 0.6933 - val_acc: 0.4999\n","Epoch 46/200\n","103324/103324 - 22s - loss: 0.6932 - acc: 0.5008 - val_loss: 0.6933 - val_acc: 0.4999\n","Epoch 47/200\n","103324/103324 - 22s - loss: 0.6932 - acc: 0.5006 - val_loss: 0.6932 - val_acc: 0.4999\n","Epoch 48/200\n","103324/103324 - 21s - loss: 0.6932 - acc: 0.5001 - val_loss: 0.6932 - val_acc: 0.4999\n","Epoch 49/200\n","103324/103324 - 21s - loss: 0.6932 - acc: 0.5005 - val_loss: 0.6932 - val_acc: 0.5001\n","Epoch 50/200\n","103324/103324 - 21s - loss: 0.6932 - acc: 0.5000 - val_loss: 0.6932 - val_acc: 0.4999\n","Epoch 51/200\n","103324/103324 - 21s - loss: 0.6933 - acc: 0.4982 - val_loss: 0.6932 - val_acc: 0.5001\n","Epoch 52/200\n","103324/103324 - 21s - loss: 0.6933 - acc: 0.4991 - val_loss: 0.6931 - val_acc: 0.4999\n","Epoch 53/200\n","103324/103324 - 22s - loss: 0.6932 - acc: 0.5008 - val_loss: 0.6932 - val_acc: 0.5001\n","Epoch 54/200\n","103324/103324 - 22s - loss: 0.6933 - acc: 0.4973 - val_loss: 0.6931 - val_acc: 0.4999\n","Epoch 55/200\n","103324/103324 - 22s - loss: 0.6933 - acc: 0.4986 - val_loss: 0.6932 - val_acc: 0.4999\n","Epoch 56/200\n","103324/103324 - 22s - loss: 0.6932 - acc: 0.5005 - val_loss: 0.6932 - val_acc: 0.5001\n","Epoch 57/200\n","103324/103324 - 21s - loss: 0.6933 - acc: 0.5013 - val_loss: 0.6932 - val_acc: 0.4999\n","Epoch 58/200\n","103324/103324 - 21s - loss: 0.6932 - acc: 0.5032 - val_loss: 0.6934 - val_acc: 0.5001\n","Epoch 59/200\n","103324/103324 - 21s - loss: 0.6932 - acc: 0.5019 - val_loss: 0.6931 - val_acc: 0.4999\n","Epoch 60/200\n","103324/103324 - 22s - loss: 0.6932 - acc: 0.5029 - val_loss: 0.6933 - val_acc: 0.5001\n","Epoch 61/200\n","103324/103324 - 21s - loss: 0.6933 - acc: 0.4988 - val_loss: 0.6932 - val_acc: 0.5001\n","Epoch 62/200\n","103324/103324 - 21s - loss: 0.6932 - acc: 0.5002 - val_loss: 0.6931 - val_acc: 0.5001\n","Epoch 63/200\n","103324/103324 - 21s - loss: 0.6933 - acc: 0.4986 - val_loss: 0.6932 - val_acc: 0.5001\n","Epoch 64/200\n","103324/103324 - 21s - loss: 0.6932 - acc: 0.5008 - val_loss: 0.6937 - val_acc: 0.4999\n","Epoch 65/200\n","103324/103324 - 21s - loss: 0.6932 - acc: 0.4989 - val_loss: 0.6932 - val_acc: 0.4999\n","Epoch 66/200\n","103324/103324 - 21s - loss: 0.6932 - acc: 0.4995 - val_loss: 0.6933 - val_acc: 0.4999\n","Epoch 67/200\n","103324/103324 - 21s - loss: 0.6932 - acc: 0.5007 - val_loss: 0.6933 - val_acc: 0.5001\n","Epoch 68/200\n","103324/103324 - 22s - loss: 0.6932 - acc: 0.4999 - val_loss: 0.6936 - val_acc: 0.4999\n","Epoch 69/200\n","103324/103324 - 21s - loss: 0.6932 - acc: 0.4987 - val_loss: 0.6933 - val_acc: 0.5001\n","Epoch 70/200\n","103324/103324 - 21s - loss: 0.6932 - acc: 0.5018 - val_loss: 0.6933 - val_acc: 0.5001\n","Epoch 71/200\n","103324/103324 - 21s - loss: 0.6932 - acc: 0.5003 - val_loss: 0.6932 - val_acc: 0.4999\n","Epoch 72/200\n","103324/103324 - 21s - loss: 0.6932 - acc: 0.5010 - val_loss: 0.6932 - val_acc: 0.5001\n","Epoch 73/200\n","103324/103324 - 21s - loss: 0.6933 - acc: 0.4967 - val_loss: 0.6931 - val_acc: 0.4999\n","Epoch 74/200\n","103324/103324 - 21s - loss: 0.6932 - acc: 0.5023 - val_loss: 0.6932 - val_acc: 0.5001\n","Epoch 75/200\n","103324/103324 - 21s - loss: 0.6932 - acc: 0.4995 - val_loss: 0.6933 - val_acc: 0.5001\n","Epoch 76/200\n","103324/103324 - 21s - loss: 0.6932 - acc: 0.5016 - val_loss: 0.6932 - val_acc: 0.5001\n","Epoch 77/200\n","103324/103324 - 21s - loss: 0.6932 - acc: 0.5012 - val_loss: 0.6932 - val_acc: 0.4999\n","Epoch 78/200\n","103324/103324 - 22s - loss: 0.6932 - acc: 0.5003 - val_loss: 0.6932 - val_acc: 0.4999\n","Epoch 79/200\n","103324/103324 - 21s - loss: 0.6932 - acc: 0.4996 - val_loss: 0.6932 - val_acc: 0.4999\n","Epoch 80/200\n","103324/103324 - 21s - loss: 0.6932 - acc: 0.4988 - val_loss: 0.6932 - val_acc: 0.4999\n","Epoch 81/200\n","103324/103324 - 21s - loss: 0.6932 - acc: 0.4999 - val_loss: 0.6932 - val_acc: 0.5001\n","Epoch 82/200\n","103324/103324 - 21s - loss: 0.6933 - acc: 0.4980 - val_loss: 0.6932 - val_acc: 0.5001\n","Epoch 83/200\n","103324/103324 - 22s - loss: 0.6932 - acc: 0.4999 - val_loss: 0.6932 - val_acc: 0.5001\n","Epoch 84/200\n","103324/103324 - 22s - loss: 0.6932 - acc: 0.5021 - val_loss: 0.6932 - val_acc: 0.4999\n","Epoch 85/200\n","103324/103324 - 21s - loss: 0.6932 - acc: 0.4980 - val_loss: 0.6933 - val_acc: 0.5001\n","Epoch 86/200\n","103324/103324 - 21s - loss: 0.6932 - acc: 0.5004 - val_loss: 0.6932 - val_acc: 0.4999\n","Epoch 87/200\n","103324/103324 - 21s - loss: 0.6932 - acc: 0.5007 - val_loss: 0.6935 - val_acc: 0.4999\n","Epoch 88/200\n","103324/103324 - 21s - loss: 0.6932 - acc: 0.4998 - val_loss: 0.6931 - val_acc: 0.4999\n","Epoch 89/200\n","103324/103324 - 21s - loss: 0.6932 - acc: 0.5000 - val_loss: 0.6932 - val_acc: 0.5001\n","Epoch 90/200\n","103324/103324 - 21s - loss: 0.6932 - acc: 0.5007 - val_loss: 0.6931 - val_acc: 0.4999\n","Epoch 91/200\n","103324/103324 - 21s - loss: 0.6932 - acc: 0.5016 - val_loss: 0.6934 - val_acc: 0.4999\n","Epoch 92/200\n","103324/103324 - 21s - loss: 0.6932 - acc: 0.5003 - val_loss: 0.6932 - val_acc: 0.4999\n","Epoch 93/200\n","103324/103324 - 21s - loss: 0.6932 - acc: 0.4987 - val_loss: 0.6931 - val_acc: 0.5001\n","Epoch 94/200\n","103324/103324 - 21s - loss: 0.6932 - acc: 0.5020 - val_loss: 0.6936 - val_acc: 0.5001\n","Epoch 95/200\n","103324/103324 - 21s - loss: 0.6933 - acc: 0.4980 - val_loss: 0.6932 - val_acc: 0.4999\n","Epoch 96/200\n","103324/103324 - 21s - loss: 0.6932 - acc: 0.5013 - val_loss: 0.6934 - val_acc: 0.4999\n","Epoch 97/200\n","103324/103324 - 21s - loss: 0.6932 - acc: 0.5019 - val_loss: 0.6932 - val_acc: 0.5001\n","Epoch 98/200\n","103324/103324 - 21s - loss: 0.6933 - acc: 0.4966 - val_loss: 0.6932 - val_acc: 0.4999\n","Epoch 99/200\n","103324/103324 - 21s - loss: 0.6933 - acc: 0.4981 - val_loss: 0.6932 - val_acc: 0.5001\n","Epoch 100/200\n","103324/103324 - 21s - loss: 0.6932 - acc: 0.5022 - val_loss: 0.6936 - val_acc: 0.4999\n","Epoch 101/200\n","103324/103324 - 21s - loss: 0.6933 - acc: 0.4976 - val_loss: 0.6932 - val_acc: 0.4999\n","Epoch 102/200\n","Restoring model weights from the end of the best epoch.\n","103324/103324 - 22s - loss: 0.6932 - acc: 0.5004 - val_loss: 0.6932 - val_acc: 0.5001\n","Epoch 00102: early stopping\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EwPAwq2fy2SQ","colab_type":"code","colab":{}},"source":["import time\n","import tensorflow.keras.initializers\n","import statistics\n","import tensorflow.keras\n","from sklearn import metrics\n","from sklearn.model_selection import StratifiedKFold\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Activation, Dropout\n","from tensorflow.keras import regularizers\n","from tensorflow.keras.callbacks import EarlyStopping\n","from sklearn.model_selection import StratifiedShuffleSplit\n","from tensorflow.keras.layers import LeakyReLU,PReLU\n","from tensorflow.keras.optimizers import Adam"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pefYXkj71GZY","colab_type":"code","colab":{}},"source":["def hms_string(sec_elapsed):\n","    h = int(sec_elapsed / (60 * 60))\n","    m = int((sec_elapsed % (60 * 60)) / 60)\n","    s = sec_elapsed % 60\n","    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"knI6Y7Rn1HBg","colab_type":"code","outputId":"b4ec373b-aac1-4cd8-ad4c-849f94d7ca94","executionInfo":{"status":"ok","timestamp":1573163212867,"user_tz":-480,"elapsed":8482,"user":{"displayName":"陆韡","photoUrl":"","userId":"00756817164624838101"}},"colab":{"base_uri":"https://localhost:8080/","height":271}},"source":["!pip install bayesian-optimization"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting bayesian-optimization\n","  Downloading https://files.pythonhosted.org/packages/72/0c/173ac467d0a53e33e41b521e4ceba74a8ac7c7873d7b857a8fbdca88302d/bayesian-optimization-1.0.1.tar.gz\n","Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (1.17.3)\n","Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (1.3.1)\n","Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from bayesian-optimization) (0.21.3)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (0.14.0)\n","Building wheels for collected packages: bayesian-optimization\n","  Building wheel for bayesian-optimization (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for bayesian-optimization: filename=bayesian_optimization-1.0.1-cp36-none-any.whl size=10032 sha256=c2c7bc69db8d97891d9d2bcb2bd53fcbbd2d83279e1a5033586fff86fabbdf55\n","  Stored in directory: /root/.cache/pip/wheels/1d/0d/3b/6b9d4477a34b3905f246ff4e7acf6aafd4cc9b77d473629b77\n","Successfully built bayesian-optimization\n","Installing collected packages: bayesian-optimization\n","Successfully installed bayesian-optimization-1.0.1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HH_2JrjHPfLZ","colab_type":"text"},"source":["##### Input Perturbation Ranking"]},{"cell_type":"code","metadata":{"id":"N6s3aJRoPik_","colab_type":"code","colab":{}},"source":["def perturbation_rank(model, x, y, names, regression):\n","    errors = []\n","\n","    for i in range(x.shape[1]):\n","        hold = np.array(x[:, i])\n","        np.random.shuffle(x[:, i])\n","        \n","        if regression:\n","            pred = model.predict(x)\n","            error = metrics.mean_squared_error(y, pred)\n","        else:\n","            pred = model.predict(x)\n","            error = metrics.log_loss(y, pred)\n","            \n","        errors.append(error)\n","        x[:, i] = hold\n","        \n","    max_error = np.max(errors)\n","    importance = [e/max_error for e in errors]\n","\n","    data = {'name':names,'error':errors,'importance':importance}\n","    result = pd.DataFrame(data, columns = ['name','error','importance'])\n","    result.sort_values(by=['importance'], ascending=[0], inplace=True)\n","    result.reset_index(inplace=True, drop=True)\n","    return result"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HUFhgqKI5aih","colab_type":"code","outputId":"821eb671-2dde-440b-e25a-a38a7251f129","executionInfo":{"status":"ok","timestamp":1573315973380,"user_tz":-480,"elapsed":179201,"user":{"displayName":"陆韡","photoUrl":"","userId":"00756817164624838101"}},"colab":{"base_uri":"https://localhost:8080/","height":415}},"source":["from IPython.display import display, HTML\n","\n","names = list(all_data_clean.drop(columns = drops).columns)\n","rank = perturbation_rank(model, x_test, y_test, names, False)\n","display(rank)"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>name</th>\n","      <th>error</th>\n","      <th>importance</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>300d_glove_canberra</td>\n","      <td>0.564802</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>diff_len</td>\n","      <td>0.553036</td>\n","      <td>0.979169</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>len_q1</td>\n","      <td>0.551476</td>\n","      <td>0.976407</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>200d_glove_canberra</td>\n","      <td>0.545663</td>\n","      <td>0.966115</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>len_q2</td>\n","      <td>0.545373</td>\n","      <td>0.965601</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>254</th>\n","      <td>cityblock_distance_model_w2v_2_100_3</td>\n","      <td>0.521215</td>\n","      <td>0.922828</td>\n","    </tr>\n","    <tr>\n","      <th>255</th>\n","      <td>canberra_distance_model_w2v_1_160_5</td>\n","      <td>0.521142</td>\n","      <td>0.922700</td>\n","    </tr>\n","    <tr>\n","      <th>256</th>\n","      <td>cityblock_distance_model_w2v_2_160_3</td>\n","      <td>0.521120</td>\n","      <td>0.922660</td>\n","    </tr>\n","    <tr>\n","      <th>257</th>\n","      <td>canberra_distance_model_w2v_1_100_3</td>\n","      <td>0.520916</td>\n","      <td>0.922300</td>\n","    </tr>\n","    <tr>\n","      <th>258</th>\n","      <td>canberra_distance_model_w2v_2_100_3</td>\n","      <td>0.520827</td>\n","      <td>0.922142</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>259 rows × 3 columns</p>\n","</div>"],"text/plain":["                                     name     error  importance\n","0                     300d_glove_canberra  0.564802    1.000000\n","1                                diff_len  0.553036    0.979169\n","2                                  len_q1  0.551476    0.976407\n","3                     200d_glove_canberra  0.545663    0.966115\n","4                                  len_q2  0.545373    0.965601\n","..                                    ...       ...         ...\n","254  cityblock_distance_model_w2v_2_100_3  0.521215    0.922828\n","255   canberra_distance_model_w2v_1_160_5  0.521142    0.922700\n","256  cityblock_distance_model_w2v_2_160_3  0.521120    0.922660\n","257   canberra_distance_model_w2v_1_100_3  0.520916    0.922300\n","258   canberra_distance_model_w2v_2_100_3  0.520827    0.922142\n","\n","[259 rows x 3 columns]"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"pvJ4GU6a8kBz","colab_type":"code","outputId":"2b0f99a5-a6bc-4872-d7cc-115e9cff4475","executionInfo":{"status":"ok","timestamp":1573316105992,"user_tz":-480,"elapsed":477,"user":{"displayName":"陆韡","photoUrl":"","userId":"00756817164624838101"}},"colab":{"base_uri":"https://localhost:8080/","height":377}},"source":["list(rank['name'])[:20]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['300d_glove_canberra',\n"," 'diff_len',\n"," 'len_q1',\n"," '200d_glove_canberra',\n"," 'len_q2',\n"," 'same_words.1',\n"," 'same_words',\n"," 'canberra_distance_model_w2v_2_300_5',\n"," '50d_glove_canberra',\n"," 'shared_count',\n"," 'canberra_distance_model_w2v_2_190_7',\n"," 'canberra_distance_model_w2v_2_160_7',\n"," 'canberra_distance_model_w2v_1_190_7',\n"," 'feature_percent_sent2',\n"," 'canberra_distance_model_w2v_2_190_5',\n"," 'len_word_q2',\n"," 'canberra_distance_model_w2v_2_250_5',\n"," 'canberra_distance_model_w2v_2_100_7',\n"," 'bow_cityblock',\n"," 'canberra_distance_model_w2v_1_130_7']"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"0r4aTZLSP3oC","colab_type":"text"},"source":["##### Bayesian Optimization"]},{"cell_type":"code","metadata":{"id":"y5jvxdEHQXOJ","colab_type":"code","colab":{}},"source":["def evaluate_network(dropout,lr,neuronPct,neuronShrink):\n","    SPLITS = 1\n","\n","    # Bootstrap\n","    boot = StratifiedShuffleSplit(n_splits=SPLITS, test_size=0.2)\n","\n","    # Track progress\n","    mean_benchmark = []\n","    epochs_needed = []\n","    num = 0\n","    neuronCount = int(neuronPct * 5000)\n","\n","    # Loop through samples\n","    for train, test in boot.split(x,y):\n","        start_time = time.time()\n","        num+=1\n","\n","        # Split train and test\n","        x_train = x[train]\n","        y_train = y[train]\n","        x_test = x[test]\n","        y_test = y[test]\n","\n","        # Construct neural network\n","        # kernel_initializer = tensorflow.keras.initializers.he_uniform(seed=None)\n","        model = Sequential()\n","        \n","        layer = 0\n","        while neuronCount>25 and layer<10:\n","            #print(neuronCount)\n","            if layer==0:\n","                model.add(Dense(neuronCount, \n","                    input_dim=x.shape[1], \n","                    activation='relu')) \n","            else:\n","                model.add(Dense(neuronCount, activation='relu')) \n","            model.add(Dropout(dropout))\n","        \n","            neuronCount = neuronCount * neuronShrink\n","        \n","        model.add(Dense(1,activation='sigmoid')) # Output\n","        model.compile(loss='binary_crossentropy', optimizer=Adam(lr=lr))\n","        monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, \n","            patience=10, verbose=0, mode='auto', restore_best_weights=True)\n","\n","        # Train on the bootstrap sample\n","        model.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[monitor],verbose=0,epochs=50)\n","        epochs = monitor.stopped_epoch\n","        epochs_needed.append(epochs)\n","\n","        # Predict on the out of boot (validation)\n","        pred = model.predict(x_test)\n","\n","        # Measure this bootstrap's log loss\n","        #y_compare = np.argmax(y_test,axis=1) # For log loss calculation\n","        score = metrics.log_loss(y_test, pred)\n","        mean_benchmark.append(score)\n","        m1 = statistics.mean(mean_benchmark)\n","        #m1 = statistics.mean(mean_benchmark[~np.isnan(mean_benchmark)])\n","        m2 = statistics.mean(epochs_needed)\n","        mdev = statistics.pstdev(mean_benchmark)\n","\n","        # Record this iteration\n","        time_took = time.time() - start_time\n","        #print(f\"#{num}: score={score:.6f}, mean score={m1:.6f}, stdev={mdev:.6f}, epochs={epochs}, mean epochs={int(m2)}, time={hms_string(time_took)}\")\n","\n","    tensorflow.keras.backend.clear_session()\n","    return (m1)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"usP9uSy25Akl","colab_type":"code","outputId":"8d5fc563-238e-465c-8cfd-f3f091cb7125","executionInfo":{"status":"error","timestamp":1573266242702,"user_tz":-480,"elapsed":1087907,"user":{"displayName":"陆韡","photoUrl":"","userId":"00756817164624838101"}},"colab":{"base_uri":"https://localhost:8080/","height":573}},"source":["for dropout in np.arange(0.0,0.49,0.1):\n","    for lr in np.arange(0.0,0.1,0.02):\n","        for neuronPct in np.arange(0.01,0.07,0.02):\n","            for neuronShrink in np.arange(0.01,1,0.2):\n","                evaluate_network(dropout,lr,neuronPct,neuronShrink)\n","                print(dropout, lr, neuronPct, neuronShrink, m1)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0.0 0.0 0.01 0.01 0.6476401327683433\n","0.0 0.0 0.01 0.21000000000000002 0.6476401327683433\n","0.0 0.0 0.01 0.41000000000000003 0.6476401327683433\n","0.0 0.0 0.01 0.6100000000000001 0.6476401327683433\n","0.0 0.0 0.01 0.81 0.6476401327683433\n","0.0 0.0 0.03 0.01 0.6476401327683433\n","0.0 0.0 0.03 0.21000000000000002 0.6476401327683433\n","0.0 0.0 0.03 0.41000000000000003 0.6476401327683433\n","0.0 0.0 0.03 0.6100000000000001 0.6476401327683433\n","0.0 0.0 0.03 0.81 0.6476401327683433\n","0.0 0.0 0.049999999999999996 0.01 0.6476401327683433\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-95-3d12b8162571>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mneuronPct\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.07\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.02\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mneuronShrink\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                 \u001b[0mevaluate_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mneuronPct\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mneuronShrink\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneuronPct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneuronShrink\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-85-570db77ddc58>\u001b[0m in \u001b[0;36mevaluate_network\u001b[0;34m(dropout, lr, neuronPct, neuronShrink)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# Train on the bootstrap sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped_epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mepochs_needed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m   def evaluate(self,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m   def evaluate(self,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    438\u001b[0m           \u001b[0mvalidation_in_fit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m           \u001b[0mprepared_feed_values_from_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m           steps_name='validation_steps')\n\u001b[0m\u001b[1;32m    441\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0mval_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mval_results\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"dvAf01WTx0ud","colab_type":"code","outputId":"76d3625e-bb77-4315-8e68-4262af2dbd1b","executionInfo":{"status":"error","timestamp":1573264353978,"user_tz":-480,"elapsed":479599,"user":{"displayName":"陆韡","photoUrl":"","userId":"00756817164624838101"}},"colab":{"base_uri":"https://localhost:8080/","height":575}},"source":["from bayes_opt import BayesianOptimization\n","import time\n","\n","# Bounded region of parameter space\n","pbounds = {'dropout': (0.0, 0.499),\n","           'lr': (0.0, 0.1),\n","           'neuronPct': (0.01, 0.07),\n","           'neuronShrink': (0.01, 1)\n","          }\n","\n","optimizer = BayesianOptimization(\n","    f=evaluate_network,\n","    pbounds=pbounds,\n","    verbose=2,  # verbose = 1 prints only when a maximum is observed, verbose = 0 is silent\n","    random_state=1,\n",")\n","\n","start_time = time.time()\n","optimizer.maximize(init_points=10, n_iter=100,)\n","time_took = time.time() - start_time\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["|   iter    |  target   |  dropout  |    lr     | neuronPct | neuron... |\n","-------------------------------------------------------------------------\n","| \u001b[0m 1       \u001b[0m | \u001b[0m 0.6932  \u001b[0m | \u001b[0m 0.2081  \u001b[0m | \u001b[0m 0.07203 \u001b[0m | \u001b[0m 0.01001 \u001b[0m | \u001b[0m 0.3093  \u001b[0m |\n","| \u001b[0m 2       \u001b[0m | \u001b[0m 0.6931  \u001b[0m | \u001b[0m 0.07323 \u001b[0m | \u001b[0m 0.009234\u001b[0m | \u001b[0m 0.02118 \u001b[0m | \u001b[0m 0.3521  \u001b[0m |\n","| \u001b[95m 3       \u001b[0m | \u001b[95m 0.6938  \u001b[0m | \u001b[95m 0.198   \u001b[0m | \u001b[95m 0.05388 \u001b[0m | \u001b[95m 0.03515 \u001b[0m | \u001b[95m 0.6884  \u001b[0m |\n","| \u001b[0m 4       \u001b[0m | \u001b[0m 0.6932  \u001b[0m | \u001b[0m 0.102   \u001b[0m | \u001b[0m 0.08781 \u001b[0m | \u001b[0m 0.01164 \u001b[0m | \u001b[0m 0.6738  \u001b[0m |\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/bayes_opt/target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: (0.20823509638119636, 0.05586898284457517, 0.018423216315714027, 0.20612047419403)","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-86-5d8f86b5a825>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_points\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mtime_took\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[0;34m(self, init_points, n_iter, acq, kappa, xi, **gp_params)\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0miteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_probe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlazy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOPTMIZATION_END\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params, lazy)\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOPTMIZATION_STEP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/bayes_opt/target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-85-570db77ddc58>\u001b[0m in \u001b[0;36mevaluate_network\u001b[0;34m(dropout, lr, neuronPct, neuronShrink)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# Train on the bootstrap sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped_epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mepochs_needed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m   def evaluate(self,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m   def evaluate(self,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    373\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mins\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m               \u001b[0;31m# Do not slice the training phase flag.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m               \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m               \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"y_OnoYqLQwji"},"source":["#### Ensemble models"]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"e29469e9-9b42-4999-d586-0dc9916b6a5a","executionInfo":{"status":"ok","timestamp":1573341095233,"user_tz":-480,"elapsed":772173,"user":{"displayName":"陆韡","photoUrl":"","userId":"00756817164624838101"}},"id":"6IE9AnmfQ11D","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["import os\n","import math\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","\n","x, X_val, y, y_val = train_test_split(x_train.values, y_train.values, test_size=0.2, random_state=47)\n","\n","SHUFFLE = False\n","FOLDS = 10\n","\n","def build_ann():\n","    model = Sequential()\n","    model.add(Dense(100, input_dim=x.shape[1], activation='relu'))\n","    model.add(Dropout(0.2))\n","    model.add(Dense(25, activation='relu'))\n","    model.add(Dense(1, activation='sigmoid'))\n","    Adam = optimizers.Adam(lr=0.001)\n","    model.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['accuracy'])\n","    return model\n","\n","\n","def stretch(y):\n","    return (y - y.min()) / (y.max() - y.min())\n","\n","\n","def blend_ensemble(x, y, x_submit):\n","    kf = StratifiedKFold(FOLDS)\n","    folds = list(kf.split(x,y))\n","\n","    models = [  build_ann(),\n","            xgb.XGBClassifier(learning_rate=0.1, max_depth=5, objective='binary:logistic', random_state=47, epoch=200),\\\n","            lgb.LGBMClassifier(max_depth=6, learning_rate=0.12, objective='binary', metric='binary_logloss', lambda_l1=0.1),\\\n","            LogisticRegression(C=100, random_state=47, penalty='l1',)]#,\\\n","            #RandomForestClassifier(n_estimators=10, max_features=3, random_state=47)]\n","\n","    dataset_blend_train = np.zeros((x.shape[0], len(models)))\n","    dataset_blend_test = np.zeros((x_submit.shape[0], len(models)))\n","\n","    for j, model in enumerate(models):\n","        print(\"Model: {} : {}\".format(j, model) )\n","        fold_sums = np.zeros((x_submit.shape[0], len(folds)))\n","        total_loss = 0\n","        for i, (train, test) in enumerate(folds):\n","            x_train = x[train]\n","            y_train = y[train]\n","            x_test = x[test]\n","            y_test = y[test]\n","            model.fit(x_train, y_train)\n","            pred = np.array(model.predict_proba(x_test))\n","            # pred = model.predict_proba(x_test)\n","            if j==0:\n","                dataset_blend_train[test, j] = pred.flatten()\n","            else:\n","                dataset_blend_train[test, j] = pred[:, 1]\n","            \n","            pred2 = np.array(model.predict_proba(x_submit))\n","            #fold_sums[:, i] = model.predict_proba(x_submit)[:, 1]\n","\n","            if j==0:\n","                fold_sums[:, i] = pred2.flatten()\n","            else:\n","                fold_sums[:, i] = pred2[:, 1]\n","\n","            loss = log_loss(y_test, pred)\n","            total_loss+=loss\n","            print(\"Fold #{}: loss={}\".format(i,loss))\n","        print(\"{}: Mean loss={}\".format(model.__class__.__name__,total_loss/len(folds)))\n","        dataset_blend_test[:, j] = fold_sums.mean(1)\n","\n","    print()\n","    print(\"Blending models.\")\n","    blend = LogisticRegression(solver='lbfgs')\n","    blend.fit(dataset_blend_train, y)\n","    return blend.predict_proba(dataset_blend_test)\n","\n","if __name__ == '__main__':\n","\n","    np.random.seed(42)  # seed to shuffle the train set\n","\n","    print(\"Loading data...\")\n","\n","    if SHUFFLE:\n","        idx = np.random.permutation(y.size)\n","        x = x[idx]\n","        y = y[idx]\n","\n","    submit_data = blend_ensemble(x, y, X_val)\n","    submit_data = stretch(submit_data)\n","\n","    ####################\n","    # Build submit file\n","    ####################\n","    ids = [id+1 for id in range(submit_data.shape[0])]\n","    submit_df = pd.DataFrame({'MoleculeId': ids, 'PredictedProbability': submit_data[:, 1]},\n","                             columns=['MoleculeId','PredictedProbability'])\n","    \n","    print(log_loss(y_val, submit_data[:, 1]))\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Loading data...\n","Model: 0 : <tensorflow.python.keras.engine.sequential.Sequential object at 0x7f9050e56748>\n","Train on 90261 samples\n","90261/90261 [==============================] - 11s 127us/sample - loss: 0.8266 - acc: 0.6385\n","Fold #0: loss=0.5511684605189048\n","Train on 90261 samples\n","90261/90261 [==============================] - 11s 123us/sample - loss: 0.5696 - acc: 0.7073\n","Fold #1: loss=0.550884449179821\n","Train on 90262 samples\n","90262/90262 [==============================] - 11s 125us/sample - loss: 0.5586 - acc: 0.7177\n","Fold #2: loss=0.5400628521962254\n","Train on 90262 samples\n","90262/90262 [==============================] - 11s 124us/sample - loss: 0.5532 - acc: 0.7215\n","Fold #3: loss=0.5447344950541149\n","Train on 90262 samples\n","90262/90262 [==============================] - 11s 127us/sample - loss: 0.5472 - acc: 0.7261\n","Fold #4: loss=0.5382207332909016\n","Train on 90262 samples\n","90262/90262 [==============================] - 11s 127us/sample - loss: 0.5446 - acc: 0.7265\n","Fold #5: loss=0.5324909045083807\n","Train on 90262 samples\n","90262/90262 [==============================] - 11s 127us/sample - loss: 0.5412 - acc: 0.7293\n","Fold #6: loss=0.5433720567166088\n","Train on 90262 samples\n","90262/90262 [==============================] - 11s 124us/sample - loss: 0.5390 - acc: 0.7311\n","Fold #7: loss=0.5495700576338789\n","Train on 90262 samples\n","90262/90262 [==============================] - 11s 123us/sample - loss: 0.5380 - acc: 0.7316\n","Fold #8: loss=0.5389333589173114\n","Train on 90263 samples\n","90263/90263 [==============================] - 11s 123us/sample - loss: 0.5369 - acc: 0.7327\n","Fold #9: loss=0.527313591164354\n","Sequential: Mean loss=0.5416750959180502\n","Model: 1 : XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n","              colsample_bynode=1, colsample_bytree=1, epoch=200, gamma=0,\n","              learning_rate=0.1, max_delta_step=0, max_depth=5,\n","              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n","              nthread=None, objective='binary:logistic', random_state=47,\n","              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n","              silent=None, subsample=1, verbosity=1)\n","Fold #0: loss=0.4960162837200836\n","Fold #1: loss=0.5055514785781241\n","Fold #2: loss=0.5050008920608853\n","Fold #3: loss=0.5065403339920467\n","Fold #4: loss=0.5075087672815584\n","Fold #5: loss=0.49712980447494154\n","Fold #6: loss=0.5034531271126932\n","Fold #7: loss=0.5021283862855703\n","Fold #8: loss=0.49738548626122264\n","Fold #9: loss=0.4945840290732723\n","XGBClassifier: Mean loss=0.5015298588840398\n","Model: 2 : LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n","               importance_type='split', lambda_l1=0.1, learning_rate=0.12,\n","               max_depth=6, metric='binary_logloss', min_child_samples=20,\n","               min_child_weight=0.001, min_split_gain=0.0, n_estimators=100,\n","               n_jobs=-1, num_leaves=31, objective='binary', random_state=None,\n","               reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,\n","               subsample_for_bin=200000, subsample_freq=0)\n","Fold #0: loss=0.49471771705183987\n","Fold #1: loss=0.5050722503218469\n","Fold #2: loss=0.5042959561545214\n","Fold #3: loss=0.5066627139753168\n","Fold #4: loss=0.5070111481348779\n","Fold #5: loss=0.49631796894916175\n","Fold #6: loss=0.5009910074135616\n","Fold #7: loss=0.5001393209836769\n","Fold #8: loss=0.4965812938707007\n","Fold #9: loss=0.4938934665288945\n","LGBMClassifier: Mean loss=0.5005682843384399\n","Model: 3 : LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n","                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n","                   multi_class='warn', n_jobs=None, penalty='l1',\n","                   random_state=47, solver='warn', tol=0.0001, verbose=0,\n","                   warm_start=False)\n","Fold #0: loss=0.5741000735328\n","Fold #1: loss=0.5775589120158534\n","Fold #2: loss=0.5752217876262256\n","Fold #3: loss=0.5639753877425157\n","Fold #4: loss=0.5783225398561265\n","Fold #5: loss=0.5601356856152603\n","Fold #6: loss=0.5659836700393609\n","Fold #7: loss=0.5646802691640457\n","Fold #8: loss=0.5760471651464654\n","Fold #9: loss=0.553791251514994\n","LogisticRegression: Mean loss=0.5689816742253647\n","\n","Blending models.\n","0.5092434381677434\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"PUBgIgYERyIP","colab_type":"text"},"source":["#### Evaluating the features"]},{"cell_type":"code","metadata":{"id":"59NzyphAR3Vj","colab_type":"code","outputId":"a294abae-7a02-45f6-bb77-78d00a352435","executionInfo":{"status":"ok","timestamp":1572988863944,"user_tz":-480,"elapsed":1726,"user":{"displayName":"陆韡","photoUrl":"","userId":"00756817164624838101"}},"colab":{"base_uri":"https://localhost:8080/","height":188}},"source":["report = all_data_clean.copy()\n","grp = report.groupby(by=['same_source']).mean()\n","grp"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>cosine_distance_model_w2v_1_40_3</th>\n","      <th>cityblock_distance_model_w2v_1_40_3</th>\n","      <th>jaccard_distance_model_w2v_1_40_3</th>\n","      <th>canberra_distance_model_w2v_1_40_3</th>\n","      <th>euclidean_distance_model_w2v_1_40_3</th>\n","      <th>minkowski_distance_model_w2v_1_40_3</th>\n","      <th>braycurtis_distance_model_w2v_1_40_3</th>\n","      <th>cosine_distance_model_w2v_1_40_5</th>\n","      <th>cityblock_distance_model_w2v_1_40_5</th>\n","      <th>jaccard_distance_model_w2v_1_40_5</th>\n","      <th>canberra_distance_model_w2v_1_40_5</th>\n","      <th>euclidean_distance_model_w2v_1_40_5</th>\n","      <th>minkowski_distance_model_w2v_1_40_5</th>\n","      <th>braycurtis_distance_model_w2v_1_40_5</th>\n","      <th>cosine_distance_model_w2v_1_40_7</th>\n","      <th>cityblock_distance_model_w2v_1_40_7</th>\n","      <th>jaccard_distance_model_w2v_1_40_7</th>\n","      <th>canberra_distance_model_w2v_1_40_7</th>\n","      <th>euclidean_distance_model_w2v_1_40_7</th>\n","      <th>minkowski_distance_model_w2v_1_40_7</th>\n","      <th>braycurtis_distance_model_w2v_1_40_7</th>\n","      <th>cosine_distance_model_w2v_1_70_3</th>\n","      <th>cityblock_distance_model_w2v_1_70_3</th>\n","      <th>jaccard_distance_model_w2v_1_70_3</th>\n","      <th>canberra_distance_model_w2v_1_70_3</th>\n","      <th>euclidean_distance_model_w2v_1_70_3</th>\n","      <th>minkowski_distance_model_w2v_1_70_3</th>\n","      <th>braycurtis_distance_model_w2v_1_70_3</th>\n","      <th>cosine_distance_model_w2v_1_70_5</th>\n","      <th>cityblock_distance_model_w2v_1_70_5</th>\n","      <th>jaccard_distance_model_w2v_1_70_5</th>\n","      <th>canberra_distance_model_w2v_1_70_5</th>\n","      <th>euclidean_distance_model_w2v_1_70_5</th>\n","      <th>minkowski_distance_model_w2v_1_70_5</th>\n","      <th>braycurtis_distance_model_w2v_1_70_5</th>\n","      <th>cosine_distance_model_w2v_1_70_7</th>\n","      <th>cityblock_distance_model_w2v_1_70_7</th>\n","      <th>jaccard_distance_model_w2v_1_70_7</th>\n","      <th>canberra_distance_model_w2v_1_70_7</th>\n","      <th>euclidean_distance_model_w2v_1_70_7</th>\n","      <th>...</th>\n","      <th>w2v_wmdmodel_w2v_1_100_5</th>\n","      <th>w2v_wmdmodel_w2v_1_100_7</th>\n","      <th>w2v_wmdmodel_w2v_1_130_3</th>\n","      <th>w2v_wmdmodel_w2v_1_130_5</th>\n","      <th>w2v_wmdmodel_w2v_1_130_7</th>\n","      <th>w2v_wmdmodel_w2v_1_160_3</th>\n","      <th>w2v_wmdmodel_w2v_1_160_5</th>\n","      <th>w2v_wmdmodel_w2v_1_160_7</th>\n","      <th>w2v_wmdmodel_w2v_1_190_3</th>\n","      <th>w2v_wmdmodel_w2v_1_190_5</th>\n","      <th>w2v_wmdmodel_w2v_1_190_7</th>\n","      <th>w2v_wmdmodel_w2v_2_40_3</th>\n","      <th>w2v_wmdmodel_w2v_2_40_5</th>\n","      <th>w2v_wmdmodel_w2v_2_40_7</th>\n","      <th>w2v_wmdmodel_w2v_2_70_3</th>\n","      <th>w2v_wmdmodel_w2v_2_70_5</th>\n","      <th>w2v_wmdmodel_w2v_2_70_7</th>\n","      <th>w2v_wmdmodel_w2v_2_100_3</th>\n","      <th>w2v_wmdmodel_w2v_2_100_5</th>\n","      <th>w2v_wmdmodel_w2v_2_100_7</th>\n","      <th>w2v_wmdmodel_w2v_2_130_3</th>\n","      <th>w2v_wmdmodel_w2v_2_130_5</th>\n","      <th>w2v_wmdmodel_w2v_2_130_7</th>\n","      <th>w2v_wmdmodel_w2v_2_160_3</th>\n","      <th>w2v_wmdmodel_w2v_2_160_5</th>\n","      <th>w2v_wmdmodel_w2v_2_160_7</th>\n","      <th>w2v_wmdmodel_w2v_2_190_3</th>\n","      <th>w2v_wmdmodel_w2v_2_190_5</th>\n","      <th>w2v_wmdmodel_w2v_2_190_7</th>\n","      <th>w2v_wmdmodel_w2v_2_250_5</th>\n","      <th>w2v_wmdmodel_w2v_2_300_5</th>\n","      <th>bow_cosine</th>\n","      <th>bow_cityblock</th>\n","      <th>bow_jaccard</th>\n","      <th>bow_canberra</th>\n","      <th>bow_euclidean</th>\n","      <th>bow_minkowski</th>\n","      <th>bow_braycurtis</th>\n","      <th>tfidf_sim</th>\n","      <th>same_words</th>\n","    </tr>\n","    <tr>\n","      <th>same_source</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.939994</td>\n","      <td>6.942299</td>\n","      <td>0.998341</td>\n","      <td>28.085681</td>\n","      <td>1.360873</td>\n","      <td>0.850351</td>\n","      <td>0.957495</td>\n","      <td>0.885093</td>\n","      <td>6.709271</td>\n","      <td>0.998341</td>\n","      <td>27.393590</td>\n","      <td>1.314488</td>\n","      <td>0.821005</td>\n","      <td>0.907533</td>\n","      <td>0.802964</td>\n","      <td>6.3206</td>\n","      <td>0.998341</td>\n","      <td>26.336231</td>\n","      <td>1.238289</td>\n","      <td>0.773397</td>\n","      <td>0.833983</td>\n","      <td>0.945128</td>\n","      <td>9.200179</td>\n","      <td>0.998341</td>\n","      <td>49.369512</td>\n","      <td>1.366862</td>\n","      <td>0.780232</td>\n","      <td>0.957488</td>\n","      <td>0.888232</td>\n","      <td>8.878867</td>\n","      <td>0.998341</td>\n","      <td>48.121596</td>\n","      <td>1.319056</td>\n","      <td>0.752946</td>\n","      <td>0.906936</td>\n","      <td>0.810079</td>\n","      <td>8.395026</td>\n","      <td>0.998341</td>\n","      <td>46.476837</td>\n","      <td>1.247076</td>\n","      <td>...</td>\n","      <td>0.040796</td>\n","      <td>0.044364</td>\n","      <td>0.034922</td>\n","      <td>0.036022</td>\n","      <td>0.039141</td>\n","      <td>0.031621</td>\n","      <td>0.032597</td>\n","      <td>0.035480</td>\n","      <td>0.029140</td>\n","      <td>0.030083</td>\n","      <td>0.032880</td>\n","      <td>0.060436</td>\n","      <td>0.063122</td>\n","      <td>0.071613</td>\n","      <td>0.047009</td>\n","      <td>0.049153</td>\n","      <td>0.055428</td>\n","      <td>0.039808</td>\n","      <td>0.041505</td>\n","      <td>0.046579</td>\n","      <td>0.035182</td>\n","      <td>0.036706</td>\n","      <td>0.041129</td>\n","      <td>0.031862</td>\n","      <td>0.033238</td>\n","      <td>0.037202</td>\n","      <td>0.029360</td>\n","      <td>0.030747</td>\n","      <td>0.034605</td>\n","      <td>0.026908</td>\n","      <td>0.024615</td>\n","      <td>0.989788</td>\n","      <td>21.245633</td>\n","      <td>0.994659</td>\n","      <td>19.940481</td>\n","      <td>4.799265</td>\n","      <td>3.020495</td>\n","      <td>0.990502</td>\n","      <td>0.769364</td>\n","      <td>0.105355</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.869206</td>\n","      <td>6.651821</td>\n","      <td>0.999247</td>\n","      <td>27.332060</td>\n","      <td>1.304625</td>\n","      <td>0.815429</td>\n","      <td>0.893123</td>\n","      <td>0.794914</td>\n","      <td>6.294989</td>\n","      <td>0.999247</td>\n","      <td>26.273414</td>\n","      <td>1.234752</td>\n","      <td>0.771767</td>\n","      <td>0.829662</td>\n","      <td>0.704269</td>\n","      <td>5.7861</td>\n","      <td>0.999247</td>\n","      <td>24.801337</td>\n","      <td>1.134846</td>\n","      <td>0.709321</td>\n","      <td>0.750512</td>\n","      <td>0.875287</td>\n","      <td>8.831502</td>\n","      <td>0.999246</td>\n","      <td>48.133598</td>\n","      <td>1.311737</td>\n","      <td>0.748508</td>\n","      <td>0.895499</td>\n","      <td>0.798325</td>\n","      <td>8.344901</td>\n","      <td>0.999246</td>\n","      <td>46.242916</td>\n","      <td>1.239801</td>\n","      <td>0.707594</td>\n","      <td>0.830750</td>\n","      <td>0.710832</td>\n","      <td>7.702942</td>\n","      <td>0.999246</td>\n","      <td>43.882598</td>\n","      <td>1.144520</td>\n","      <td>...</td>\n","      <td>0.038686</td>\n","      <td>0.040700</td>\n","      <td>0.033535</td>\n","      <td>0.034171</td>\n","      <td>0.035929</td>\n","      <td>0.030358</td>\n","      <td>0.030917</td>\n","      <td>0.032536</td>\n","      <td>0.027965</td>\n","      <td>0.028497</td>\n","      <td>0.030053</td>\n","      <td>0.057880</td>\n","      <td>0.059398</td>\n","      <td>0.064198</td>\n","      <td>0.045038</td>\n","      <td>0.046220</td>\n","      <td>0.049730</td>\n","      <td>0.038149</td>\n","      <td>0.039075</td>\n","      <td>0.041911</td>\n","      <td>0.033714</td>\n","      <td>0.034540</td>\n","      <td>0.037013</td>\n","      <td>0.030519</td>\n","      <td>0.031262</td>\n","      <td>0.033475</td>\n","      <td>0.028106</td>\n","      <td>0.028849</td>\n","      <td>0.030988</td>\n","      <td>0.025249</td>\n","      <td>0.023105</td>\n","      <td>0.940125</td>\n","      <td>20.303832</td>\n","      <td>0.975169</td>\n","      <td>19.046835</td>\n","      <td>4.675032</td>\n","      <td>2.958895</td>\n","      <td>0.948362</td>\n","      <td>0.789402</td>\n","      <td>0.654956</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2 rows × 333 columns</p>\n","</div>"],"text/plain":["             cosine_distance_model_w2v_1_40_3  ...  same_words\n","same_source                                    ...            \n","0                                    0.939994  ...    0.105355\n","1                                    0.869206  ...    0.654956\n","\n","[2 rows x 333 columns]"]},"metadata":{"tags":[]},"execution_count":57}]},{"cell_type":"code","metadata":{"id":"t-_ebeqVcnV4","colab_type":"code","colab":{}},"source":["def evaluate_feature(threshold):\n","    for i in range(grp.shape[1]):\n","        if abs(grp.iat[0,i]-grp.iat[1,i])/max(grp.iat[0,i],grp.iat[0,i]) > threshold:\n","             impt_features.append(grp.columns[i])\n","    return impt_features"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gxs1ULm2eFUx","colab_type":"code","outputId":"f3008f9e-83ff-4ffa-a11f-7c4eaaee2f89","executionInfo":{"status":"ok","timestamp":1572989779311,"user_tz":-480,"elapsed":362,"user":{"displayName":"陆韡","photoUrl":"","userId":"00756817164624838101"}},"colab":{"base_uri":"https://localhost:8080/","height":948}},"source":["impt_features = []\n","evaluate_feature(0.09)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['cosine_distance_model_w2v_1_40_5',\n"," 'cosine_distance_model_w2v_1_40_7',\n"," 'braycurtis_distance_model_w2v_1_40_7',\n"," 'cosine_distance_model_w2v_1_70_5',\n"," 'cosine_distance_model_w2v_1_70_7',\n"," 'braycurtis_distance_model_w2v_1_70_7',\n"," 'cosine_distance_model_w2v_1_100_5',\n"," 'cosine_distance_model_w2v_1_100_7',\n"," 'braycurtis_distance_model_w2v_1_100_7',\n"," 'cosine_distance_model_w2v_1_130_5',\n"," 'cosine_distance_model_w2v_1_130_7',\n"," 'braycurtis_distance_model_w2v_1_130_7',\n"," 'cosine_distance_model_w2v_1_160_5',\n"," 'cosine_distance_model_w2v_1_160_7',\n"," 'braycurtis_distance_model_w2v_1_160_7',\n"," 'cosine_distance_model_w2v_1_190_5',\n"," 'cosine_distance_model_w2v_1_190_7',\n"," 'braycurtis_distance_model_w2v_1_190_7',\n"," 'cosine_distance_model_w2v_2_40_5',\n"," 'braycurtis_distance_model_w2v_2_40_5',\n"," 'cosine_distance_model_w2v_2_40_7',\n"," 'cityblock_distance_model_w2v_2_40_7',\n"," 'braycurtis_distance_model_w2v_2_40_7',\n"," 'cosine_distance_model_w2v_2_70_5',\n"," 'cosine_distance_model_w2v_2_70_7',\n"," 'braycurtis_distance_model_w2v_2_70_7',\n"," 'cosine_distance_model_w2v_2_100_5',\n"," 'cosine_distance_model_w2v_2_100_7',\n"," 'braycurtis_distance_model_w2v_2_100_7',\n"," 'cosine_distance_model_w2v_2_130_5',\n"," 'cosine_distance_model_w2v_2_130_7',\n"," 'braycurtis_distance_model_w2v_2_130_7',\n"," 'cosine_distance_model_w2v_2_160_5',\n"," 'cosine_distance_model_w2v_2_160_7',\n"," 'braycurtis_distance_model_w2v_2_160_7',\n"," 'cosine_distance_model_w2v_2_190_5',\n"," 'cosine_distance_model_w2v_2_190_7',\n"," 'braycurtis_distance_model_w2v_2_190_7',\n"," 'cosine_distance_model_w2v_2_250_5',\n"," 'cosine_distance_model_w2v_2_300_5',\n"," 'word_match',\n"," 'tfidf_word_match',\n"," 'shared_count',\n"," 'diff_stops_r',\n"," 'exactly_same',\n"," 'duplicated',\n"," 'w2v_wmdmodel_w2v_2_40_7',\n"," 'w2v_wmdmodel_w2v_2_70_7',\n"," 'w2v_wmdmodel_w2v_2_100_7',\n"," 'w2v_wmdmodel_w2v_2_130_7',\n"," 'w2v_wmdmodel_w2v_2_160_7',\n"," 'w2v_wmdmodel_w2v_2_190_7',\n"," 'same_words']"]},"metadata":{"tags":[]},"execution_count":80}]},{"cell_type":"markdown","metadata":{"id":"5A6Nb1Yw0ojB","colab_type":"text"},"source":["#### Ensemble models"]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"e29469e9-9b42-4999-d586-0dc9916b6a5a","executionInfo":{"status":"ok","timestamp":1573341095233,"user_tz":-480,"elapsed":772173,"user":{"displayName":"陆韡","photoUrl":"","userId":"00756817164624838101"}},"id":"m4K-nsaphGBO","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["import os\n","import math\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","\n","x, X_val, y, y_val = train_test_split(x_train.values, y_train.values, test_size=0.2, random_state=47)\n","\n","SHUFFLE = False\n","FOLDS = 10\n","\n","def build_ann():\n","    model = Sequential()\n","    model.add(Dense(100, input_dim=x.shape[1], activation='relu'))\n","    model.add(Dropout(0.2))\n","    model.add(Dense(25, activation='relu'))\n","    model.add(Dense(1, activation='sigmoid'))\n","    Adam = optimizers.Adam(lr=0.001)\n","    model.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['accuracy'])\n","    return model\n","\n","\n","def stretch(y):\n","    return (y - y.min()) / (y.max() - y.min())\n","\n","\n","def blend_ensemble(x, y, x_submit):\n","    kf = StratifiedKFold(FOLDS)\n","    folds = list(kf.split(x,y))\n","\n","    models = [  build_ann(),\n","            xgb.XGBClassifier(learning_rate=0.1, max_depth=5, objective='binary:logistic', random_state=47, epoch=200),\\\n","            lgb.LGBMClassifier(max_depth=6, learning_rate=0.12, objective='binary', metric='binary_logloss', lambda_l1=0.1),\\\n","            LogisticRegression(C=100, random_state=47, penalty='l1',)]#,\\\n","            #RandomForestClassifier(n_estimators=10, max_features=3, random_state=47)]\n","\n","    dataset_blend_train = np.zeros((x.shape[0], len(models)))\n","    dataset_blend_test = np.zeros((x_submit.shape[0], len(models)))\n","\n","    for j, model in enumerate(models):\n","        print(\"Model: {} : {}\".format(j, model) )\n","        fold_sums = np.zeros((x_submit.shape[0], len(folds)))\n","        total_loss = 0\n","        for i, (train, test) in enumerate(folds):\n","            x_train = x[train]\n","            y_train = y[train]\n","            x_test = x[test]\n","            y_test = y[test]\n","            model.fit(x_train, y_train)\n","            pred = np.array(model.predict_proba(x_test))\n","            # pred = model.predict_proba(x_test)\n","            if j==0:\n","                dataset_blend_train[test, j] = pred.flatten()\n","            else:\n","                dataset_blend_train[test, j] = pred[:, 1]\n","            \n","            pred2 = np.array(model.predict_proba(x_submit))\n","            #fold_sums[:, i] = model.predict_proba(x_submit)[:, 1]\n","\n","            if j==0:\n","                fold_sums[:, i] = pred2.flatten()\n","            else:\n","                fold_sums[:, i] = pred2[:, 1]\n","\n","            loss = log_loss(y_test, pred)\n","            total_loss+=loss\n","            print(\"Fold #{}: loss={}\".format(i,loss))\n","        print(\"{}: Mean loss={}\".format(model.__class__.__name__,total_loss/len(folds)))\n","        dataset_blend_test[:, j] = fold_sums.mean(1)\n","\n","    print()\n","    print(\"Blending models.\")\n","    blend = LogisticRegression(solver='lbfgs')\n","    blend.fit(dataset_blend_train, y)\n","    return blend.predict_proba(dataset_blend_test)\n","\n","if __name__ == '__main__':\n","\n","    np.random.seed(42)  # seed to shuffle the train set\n","\n","    print(\"Loading data...\")\n","\n","    if SHUFFLE:\n","        idx = np.random.permutation(y.size)\n","        x = x[idx]\n","        y = y[idx]\n","\n","    submit_data = blend_ensemble(x, y, X_val)\n","    submit_data = stretch(submit_data)\n","\n","    ####################\n","    # Build submit file\n","    ####################\n","    ids = [id+1 for id in range(submit_data.shape[0])]\n","    submit_df = pd.DataFrame({'MoleculeId': ids, 'PredictedProbability': submit_data[:, 1]},\n","                             columns=['MoleculeId','PredictedProbability'])\n","    \n","    print(log_loss(y_val, submit_data[:, 1]))\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Loading data...\n","Model: 0 : <tensorflow.python.keras.engine.sequential.Sequential object at 0x7f9050e56748>\n","Train on 90261 samples\n","90261/90261 [==============================] - 11s 127us/sample - loss: 0.8266 - acc: 0.6385\n","Fold #0: loss=0.5511684605189048\n","Train on 90261 samples\n","90261/90261 [==============================] - 11s 123us/sample - loss: 0.5696 - acc: 0.7073\n","Fold #1: loss=0.550884449179821\n","Train on 90262 samples\n","90262/90262 [==============================] - 11s 125us/sample - loss: 0.5586 - acc: 0.7177\n","Fold #2: loss=0.5400628521962254\n","Train on 90262 samples\n","90262/90262 [==============================] - 11s 124us/sample - loss: 0.5532 - acc: 0.7215\n","Fold #3: loss=0.5447344950541149\n","Train on 90262 samples\n","90262/90262 [==============================] - 11s 127us/sample - loss: 0.5472 - acc: 0.7261\n","Fold #4: loss=0.5382207332909016\n","Train on 90262 samples\n","90262/90262 [==============================] - 11s 127us/sample - loss: 0.5446 - acc: 0.7265\n","Fold #5: loss=0.5324909045083807\n","Train on 90262 samples\n","90262/90262 [==============================] - 11s 127us/sample - loss: 0.5412 - acc: 0.7293\n","Fold #6: loss=0.5433720567166088\n","Train on 90262 samples\n","90262/90262 [==============================] - 11s 124us/sample - loss: 0.5390 - acc: 0.7311\n","Fold #7: loss=0.5495700576338789\n","Train on 90262 samples\n","90262/90262 [==============================] - 11s 123us/sample - loss: 0.5380 - acc: 0.7316\n","Fold #8: loss=0.5389333589173114\n","Train on 90263 samples\n","90263/90263 [==============================] - 11s 123us/sample - loss: 0.5369 - acc: 0.7327\n","Fold #9: loss=0.527313591164354\n","Sequential: Mean loss=0.5416750959180502\n","Model: 1 : XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n","              colsample_bynode=1, colsample_bytree=1, epoch=200, gamma=0,\n","              learning_rate=0.1, max_delta_step=0, max_depth=5,\n","              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n","              nthread=None, objective='binary:logistic', random_state=47,\n","              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n","              silent=None, subsample=1, verbosity=1)\n","Fold #0: loss=0.4960162837200836\n","Fold #1: loss=0.5055514785781241\n","Fold #2: loss=0.5050008920608853\n","Fold #3: loss=0.5065403339920467\n","Fold #4: loss=0.5075087672815584\n","Fold #5: loss=0.49712980447494154\n","Fold #6: loss=0.5034531271126932\n","Fold #7: loss=0.5021283862855703\n","Fold #8: loss=0.49738548626122264\n","Fold #9: loss=0.4945840290732723\n","XGBClassifier: Mean loss=0.5015298588840398\n","Model: 2 : LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n","               importance_type='split', lambda_l1=0.1, learning_rate=0.12,\n","               max_depth=6, metric='binary_logloss', min_child_samples=20,\n","               min_child_weight=0.001, min_split_gain=0.0, n_estimators=100,\n","               n_jobs=-1, num_leaves=31, objective='binary', random_state=None,\n","               reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,\n","               subsample_for_bin=200000, subsample_freq=0)\n","Fold #0: loss=0.49471771705183987\n","Fold #1: loss=0.5050722503218469\n","Fold #2: loss=0.5042959561545214\n","Fold #3: loss=0.5066627139753168\n","Fold #4: loss=0.5070111481348779\n","Fold #5: loss=0.49631796894916175\n","Fold #6: loss=0.5009910074135616\n","Fold #7: loss=0.5001393209836769\n","Fold #8: loss=0.4965812938707007\n","Fold #9: loss=0.4938934665288945\n","LGBMClassifier: Mean loss=0.5005682843384399\n","Model: 3 : LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n","                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n","                   multi_class='warn', n_jobs=None, penalty='l1',\n","                   random_state=47, solver='warn', tol=0.0001, verbose=0,\n","                   warm_start=False)\n","Fold #0: loss=0.5741000735328\n","Fold #1: loss=0.5775589120158534\n","Fold #2: loss=0.5752217876262256\n","Fold #3: loss=0.5639753877425157\n","Fold #4: loss=0.5783225398561265\n","Fold #5: loss=0.5601356856152603\n","Fold #6: loss=0.5659836700393609\n","Fold #7: loss=0.5646802691640457\n","Fold #8: loss=0.5760471651464654\n","Fold #9: loss=0.553791251514994\n","LogisticRegression: Mean loss=0.5689816742253647\n","\n","Blending models.\n","0.5092434381677434\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iCswoYlJXIrO","colab_type":"code","outputId":"642a3d2a-d2b2-43e7-e255-e93afdf01560","executionInfo":{"status":"ok","timestamp":1573071394424,"user_tz":-480,"elapsed":760,"user":{"displayName":"陆韡","photoUrl":"","userId":"00756817164624838101"}},"colab":{"base_uri":"https://localhost:8080/","height":415}},"source":["submit_df"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>MoleculeId</th>\n","      <th>PredictedProbability</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0.928677</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>0.830183</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>0.963911</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>0.362951</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>0.449426</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>25827</th>\n","      <td>25828</td>\n","      <td>0.581315</td>\n","    </tr>\n","    <tr>\n","      <th>25828</th>\n","      <td>25829</td>\n","      <td>0.135866</td>\n","    </tr>\n","    <tr>\n","      <th>25829</th>\n","      <td>25830</td>\n","      <td>0.540500</td>\n","    </tr>\n","    <tr>\n","      <th>25830</th>\n","      <td>25831</td>\n","      <td>0.475617</td>\n","    </tr>\n","    <tr>\n","      <th>25831</th>\n","      <td>25832</td>\n","      <td>0.349513</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>25832 rows × 2 columns</p>\n","</div>"],"text/plain":["       MoleculeId  PredictedProbability\n","0               1              0.928677\n","1               2              0.830183\n","2               3              0.963911\n","3               4              0.362951\n","4               5              0.449426\n","...           ...                   ...\n","25827       25828              0.581315\n","25828       25829              0.135866\n","25829       25830              0.540500\n","25830       25831              0.475617\n","25831       25832              0.349513\n","\n","[25832 rows x 2 columns]"]},"metadata":{"tags":[]},"execution_count":109}]}]}